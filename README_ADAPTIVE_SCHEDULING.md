# Self-Correcting Adaptive Inference Scheduling for Diffusion LLMs

A novel, training-free inference optimization framework that makes diffusion-based Large Language Models dynamically adapt their generation strategy in real-time. Instead of continuous parameter adjustments, this approach switches between two distinct generation modes: **High-Efficiency** and **High-Quality**.

## ğŸ¯ Overview

Traditional Fast-dLLM approaches rely on static hyperparameters (fixed block size `B` and confidence threshold `Ï„`) that remain constant throughout the entire generation process. However, text generation naturally has phases of varying difficulty.

**Self-Correcting Adaptive Inference Scheduling** introduces a framework that switches between two pre-defined operational modes based on real-time model confidence, enabling:

- **Dual-Mode Strategy**: Switches between a `High-Efficiency` mode for confident predictions and a `High-Quality` mode for uncertain regions.
- **Stable Configuration**: Avoids parameter oscillation by using fixed, optimized settings for each mode.
- **Hysteresis-based Switching**: Prevents rapid, unstable mode changes through a robust transition logic.
- **Tiered Cache Management**: Intelligent cache update strategy that reduces computational redundancy.

## ğŸ—ï¸ Architecture

### Core Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                AdaptiveInferenceScheduler                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   High-Efficiency Mode   â”‚   â”‚    High-Quality Mode     â”‚     â”‚
â”‚  â”‚                        â”‚   â”‚                        â”‚     â”‚
â”‚  â”‚ - Large Block Size     â”‚   â”‚ - Small Block Size     â”‚     â”‚
â”‚  â”‚ - Lower Ï„ for Speed    â”‚   â”‚ - Higher Ï„ for Accuracyâ”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                  â–²                                  â”‚     â”‚
â”‚                  â”‚ (Confidence > 0.95)              â”‚     â”‚
â”‚                  â–¼                                  â”‚     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚   Mode Transition Logic  â”‚                          â”‚
â”‚  â”‚ (Moving Avg. Confidence)â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Adaptive Logic Flow

The system uses a hysteresis mechanism to ensure stable transitions between modes.

```mermaid
graph TD
    A[Start in High-Efficiency Mode] --> B[Generate Block & Calculate Avg. Confidence]
    
    subgraph "Current Mode: High-Efficiency"
        B --> C{"Avg. Confidence < 0.80?"};
        C --o|No| B;
        C -->|Yes| D[Switch to High-Quality Mode];
    end

    D --> E[Generate Block & Calculate Avg. Confidence];
    
    subgraph "Current Mode: High-Quality"
        E --> F{"Avg. Confidence > 0.95?"};
        F --o|No| E;
        F -->|Yes| G[Switch to High-Efficiency Mode];
    end

    G --> B
    B --> H{Generation Complete?};
    H -->|Yes| I[End];

```

## ğŸ”§ Technical Approach

### 1. Dual-Mode Configuration

The core of the system is two distinct, pre-configured modes for generation.

| Mode | Block Size `B` | Confidence `Ï„` | Goal |
|---|---|---|---|
| **High-Efficiency** | 32 | 0.75 | Maximize throughput on predictable text |
| **High-Quality** | 8 | 0.95 | Maximize accuracy on complex or uncertain text |

This eliminates the overhead and instability of continuous, fine-grained parameter tuning.

### 2. Hysteresis-based Mode Switching

To prevent the system from oscillating rapidly between modes, we use a transition logic with a dead zone (hysteresis).

```python
# The system switches from High-Efficiency to High-Quality
# only when confidence drops below a low threshold.
if current_mode == 'High-Efficiency' and avg_confidence < 0.80:
    current_mode = 'High-Quality'

# It only switches back when confidence significantly recovers,
# surpassing a much higher threshold.
elif current_mode == 'High-Quality' and avg_confidence > 0.95:
    current_mode = 'High-Efficiency'
```
This ensures that the mode changes only when there is a clear and sustained shift in model confidence.

### 3. Tiered Cache Management

A novel three-tier cache system that eliminates redundant recomputation:

| Tier | Content | Update Policy | Purpose |
|------|---------|---------------|---------|
| **Tier 1 (Frozen)** | Prompt tokens | Once, never updated | Eliminate prompt recomputation |
| **Tier 2 (Stable)** | High-confidence generated blocks | Infrequent updates | Reduce stable content overhead |
| **Tier 3 (Active)** | Recent/uncertain blocks | Every step | Maintain accuracy for dynamic content |

**Benefits:**
- Dramatically reduces cache update overhead for long sequences
- Maintains accuracy where it matters most
- Scales efficiently with sequence length

## ğŸš€ Key Features

### âœ¨ Training-Free
- No model retraining required
- Works with any pre-trained diffusion LLM
- Plug-and-play optimization

### ğŸ“Š Real-Time Adaptation
- Stable mode switching based on model confidence
- Robust against parameter oscillation
- Self-correcting behavior at a strategic level

### ğŸ¯ Task-Agnostic
- Adapts to math, code, creative writing, QA
- No manual task-specific tuning
- Robust across diverse content types

### âš¡ Performance Optimized
- Reduced computational redundancy
- Intelligent resource allocation
- Aims for a superior accuracy-throughput frontier

## ğŸ“ˆ Performance Goals

This new approach is designed to outperform both static methods and fine-grained adaptive methods.

| Metric | Target Improvement vs Static Fast-dLLM |
|--------|--------------------------------|
| **Throughput (easy content)** | +15-25% |
| **Accuracy (complex reasoning)** | +5-12% |
| **NFE Reduction** | -10-20% |
| **Long-sequence scaling** | +20-35% |

### Accuracy vs Throughput Frontier

```
Accuracy â†‘
    â”‚
    â”‚    â— Adaptive Mode-Switching (Target)
    â”‚   â•±
    â”‚  â•±
    â”‚ â•± â— Static Fast-dLLM (B=32, Ï„=0.9)
    â”‚â•±
    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â†’ Throughput
```

## ğŸ› ï¸ Usage

### Basic Usage

The new simplified API focuses on enabling the scheduler.

```python
from generate import generate_with_adaptive_scheduling
from model.modeling_llada import LLaDAModelLM

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
model = LLaDAModelLM.from_pretrained('GSAI-ML/LLaDA-8B-Instruct')
tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct')

# ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã§ç”Ÿæˆ
output, metrics = generate_with_adaptive_scheduling(
    model=model,
    prompt=input_ids,
    gen_length=128,
    enable_tiered_cache=True     # éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–
)

# é©å¿œãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¡¨ç¤º
print(f"Average block size: {metrics['avg_block_size']:.1f}")
print(f"Mode changes: {metrics['total_adaptations']}")
```

### Advanced Configuration

You can customize the mode parameters and transition thresholds.

```python
# è©³ç´°è¨­å®šã§ã®ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
from adaptive_scheduler import AdaptiveInferenceScheduler
from cache_manager import TieredCacheManager

scheduler_config = {
    'to_quality_threshold': 0.80,
    'to_efficiency_threshold': 0.95,
    'confidence_window_size': 2,
    'high_efficiency_params': {'block_size': 32, 'threshold': 0.75},
    'high_quality_params': {'block_size': 8, 'threshold': 0.95}
}

# ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã§ç”Ÿæˆ
output, detailed_metrics = generate_with_adaptive_scheduling(
    model=model,
    prompt=input_ids,
    scheduler_config=scheduler_config
)
```

## ğŸ“Š Evaluation

### Benchmarks

The system will be evaluated on:

- **GSM8K**: Math reasoning tasks
- **HumanEval**: Code generation
- **MBPP**: Programming problems  
- **Long-form QA**: Complex reasoning
- **Creative Writing**: Open-ended generation

### Running Evaluation

```bash
# åŸºæœ¬è©•ä¾¡ã®å®Ÿè¡Œ
python test_adaptive_scheduling.py --benchmark gsm8k --model LLaDA-8B

# åŒ…æ‹¬çš„è©•ä¾¡
python test_adaptive_scheduling.py --comprehensive
```

## ğŸ”¬ Research Insights

### Expected Adaptation Patterns

The system is expected to exhibit clear mode-switching patterns:

1.  **Math Problems**: Switch to `High-Quality` for reasoning steps, return to `High-Efficiency` for computation.
2.  **Code Generation**: Use `High-Quality` for complex logic (loops, functions), `High-Efficiency` for boilerplate.
3.  **Creative Writing**: Use `High-Efficiency` for narrative, `High-Quality` for nuanced dialogue.
4.  **Q&A**: Use `High-Quality` for factual retrieval, `High-Efficiency` for explanatory parts.

### Key Hypotheses

- **Stability is Key**: Avoiding parameter oscillation is more important than fine-grained adaptation.
- **Strategic Switching**: A few, well-placed mode shifts yield better results than constant adjustments.
- **Reduced Overhead**: The simpler logic will lead to lower computational overhead and improved NFE.
- **Robustness**: The dual-mode system will be more robust across different tasks and models.

## ğŸ† Advantages over Other Approaches

| Aspect | Static Fast-dLLM | Fine-Grained Adaptation | Mode-Switching |
|--------|------------------|---------------------------|-------------------|
| **Strategy** | Fixed (B=32, Ï„=0.9) | Continuously variable | Dual-Mode |
| **Stability** | High | Potentially Low (oscillates) | High (hysteresis) |
| **Overhead** | Low | High | Medium |
| **Content Awareness** | None | Real-time (micro) | Real-time (macro) |
| **Task Generalization** | Manual tuning | Automatic but sensitive | Automatic and robust |

## ğŸ”® Future Directions

### Potential Extensions

1.  **More Modes**: Introduce an "ultra-quality" mode for extremely difficult sections.
2.  **Learned Modes**: Use offline analysis to learn the optimal parameters for each mode.
3.  **Distributed**: Adaptive mode-switching across multiple GPUs.
4.  **Hardware-Aware**: Adapt mode parameters to specific hardware constraints.

### Research Opportunities

- **Theoretical Analysis**: Formal bounds on mode-switching vs. static performance.
- **Causality-Aware**: Integrate mode-switching with causal attention mechanisms.
- **Energy Efficiency**: Optimize modes for both speed and power consumption.
- **Human Preference**: Align mode-switching logic with human judgment of difficulty.

## ğŸ“š Citation

```bibtex
@article{adaptive_scheduling_2025,
    title={Self-Correcting Adaptive Inference Scheduling for Diffusion Large Language Models},
    author={[Authors]},
    journal={[Venue]},
    year={2025},
    note={Training-free dynamic optimization for dLLM inference using mode-switching}
}
```

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ·ï¸ Tags

`diffusion-llm` `adaptive-inference` `training-free` `optimization` `scheduling` `mode-switching` `dynamic-generation` 