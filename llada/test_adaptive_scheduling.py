# Copyright 2025 NVIDIA CORPORATION & AFFILIATES
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

import torch
import numpy as np
import time
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Tuple
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

from transformers import AutoTokenizer
from model.modeling_llada import LLaDAModelLM
from generate import generate_with_dual_cache
from generate_adaptive import generate_with_adaptive_scheduling, generate_with_custom_scheduler
from adaptive_scheduler import AdaptiveInferenceScheduler
from cache_manager import TieredCacheManager


class AdaptiveSchedulingTester:
    """
    Self-Correcting Adaptive Inference Scheduling ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

    åŒ…æ‹¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã€æ¯”è¼ƒè©•ä¾¡ã‚’å®Ÿæ–½
    """

    def __init__(self, model_name: str = "GSAI-ML/LLaDA-8B-Instruct", device: str = "auto"):
        """
        ãƒ†ã‚¹ã‚¿ãƒ¼ã®åˆæœŸåŒ–

        Args:
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
            device: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹
        """
        self.model_name = model_name
        self.device = device if device != "auto" else (
            "cuda" if torch.cuda.is_available() else "cpu")

        print(f"ğŸ”§ AdaptiveSchedulingTesteråˆæœŸåŒ–")
        print(f"   ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print(f"   ãƒ‡ãƒã‚¤ã‚¹: {self.device}")

        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        self.model = LLaDAModelLM.from_pretrained(model_name).to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model.eval()

        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        self.test_cases = self._prepare_test_cases()

        print(f"âœ… åˆæœŸåŒ–å®Œäº† ({len(self.test_cases)} ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹)")

    def _prepare_test_cases(self) -> List[Dict[str, Any]]:
        """å¤šæ§˜ãªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’æº–å‚™"""
        return [
            {
                "name": "math_reasoning",
                "prompt": "Solve this step by step: If a train travels 120 km in 2 hours, what is its average speed?",
                "category": "reasoning",
                "expected_difficulty": "medium"
            },
            {
                "name": "code_generation",
                "prompt": "Write a Python function to calculate the factorial of a number:",
                "category": "code",
                "expected_difficulty": "medium"
            },
            {
                "name": "creative_writing",
                "prompt": "Write a short story about a robot discovering emotions:",
                "category": "creative",
                "expected_difficulty": "high"
            },
            {
                "name": "simple_qa",
                "prompt": "What is the capital of France?",
                "category": "factual",
                "expected_difficulty": "low",
                "expected_mode": "HIGH_EFFICIENCY"  # ç°¡å˜ãªäº‹å®Ÿå•é¡Œã§ã¯åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ãŒæœŸå¾…ã•ã‚Œã‚‹
            },
            {
                "name": "complex_reasoning",
                "prompt": "Explain the relationship between quantum mechanics and general relativity in simple terms:",
                "category": "reasoning",
                "expected_difficulty": "high",
                "expected_mode": "HIGH_QUALITY"  # è¤‡é›‘ãªæ¨è«–ã§ã¯å“è³ªãƒ¢ãƒ¼ãƒ‰ãŒæœŸå¾…ã•ã‚Œã‚‹
            },
            {
                "name": "list_generation",
                "prompt": "List 10 benefits of regular exercise:",
                "category": "structured",
                "expected_difficulty": "low",
                "expected_mode": "HIGH_EFFICIENCY"  # ãƒªã‚¹ãƒˆç”Ÿæˆã§ã¯åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ãŒæœŸå¾…ã•ã‚Œã‚‹
            }
        ]

    def plot_confidence_movement(self, metrics: Dict[str, Any], test_case_name: str,
                                 save_plots: bool = True, scheduler_config: Dict = None) -> None:
        """
        ä¿¡é ¼åº¦ã®å‹•ãã¨ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆã‚’ãƒ—ãƒ­ãƒƒãƒˆ

        Args:
            metrics: ç”Ÿæˆãƒ¡ãƒˆãƒªã‚¯ã‚¹
            test_case_name: ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å
            save_plots: ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜ã™ã‚‹ã‹
            scheduler_config: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šï¼ˆé–¾å€¤ç·šã®è¡¨ç¤ºç”¨ï¼‰
        """
        if 'confidence_history' not in metrics or 'mode_history' not in metrics:
            print("âŒ ä¿¡é ¼åº¦å±¥æ­´ã¾ãŸã¯ãƒ¢ãƒ¼ãƒ‰å±¥æ­´ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return

        confidence_history = metrics['confidence_history']
        mode_history = metrics['mode_history']

        if len(confidence_history) == 0:
            print("âŒ ä¿¡é ¼åº¦å±¥æ­´ãŒç©ºã§ã™")
            return

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤
        quality_threshold = 0.80
        efficiency_threshold = 0.95

        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šã‹ã‚‰é–¾å€¤ã‚’å–å¾—
        if scheduler_config:
            quality_threshold = scheduler_config.get(
                'to_quality_threshold', 0.80)
            efficiency_threshold = scheduler_config.get(
                'to_efficiency_threshold', 0.95)

        # ãƒ—ãƒ­ãƒƒãƒˆã®è¨­å®š
        plt.figure(figsize=(12, 8))

        # ä¸Šæ®µ: ä¿¡é ¼åº¦ã®å¤‰åŒ–
        plt.subplot(2, 1, 1)
        steps = range(len(confidence_history))
        plt.plot(steps, confidence_history, 'b-',
                 linewidth=2, label='Confidence')

        # é–¾å€¤ç·šã‚’è¿½åŠ 
        plt.axhline(y=quality_threshold, color='r', linestyle='--',
                    alpha=0.7, label=f'Quality Threshold ({quality_threshold})')
        plt.axhline(y=efficiency_threshold, color='g', linestyle='--', alpha=0.7,
                    label=f'Efficiency Threshold ({efficiency_threshold})')

        # ãƒ¢ãƒ¼ãƒ‰å¤‰æ›´ç‚¹ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
        mode_changes = []
        for i in range(1, len(mode_history)):
            if mode_history[i] != mode_history[i-1]:
                mode_changes.append(i)
                plt.axvline(x=i, color='orange', linestyle=':', alpha=0.8)

        plt.ylabel('Confidence', fontsize=12)
        plt.title(
            f'Confidence Movement - {test_case_name}', fontsize=14, fontweight='bold')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1)

        # ä¸‹æ®µ: ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆ
        plt.subplot(2, 1, 2)

        # ãƒ¢ãƒ¼ãƒ‰ã‚’æ•°å€¤ã«å¤‰æ› (HIGH_EFFICIENCY=1, HIGH_QUALITY=0)
        mode_numeric = [1 if mode ==
                        'HIGH_EFFICIENCY' else 0 for mode in mode_history]

        # ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆã‚’è‰²åˆ†ã‘ã—ã¦è¡¨ç¤º
        for i in range(len(mode_numeric)):
            color = 'lightgreen' if mode_numeric[i] == 1 else 'lightcoral'
            label = 'High-Efficiency' if mode_numeric[i] == 1 else 'High-Quality'

            # æœ€åˆã®å‡ºç¾æ™‚ã®ã¿ãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã‚‹
            if i == 0 or (i > 0 and mode_numeric[i] != mode_numeric[i-1]):
                plt.bar(i, 1, color=color, alpha=0.7, label=label if i == 0 or label not in [
                        item.get_label() for item in plt.gca().get_legend_handles_labels()[1]] else "")
            else:
                plt.bar(i, 1, color=color, alpha=0.7)

        plt.ylabel('Mode', fontsize=12)
        plt.xlabel('Generation Step', fontsize=12)
        plt.title('Mode Switching Pattern', fontsize=14, fontweight='bold')
        plt.yticks([0, 1], ['High-Quality', 'High-Efficiency'])

        # é‡è¤‡ãƒ©ãƒ™ãƒ«ã‚’é¿ã‘ã‚‹
        handles, labels = plt.gca().get_legend_handles_labels()
        by_label = dict(zip(labels, handles))
        plt.legend(by_label.values(), by_label.keys())

        plt.grid(True, alpha=0.3, axis='x')

        plt.tight_layout()

        # ãƒ—ãƒ­ãƒƒãƒˆã®ä¿å­˜
        if save_plots:
            output_dir = Path("confidence_plots")
            output_dir.mkdir(exist_ok=True)

            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = output_dir / \
                f"confidence_{test_case_name}_{timestamp}.png"
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ä¿¡é ¼åº¦ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜: {filename}")

        plt.show()

        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        self._print_confidence_stats(
            confidence_history, mode_history, mode_changes)

    def _print_confidence_stats(self, confidence_history: List[float],
                                mode_history: List[str], mode_changes: List[int]) -> None:
        """ä¿¡é ¼åº¦çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        print(f"\nğŸ“ˆ ä¿¡é ¼åº¦çµ±è¨ˆ:")
        print(f"   å¹³å‡ä¿¡é ¼åº¦: {np.mean(confidence_history):.3f}")
        print(f"   æœ€å°ä¿¡é ¼åº¦: {np.min(confidence_history):.3f}")
        print(f"   æœ€å¤§ä¿¡é ¼åº¦: {np.max(confidence_history):.3f}")
        print(f"   ä¿¡é ¼åº¦æ¨™æº–åå·®: {np.std(confidence_history):.3f}")
        print(f"   ãƒ¢ãƒ¼ãƒ‰å¤‰æ›´å›æ•°: {len(mode_changes)}")

        # ãƒ¢ãƒ¼ãƒ‰åˆ¥æ™‚é–“çµ±è¨ˆ
        efficiency_steps = sum(
            1 for mode in mode_history if mode == 'HIGH_EFFICIENCY')
        quality_steps = sum(
            1 for mode in mode_history if mode == 'HIGH_QUALITY')
        total_steps = len(mode_history)

        print(
            f"   åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰æ™‚é–“: {efficiency_steps}/{total_steps} ({efficiency_steps/total_steps*100:.1f}%)")
        print(
            f"   å“è³ªãƒ¢ãƒ¼ãƒ‰æ™‚é–“: {quality_steps}/{total_steps} ({quality_steps/total_steps*100:.1f}%)")

    def run_comprehensive_benchmark(self,
                                    gen_length: int = 128,
                                    num_runs: int = 3,
                                    save_results: bool = True,
                                    scheduler_config: Dict = None) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ

        Args:
            gen_length: ç”Ÿæˆé•·
            num_runs: å®Ÿè¡Œå›æ•°ï¼ˆå¹³å‡ã‚’å–ã‚‹ï¼‰
            save_results: çµæœã‚’ä¿å­˜ã™ã‚‹ã‹
            scheduler_config: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š

        Returns:
            ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        print(f"\nğŸš€ åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print(f"   ç”Ÿæˆé•·: {gen_length}")
        print(f"   å®Ÿè¡Œå›æ•°: {num_runs}")

        if scheduler_config:
            print(f"   ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š: {scheduler_config}")

        methods = {
            "adaptive_scheduling": lambda tc, gl: self._run_adaptive_scheduling(tc, gl, scheduler_config),
            "dual_cache": self._run_dual_cache,
        }

        all_results = {}

        for method_name, method_func in methods.items():
            print(f"\nğŸ“Š {method_name} è©•ä¾¡ä¸­...")
            method_results = {}

            for test_case in tqdm(self.test_cases, desc=f"{method_name}"):
                case_results = []

                for run in range(num_runs):
                    try:
                        result = method_func(test_case, gen_length)
                        case_results.append(result)
                    except Exception as e:
                        print(f"âŒ ã‚¨ãƒ©ãƒ¼ in {test_case['name']}, run {run}: {e}")
                        continue

                if case_results:
                    # å¹³å‡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—
                    avg_result = self._average_results(case_results)
                    method_results[test_case['name']] = avg_result

            all_results[method_name] = method_results

        # æ¯”è¼ƒåˆ†æ
        comparison = self._analyze_comparison(all_results)

        if save_results:
            self._save_results(all_results, comparison, gen_length)

        return {
            'detailed_results': all_results,
            'comparison': comparison,
            'config': {
                'gen_length': gen_length,
                'num_runs': num_runs,
                'test_cases': len(self.test_cases),
                'scheduler_config': scheduler_config
            }
        }

    def run_ablation_study(self, gen_length: int = 128,
                           base_scheduler_config: Dict = None) -> Dict[str, Any]:
        """
        ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶ã‚’å®Ÿè¡Œ

        Args:
            gen_length: ç”Ÿæˆé•·
            base_scheduler_config: ãƒ™ãƒ¼ã‚¹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š

        Returns:
            ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
        """
        print(f"\nğŸ”¬ ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶é–‹å§‹")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        default_config = {
            'to_quality_threshold': 0.80,
            'to_efficiency_threshold': 0.95,
            'confidence_window_size': 2,
            'high_efficiency_params': {'block_size': 32, 'threshold': 0.75},
            'high_quality_params': {'block_size': 8, 'threshold': 0.95}
        }

        # ãƒ™ãƒ¼ã‚¹è¨­å®šãŒã‚ã‚Œã°ãƒãƒ¼ã‚¸
        if base_scheduler_config:
            default_config.update(base_scheduler_config)

        configurations = [
            {
                "name": "full_system",
                "mode_switching": True,
                "tiered_cache": True,
                "scheduler_config": default_config.copy()
            },
            {
                "name": "aggressive_switching",
                "mode_switching": True,
                "tiered_cache": True,
                "scheduler_config": {
                    **default_config,
                    'to_quality_threshold': 0.70,
                    'to_efficiency_threshold': 0.85,
                    'confidence_window_size': 1
                }
            },
            {
                "name": "conservative_switching",
                "mode_switching": True,
                "tiered_cache": True,
                "scheduler_config": {
                    **default_config,
                    'to_quality_threshold': 0.90,
                    'to_efficiency_threshold': 0.98,
                    'confidence_window_size': 3
                }
            },
            {
                "name": "efficiency_only",
                "mode_switching": False,
                "tiered_cache": True,
                "scheduler_config": {
                    **default_config,
                    'high_quality_params': default_config['high_efficiency_params'].copy()
                }
            },
            {
                "name": "quality_only",
                "mode_switching": False,
                "tiered_cache": True,
                "scheduler_config": {
                    **default_config,
                    'high_efficiency_params': default_config['high_quality_params'].copy()
                }
            },
            {
                "name": "no_tiered_cache",
                "mode_switching": True,
                "tiered_cache": False,
                "scheduler_config": default_config.copy()
            }
        ]

        ablation_results = {}

        for config in configurations:
            print(f"\nğŸ§ª è¨­å®š: {config['name']}")
            config_results = {}

            for test_case in tqdm(self.test_cases, desc=config['name']):
                try:
                    result = self._run_ablation_config(
                        test_case, gen_length, config)
                    config_results[test_case['name']] = result
                except Exception as e:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼ in {test_case['name']}: {e}")
                    continue

            ablation_results[config['name']] = config_results

        # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æ
        analysis = self._analyze_ablation(ablation_results)

        return {
            'ablation_results': ablation_results,
            'analysis': analysis,
            'configurations': configurations
        }

    def run_long_context_evaluation(self, seq_lengths: List[int] = [512, 1024, 2048],
                                    scheduler_config: Dict = None) -> Dict[str, Any]:
        """
        é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè©•ä¾¡

        Args:
            seq_lengths: è©•ä¾¡ã™ã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®ãƒªã‚¹ãƒˆ
            scheduler_config: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š

        Returns:
            é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè©•ä¾¡çµæœ
        """
        print(f"\nğŸ“ é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè©•ä¾¡é–‹å§‹")

        # é•·æ–‡ç”¨ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        long_context_case = {
            "name": "long_context_qa",
            "prompt": "Based on the following context, answer the question: " + "A" * 200 + " Question: What is the main topic?",
            "category": "long_context",
            "expected_difficulty": "high"
        }

        results = {}

        for seq_length in seq_lengths:
            print(f"\nğŸ” ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: {seq_length}")

            # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
            adaptive_result = self._run_adaptive_scheduling(
                long_context_case, seq_length, scheduler_config)

            # é™çš„æ‰‹æ³•ï¼ˆæ¯”è¼ƒç”¨ï¼‰
            static_result = self._run_dual_cache(long_context_case, seq_length)

            results[seq_length] = {
                'adaptive': adaptive_result,
                'static': static_result,
                'speedup': static_result['total_time'] / adaptive_result['total_time'] if adaptive_result['total_time'] > 0 else 0,
                'efficiency_gain': (static_result['nfe'] - adaptive_result['nfe']) / static_result['nfe'] if static_result['nfe'] > 0 else 0
            }

            print(
                f"   ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–: {adaptive_result['total_time']:.2f}s, NFE={adaptive_result['nfe']}")
            print(
                f"   é™çš„: {static_result['total_time']:.2f}s, NFE={static_result['nfe']}")
            print(f"   ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—: {results[seq_length]['speedup']:.2f}x")

        return results

    def _run_adaptive_scheduling(self, test_case: Dict, gen_length: int,
                                 scheduler_config: Dict = None) -> Dict[str, Any]:
        """ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆå¼ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
        prompt = self.tokenizer.encode(
            test_case['prompt'], return_tensors='pt').to(self.device)

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        default_scheduler_config = {
            'to_quality_threshold': 0.80,
            'to_efficiency_threshold': 0.95,
            'confidence_window_size': 2,
            'high_efficiency_params': {'block_size': 32, 'threshold': 0.75},
            'high_quality_params': {'block_size': 8, 'threshold': 0.95}
        }

        # è¨­å®šã‚’ãƒãƒ¼ã‚¸
        if scheduler_config:
            final_config = {**default_scheduler_config, **scheduler_config}
        else:
            final_config = default_scheduler_config

        start_time = time.time()
        output, metrics = generate_with_adaptive_scheduling(
            model=self.model,
            prompt=prompt,
            gen_length=gen_length,
            enable_tiered_cache=True,
            scheduler_config=final_config,
            verbose=True  # è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–
        )
        end_time = time.time()

        # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        generated_text = self.tokenizer.decode(
            output[0, prompt.shape[1]:], skip_special_tokens=True)

        return {
            'method': 'adaptive_scheduling',
            'total_time': end_time - start_time,
            'nfe': metrics['nfe'],
            'adaptations': metrics['total_adaptations'],
            'avg_block_size': metrics.get('avg_block_size', 0),
            'cache_hit_rate': metrics.get('cache_efficiency', {}).get('cache_hit_rate', 0),
            'mode_changes': metrics['total_adaptations'],  # ãƒ¢ãƒ¼ãƒ‰å¤‰æ›´å›æ•°
            'final_mode': metrics.get('mode_history', ['UNKNOWN'])[-1] if metrics.get('mode_history') else 'UNKNOWN',
            'generated_text': generated_text,
            'text_length': len(generated_text),
            'metrics': metrics,
            'scheduler_config': final_config
        }

    def _run_dual_cache(self, test_case: Dict, gen_length: int) -> Dict[str, Any]:
        """ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å®Ÿè¡Œ"""
        prompt = self.tokenizer.encode(
            test_case['prompt'], return_tensors='pt').to(self.device)

        start_time = time.time()
        output, nfe = generate_with_dual_cache(
            model=self.model,
            prompt=prompt,
            steps=128,
            gen_length=gen_length,
            block_length=32,
            threshold=0.8
        )
        end_time = time.time()

        generated_text = self.tokenizer.decode(
            output[0, prompt.shape[1]:], skip_special_tokens=True)

        return {
            'method': 'dual_cache',
            'total_time': end_time - start_time,
            'nfe': nfe,
            'adaptations': 0,
            'avg_block_size': 32,
            'cache_hit_rate': 0,
            'generated_text': generated_text,
            'text_length': len(generated_text)
        }

    def _run_ablation_config(self, test_case: Dict, gen_length: int, config: Dict) -> Dict[str, Any]:
        """ç‰¹å®šã®ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã‚’å®Ÿè¡Œ"""
        prompt = self.tokenizer.encode(
            test_case['prompt'], return_tensors='pt').to(self.device)

        # ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆæ–¹å¼ã«åŸºã¥ãè¨­å®š
        scheduler_config = config['scheduler_config'].copy()

        start_time = time.time()
        output, metrics = generate_with_adaptive_scheduling(
            model=self.model,
            prompt=prompt,
            gen_length=gen_length,
            enable_tiered_cache=config['tiered_cache'],
            scheduler_config=scheduler_config,
            verbose=True  # è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–
        )
        end_time = time.time()

        generated_text = self.tokenizer.decode(
            output[0, prompt.shape[1]:], skip_special_tokens=True)

        return {
            'config': config['name'],
            'total_time': end_time - start_time,
            'nfe': metrics['nfe'],
            'adaptations': metrics['total_adaptations'],
            'mode_changes': metrics['total_adaptations'],
            'final_mode': metrics.get('mode_history', ['UNKNOWN'])[-1] if metrics.get('mode_history') else 'UNKNOWN',
            'avg_block_size': metrics.get('avg_block_size', 0),
            'generated_text': generated_text,
            'metrics': metrics
        }

    def _average_results(self, results: List[Dict]) -> Dict[str, Any]:
        """è¤‡æ•°å®Ÿè¡Œã®çµæœã‚’å¹³å‡"""
        if not results:
            return {}

        numeric_keys = ['total_time', 'nfe', 'adaptations', 'mode_changes',
                        'avg_block_size', 'cache_hit_rate', 'text_length']
        averaged = {'method': results[0]['method']}

        for key in numeric_keys:
            values = [r.get(key, 0) for r in results if key in r]
            if values:
                averaged[key] = np.mean(values)
                averaged[f'{key}_std'] = np.std(values)

        # æœ€åˆã®çµæœã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒ
        averaged['generated_text'] = results[0].get('generated_text', '')

        return averaged

    def _analyze_comparison(self, all_results: Dict) -> Dict[str, Any]:
        """æ‰‹æ³•é–“ã®æ¯”è¼ƒåˆ†æ"""
        comparison = {
            'summary': {},
            'per_category': {},
            'overall_metrics': {}
        }

        # å…¨ä½“çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ
        methods = list(all_results.keys())
        if len(methods) >= 2:
            for metric in ['total_time', 'nfe', 'adaptations', 'mode_changes']:
                comparison['overall_metrics'][metric] = {}
                for method in methods:
                    values = []
                    for case_name, case_result in all_results[method].items():
                        if metric in case_result:
                            values.append(case_result[metric])
                    if values:
                        comparison['overall_metrics'][metric][method] = {
                            'mean': np.mean(values),
                            'std': np.std(values),
                            'min': np.min(values),
                            'max': np.max(values)
                        }

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†æ
        categories = set(case['category'] for case in self.test_cases)
        for category in categories:
            comparison['per_category'][category] = {}

            # ãã®ã‚«ãƒ†ã‚´ãƒªã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’ç‰¹å®š
            category_cases = [case['name']
                              for case in self.test_cases if case['category'] == category]

            for method in methods:
                category_times = []
                category_nfes = []
                for case_name in category_cases:
                    if case_name in all_results[method]:
                        result = all_results[method][case_name]
                        if 'total_time' in result:
                            category_times.append(result['total_time'])
                        if 'nfe' in result:
                            category_nfes.append(result['nfe'])

                if category_times:
                    comparison['per_category'][category][method] = {
                        'avg_time': np.mean(category_times),
                        'avg_nfe': np.mean(category_nfes) if category_nfes else 0
                    }

        return comparison

    def _analyze_ablation(self, ablation_results: Dict) -> Dict[str, Any]:
        """ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®åˆ†æ"""
        analysis = {
            'component_impact': {},
            'relative_performance': {}
        }

        # ãƒ•ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ä½¿ç”¨
        if 'full_system' in ablation_results:
            baseline = ablation_results['full_system']

            for config_name, config_results in ablation_results.items():
                if config_name == 'full_system':
                    continue

                # å¹³å‡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¨ˆç®—
                baseline_times = [r['total_time'] for r in baseline.values()]
                config_times = [r['total_time']
                                for r in config_results.values()]

                baseline_nfes = [r['nfe'] for r in baseline.values()]
                config_nfes = [r['nfe'] for r in config_results.values()]

                if baseline_times and config_times:
                    time_impact = (
                        np.mean(config_times) - np.mean(baseline_times)) / np.mean(baseline_times) * 100
                    nfe_impact = (np.mean(config_nfes) - np.mean(baseline_nfes)) / \
                        np.mean(baseline_nfes) * 100 if baseline_nfes else 0

                    analysis['component_impact'][config_name] = {
                        'time_impact_percent': time_impact,
                        'nfe_impact_percent': nfe_impact
                    }

        return analysis

    def _save_results(self, results: Dict, comparison: Dict, gen_length: int):
        """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output_dir = Path("adaptive_scheduling_results")
        output_dir.mkdir(exist_ok=True)

        timestamp = time.strftime("%Y%m%d_%H%M%S")

        # è©³ç´°çµæœ
        with open(output_dir / f"detailed_results_{timestamp}.json", 'w') as f:
            json.dump(results, f, indent=2, default=str)

        # æ¯”è¼ƒçµæœ
        with open(output_dir / f"comparison_{timestamp}.json", 'w') as f:
            json.dump(comparison, f, indent=2, default=str)

        print(f"ğŸ’¾ çµæœã‚’ä¿å­˜: {output_dir}")

    def create_performance_plots(self, results: Dict):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ"""
        # TODO: matplotlib ã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ
        pass

    def quick_mode_switching_test(self, test_case_name: str = "complex_reasoning",
                                  gen_length: int = 64, scheduler_config: Dict = None,
                                  plot_confidence: bool = True) -> Dict[str, Any]:
        """
        ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆæ©Ÿèƒ½ã®ç°¡å˜ãªãƒ†ã‚¹ãƒˆ

        Args:
            test_case_name: ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚±ãƒ¼ã‚¹å
            gen_length: ç”Ÿæˆé•·
            scheduler_config: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
            plot_confidence: ä¿¡é ¼åº¦ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã‹

        Returns:
            ãƒ†ã‚¹ãƒˆçµæœ
        """
        print(f"\nğŸ”„ ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆãƒ†ã‚¹ãƒˆ: {test_case_name}")

        # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å–å¾—
        test_case = next(
            (case for case in self.test_cases if case['name'] == test_case_name), None)
        if not test_case:
            print(f"âŒ ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ '{test_case_name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}

        # è¨­å®šã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        result = self._run_adaptive_scheduling(
            test_case, gen_length, scheduler_config)

        print(f"âœ… ãƒ†ã‚¹ãƒˆå®Œäº†:")
        print(f"   å®Ÿè¡Œæ™‚é–“: {result['total_time']:.2f}ç§’")
        print(f"   NFE: {result['nfe']}")
        print(f"   ãƒ¢ãƒ¼ãƒ‰å¤‰æ›´: {result['mode_changes']}å›")
        print(f"   æœ€çµ‚ãƒ¢ãƒ¼ãƒ‰: {result['final_mode']}")
        print(f"   å¹³å‡ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {result['avg_block_size']:.1f}")
        print(f"   æœŸå¾…ãƒ¢ãƒ¼ãƒ‰: {test_case.get('expected_mode', 'æœªå®šç¾©')}")
        print(f"   ä½¿ç”¨è¨­å®š: {result['scheduler_config']}")

        # ä¿¡é ¼åº¦ãƒ—ãƒ­ãƒƒãƒˆã‚’ç”Ÿæˆ
        if plot_confidence and 'metrics' in result:
            print(f"\nğŸ“Š ä¿¡é ¼åº¦ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆä¸­...")
            self.plot_confidence_movement(result['metrics'], test_case_name,
                                          save_plots=True, scheduler_config=scheduler_config)

        return result

    def compare_parameter_settings(self, test_case_name: str = "complex_reasoning",
                                   gen_length: int = 64) -> None:
        """
        ç•°ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã‚’æ¯”è¼ƒ

        Args:
            test_case_name: ãƒ†ã‚¹ãƒˆã™ã‚‹ã‚±ãƒ¼ã‚¹å
            gen_length: ç”Ÿæˆé•·
        """
        print(f"\nğŸ” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šæ¯”è¼ƒ: {test_case_name}")

        # ç•°ãªã‚‹è¨­å®šã‚’å®šç¾©
        configurations = [
            {
                'name': 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ',
                'config': {
                    'to_quality_threshold': 0.80,
                    'to_efficiency_threshold': 0.95,
                    'confidence_window_size': 2
                }
            },
            {
                'name': 'ã‚¢ã‚°ãƒ¬ãƒƒã‚·ãƒ–',
                'config': {
                    'to_quality_threshold': 0.70,
                    'to_efficiency_threshold': 0.85,
                    'confidence_window_size': 1
                }
            },
            {
                'name': 'ä¿å®ˆçš„',
                'config': {
                    'to_quality_threshold': 0.90,
                    'to_efficiency_threshold': 0.98,
                    'confidence_window_size': 3
                }
            }
        ]

        results = {}

        for config_info in configurations:
            print(f"\nğŸ§ª ãƒ†ã‚¹ãƒˆä¸­: {config_info['name']}")

            # ãƒ™ãƒ¼ã‚¹è¨­å®šã‚’æ›´æ–°
            full_config = {
                **config_info['config'],
                'high_efficiency_params': {'block_size': 32, 'threshold': 0.75},
                'high_quality_params': {'block_size': 8, 'threshold': 0.95}
            }

            result = self.quick_mode_switching_test(
                test_case_name=test_case_name,
                gen_length=gen_length,
                scheduler_config=full_config,
                plot_confidence=False  # æ¯”è¼ƒæ™‚ã¯å€‹åˆ¥ãƒ—ãƒ­ãƒƒãƒˆã‚’ç„¡åŠ¹åŒ–
            )

            results[config_info['name']] = {
                'result': result,
                'config': full_config
            }

        # æ¯”è¼ƒçµæœã‚’è¡¨ç¤º
        print(f"\nğŸ“Š è¨­å®šæ¯”è¼ƒçµæœ:")
        print(f"{'è¨­å®š':<12} {'æ™‚é–“(s)':<10} {'NFE':<8} {'ãƒ¢ãƒ¼ãƒ‰å¤‰æ›´':<10} {'åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰(%)':<15}")
        print("-" * 65)

        for name, data in results.items():
            result = data['result']
            if 'metrics' in result:
                metrics = result['metrics']
                mode_history = metrics.get('mode_history', [])
                efficiency_percent = sum(1 for mode in mode_history if mode ==
                                         'HIGH_EFFICIENCY') / len(mode_history) * 100 if mode_history else 0

                print(
                    f"{name:<12} {result['total_time']:<10.2f} {result['nfe']:<8} {result['mode_changes']:<10} {efficiency_percent:<15.1f}")

        # çµ±åˆãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆï¼ˆã™ã¹ã¦ã®è¨­å®šã®ä¿¡é ¼åº¦ã‚’é‡ã­ã¦è¡¨ç¤ºï¼‰
        self._create_comparison_plot(results, test_case_name)

    def _create_comparison_plot(self, results: Dict, test_case_name: str) -> None:
        """
        è¤‡æ•°è¨­å®šã®æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ

        Args:
            results: æ¯”è¼ƒçµæœ
            test_case_name: ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹å
        """
        plt.figure(figsize=(15, 10))

        colors = ['blue', 'red', 'green', 'purple', 'orange']

        # ä¸Šæ®µ: ä¿¡é ¼åº¦æ¯”è¼ƒ
        plt.subplot(2, 1, 1)

        for i, (name, data) in enumerate(results.items()):
            result = data['result']
            config = data['config']

            if 'metrics' in result and 'confidence_history' in result['metrics']:
                confidence_history = result['metrics']['confidence_history']
                steps = range(len(confidence_history))

                plt.plot(steps, confidence_history,
                         color=colors[i % len(colors)], linewidth=2, label=f'{name}')

                # å„è¨­å®šã®é–¾å€¤ç·š
                quality_threshold = config.get('to_quality_threshold', 0.80)
                efficiency_threshold = config.get(
                    'to_efficiency_threshold', 0.95)

                plt.axhline(y=quality_threshold, color=colors[i % len(colors)],
                            linestyle='--', alpha=0.3, linewidth=1)
                plt.axhline(y=efficiency_threshold, color=colors[i % len(colors)],
                            linestyle=':', alpha=0.3, linewidth=1)

        plt.ylabel('Confidence', fontsize=12)
        plt.title(
            f'Confidence Comparison - {test_case_name}', fontsize=14, fontweight='bold')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.ylim(0, 1)

        # ä¸‹æ®µ: ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¯”è¼ƒ
        plt.subplot(2, 1, 2)

        for i, (name, data) in enumerate(results.items()):
            result = data['result']

            if 'metrics' in result and 'mode_history' in result['metrics']:
                mode_history = result['metrics']['mode_history']
                mode_numeric = [
                    1 if mode == 'HIGH_EFFICIENCY' else 0 for mode in mode_history]
                steps = range(len(mode_numeric))

                # ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®è¨­å®šã‚’è¡¨ç¤º
                offset = i * 0.2
                plt.plot(steps, [m + offset for m in mode_numeric],
                         color=colors[i % len(colors)], linewidth=2, label=f'{name}',
                         marker='o', markersize=3, alpha=0.7)

        plt.ylabel('Mode + Offset', fontsize=12)
        plt.xlabel('Generation Step', fontsize=12)
        plt.title('Mode Switching Comparison', fontsize=14, fontweight='bold')
        plt.legend()
        plt.grid(True, alpha=0.3)

        plt.tight_layout()

        # ãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜
        output_dir = Path("confidence_plots")
        output_dir.mkdir(exist_ok=True)

        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = output_dir / f"comparison_{test_case_name}_{timestamp}.png"
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š æ¯”è¼ƒãƒ—ãƒ­ãƒƒãƒˆã‚’ä¿å­˜: {filename}")

        plt.show()


def main():
    """
    ä½¿ç”¨ä¾‹:
    # åŸºæœ¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    python test_adaptive_scheduling.py --benchmark

    # ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶
    python test_adaptive_scheduling.py --ablation

    # ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼‰
    python test_adaptive_scheduling.py --quick-test complex_reasoning

    # ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆï¼ˆã‚«ã‚¹ã‚¿ãƒ è¨­å®šï¼‰
    python test_adaptive_scheduling.py --quick-test complex_reasoning --to-quality-threshold 0.75 --to-efficiency-threshold 0.90 --efficiency-block-size 16

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šæ¯”è¼ƒ
    python test_adaptive_scheduling.py --compare-settings complex_reasoning

    # ãƒ—ãƒ­ãƒƒãƒˆç„¡åŠ¹åŒ–
    python test_adaptive_scheduling.py --quick-test simple_qa --no-plot

    # å…¨è©•ä¾¡
    python test_adaptive_scheduling.py --comprehensive
    """
    parser = argparse.ArgumentParser(
        description="Adaptive Scheduling Test Suite")

    # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument(
        "--model", default="GSAI-ML/LLaDA-8B-Instruct", help="Model name")
    parser.add_argument("--gen-length", type=int,
                        default=128, help="Generation length")
    parser.add_argument("--device", default="auto", help="Device to use")

    # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
    parser.add_argument("--benchmark", action="store_true",
                        help="Run comprehensive benchmark")
    parser.add_argument("--ablation", action="store_true",
                        help="Run ablation study")
    parser.add_argument("--long-context", action="store_true",
                        help="Run long context evaluation")
    parser.add_argument("--comprehensive", action="store_true",
                        help="Run all evaluations")
    parser.add_argument("--quick-test", type=str, default=None,
                        help="Run quick mode switching test for specific case")
    parser.add_argument("--compare-settings", type=str, default=None,
                        help="Compare different parameter settings for specific case")

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    scheduler_group = parser.add_argument_group(
        'scheduler', 'Adaptive Scheduler Configuration')
    scheduler_group.add_argument("--to-quality-threshold", type=float, default=0.80,
                                 help="Confidence threshold to switch to HIGH_QUALITY mode (default: 0.80)")
    scheduler_group.add_argument("--to-efficiency-threshold", type=float, default=0.95,
                                 help="Confidence threshold to switch to HIGH_EFFICIENCY mode (default: 0.95)")
    scheduler_group.add_argument("--confidence-window-size", type=int, default=2,
                                 help="Window size for confidence smoothing (default: 2)")

    # åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    efficiency_group = parser.add_argument_group(
        'efficiency_mode', 'High-Efficiency Mode Parameters')
    efficiency_group.add_argument("--efficiency-block-size", type=int, default=32,
                                  help="Block size for HIGH_EFFICIENCY mode (default: 32)")
    efficiency_group.add_argument("--efficiency-threshold", type=float, default=0.75,
                                  help="Confidence threshold for HIGH_EFFICIENCY mode (default: 0.75)")

    # å“è³ªãƒ¢ãƒ¼ãƒ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    quality_group = parser.add_argument_group(
        'quality_mode', 'High-Quality Mode Parameters')
    quality_group.add_argument("--quality-block-size", type=int, default=8,
                               help="Block size for HIGH_QUALITY mode (default: 8)")
    quality_group.add_argument("--quality-threshold", type=float, default=0.95,
                               help="Confidence threshold for HIGH_QUALITY mode (default: 0.95)")

    # ãƒ—ãƒ­ãƒƒãƒˆè¨­å®š
    plot_group = parser.add_argument_group('plotting', 'Plotting Options')
    plot_group.add_argument("--no-plot", action="store_true",
                            help="Disable confidence plotting for quick test")
    plot_group.add_argument("--save-plots", action="store_true", default=True,
                            help="Save plots to files (default: True)")

    args = parser.parse_args()

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šã‚’æ§‹ç¯‰
    scheduler_config = {
        'to_quality_threshold': args.to_quality_threshold,
        'to_efficiency_threshold': args.to_efficiency_threshold,
        'confidence_window_size': args.confidence_window_size,
        'high_efficiency_params': {
            'block_size': args.efficiency_block_size,
            'threshold': args.efficiency_threshold
        },
        'high_quality_params': {
            'block_size': args.quality_block_size,
            'threshold': args.quality_threshold
        }
    }

    # è¨­å®šã®è¡¨ç¤º
    print(f"ğŸ”§ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š:")
    print(f"   åŠ¹ç‡â†’å“è³ªé–¾å€¤: {args.to_quality_threshold}")
    print(f"   å“è³ªâ†’åŠ¹ç‡é–¾å€¤: {args.to_efficiency_threshold}")
    print(f"   ä¿¡é ¼åº¦ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦: {args.confidence_window_size}")
    print(
        f"   åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰: ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º={args.efficiency_block_size}, é–¾å€¤={args.efficiency_threshold}")
    print(
        f"   å“è³ªãƒ¢ãƒ¼ãƒ‰: ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º={args.quality_block_size}, é–¾å€¤={args.quality_threshold}")

    # ãƒ†ã‚¹ã‚¿ãƒ¼ã®åˆæœŸåŒ–
    tester = AdaptiveSchedulingTester(
        model_name=args.model, device=args.device)

    if args.quick_test:
        print("\n" + "="*50)
        print("ğŸ”„ ã‚¯ã‚¤ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        print("="*50)
        quick_result = tester.quick_mode_switching_test(
            test_case_name=args.quick_test,
            gen_length=args.gen_length,
            scheduler_config=scheduler_config,
            plot_confidence=not args.no_plot
        )
        print("\nâœ… ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆå®Œäº†")
        return

    if args.compare_settings:
        print("\n" + "="*50)
        print("ğŸ” ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šæ¯”è¼ƒå®Ÿè¡Œ")
        print("="*50)
        tester.compare_parameter_settings(
            test_case_name=args.compare_settings,
            gen_length=args.gen_length
        )
        print("\nâœ… è¨­å®šæ¯”è¼ƒå®Œäº†")
        return

    if args.comprehensive or args.benchmark:
        print("\n" + "="*50)
        print("ğŸš€ åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
        print("="*50)
        benchmark_results = tester.run_comprehensive_benchmark(
            gen_length=args.gen_length,
            scheduler_config=scheduler_config)
        print("\nâœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†")

    if args.comprehensive or args.ablation:
        print("\n" + "="*50)
        print("ğŸ”¬ ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶å®Ÿè¡Œ")
        print("="*50)
        ablation_results = tester.run_ablation_study(
            gen_length=args.gen_length,
            base_scheduler_config=scheduler_config)
        print("\nâœ… ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶å®Œäº†")

    if args.comprehensive or args.long_context:
        print("\n" + "="*50)
        print("ğŸ“ é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè©•ä¾¡å®Ÿè¡Œ")
        print("="*50)
        long_context_results = tester.run_long_context_evaluation(
            scheduler_config=scheduler_config)
        print("\nâœ… é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè©•ä¾¡å®Œäº†")

    print("\nğŸ‰ å…¨è©•ä¾¡å®Œäº†!")


if __name__ == "__main__":
    main()
