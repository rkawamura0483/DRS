# Copyright 2025 NVIDIA CORPORATION & AFFILIATES
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0
# Modified from LLaDA repos: https://github.com/ML-GSAI/LLaDA

import os

import numpy as np
import torch
import torch.nn.functional as F
from model.modeling_llada import LLaDAModelLM
from tqdm import tqdm
from transformers import AutoModel, AutoTokenizer


def add_gumbel_noise(logits, temperature):
    '''
    The Gumbel max is a method for sampling categorical distributions.
    According to arXiv:2409.02908, for MDM, low-precision Gumbel Max improves perplexity score but reduces generation quality.
    Thus, we use float64.
    '''
    if temperature == 0:
        return logits
    logits = logits.to(torch.float64)
    noise = torch.rand_like(logits, dtype=torch.float64)
    gumbel_noise = (- torch.log(noise)) ** temperature
    return logits.exp() / gumbel_noise


def get_num_transfer_tokens(mask_index, steps):
    '''
    In the reverse process, the interval [0, 1] is uniformly discretized into steps intervals.
    Furthermore, because LLaDA employs a linear noise schedule (as defined in Eq. (8)),
    the expected number of tokens transitioned at each step should be consistent.

    This function is designed to precompute the number of tokens that need to be transitioned at each step.
    '''
    mask_num = mask_index.sum(dim=1, keepdim=True)

    base = mask_num // steps
    remainder = mask_num % steps

    num_transfer_tokens = torch.zeros(mask_num.size(
        0), steps, device=mask_index.device, dtype=torch.int64) + base

    for i in range(mask_num.size(0)):
        num_transfer_tokens[i, :remainder[i]] += 1

    return num_transfer_tokens


@torch.no_grad()
def generate(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,
             remasking='low_confidence', mask_id=None, threshold=None):
    '''
    Args:
        model: Mask predictor.
        prompt: A tensor of shape (1, L).
        steps: Sampling steps, less than or equal to gen_length.
        gen_length: Generated answer length.
        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.
        temperature: Categorical distribution sampling temperature.
        cfg_scale: Unsupervised classifier-free guidance scale.
        remasking: Remasking strategy. 'low_confidence' or 'random'.
        mask_id: The toke id of [MASK]. If None, will be obtained from model config.
    '''
    # mask_idã‚’é©åˆ‡ã«å–å¾—
    if mask_id is None:
        if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'mask_token_id') and model.tokenizer.mask_token_id is not None:
            mask_id = model.tokenizer.mask_token_id
        else:
            mask_id = model.config.mask_token_id

    x = torch.full((1, prompt.shape[1] + gen_length),
                   mask_id, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length

    assert steps % num_blocks == 0
    steps = steps // num_blocks

    nfe = 0
    for num_block in range(num_blocks):
        block_mask_index = (x[:, prompt.shape[1] + num_block *
                            block_length: prompt.shape[1] + (num_block + 1) * block_length] == mask_id)
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)
        i = 0
        while True:
            nfe += 1
            mask_index = (x == mask_id)
            logits = model(x).logits
            mask_index[:, prompt.shape[1] +
                       (num_block + 1) * block_length:] = 0
            x0, transfer_index = get_transfer_index(
                logits, temperature, remasking, mask_index, x, num_transfer_tokens[:, i] if threshold is None else None, threshold)
            x[transfer_index] = x0[transfer_index]
            i += 1
            if (x[:, prompt.shape[1] + num_block * block_length: prompt.shape[1] + (num_block + 1) * block_length] == mask_id).sum() == 0:
                break
    return x, nfe


@torch.no_grad()
def generate_with_prefix_cache(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,
                               remasking='low_confidence', mask_id=None, threshold=None):
    '''
    Args:
        model: Mask predictor.
        prompt: A tensor of shape (1, L).
        steps: Sampling steps, less than or equal to gen_length.
        gen_length: Generated answer length.
        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.
        temperature: Categorical distribution sampling temperature.
        cfg_scale: Unsupervised classifier-free guidance scale.
        remasking: Remasking strategy. 'low_confidence' or 'random'.
        mask_id: The toke id of [MASK]. If None, will be obtained from model config.
    '''
    # mask_idã‚’é©åˆ‡ã«å–å¾—
    if mask_id is None:
        if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'mask_token_id') and model.tokenizer.mask_token_id is not None:
            mask_id = model.tokenizer.mask_token_id
        else:
            mask_id = model.config.mask_token_id

    x = torch.full((1, prompt.shape[1] + gen_length),
                   mask_id, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length

    assert steps % num_blocks == 0
    steps = steps // num_blocks

    nfe = 0

    for num_block in range(num_blocks):
        current_block_start = prompt.shape[1] + num_block * block_length
        current_block_end = current_block_start + block_length

        block_mask_index = (
            x[:, current_block_start:current_block_end] == mask_id)
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)

        output = model(x, use_cache=True)
        past_key_values = output.past_key_values

        mask_index = (x == mask_id)
        mask_index[:, current_block_end:] = 0
        x0, transfer_index = get_transfer_index(
            output.logits, temperature, remasking, mask_index, x, num_transfer_tokens[:, 0] if threshold is None else None, threshold)
        x[transfer_index] = x0[transfer_index]

        new_past_key_values = []
        for i in range(len(past_key_values)):
            new_past_key_values.append(())
            for j in range(len(past_key_values[i])):
                new_past_key_values[i] += (past_key_values[i]
                                           [j][:, :, :current_block_start],)

        past_key_values = new_past_key_values
        nfe += 1

        i = 1
        while True:
            nfe += 1
            mask_index = (x[:, current_block_start:] == mask_id)
            mask_index[:, block_length:] = 0

            logits = model(x[:, current_block_start:],
                           past_key_values=past_key_values, use_cache=True).logits

            logits_with_noise = add_gumbel_noise(
                logits, temperature=temperature)
            x0 = torch.argmax(logits_with_noise, dim=-1)  # b, l

            x0, transfer_index = get_transfer_index(logits, temperature, remasking, mask_index,
                                                    x[:, current_block_start:], num_transfer_tokens[:, i] if threshold is None else None, threshold)
            x[:, current_block_start:][transfer_index] = x0[transfer_index]
            if (x[:, current_block_start:current_block_end] == mask_id).sum() == 0:
                break
            i += 1

    return x, nfe


@torch.no_grad()
def generate_with_dual_cache(model, prompt, steps=128, gen_length=128, block_length=128, temperature=0.,
                             remasking='low_confidence', mask_id=None, threshold=None):
    '''
    Args:
        model: Mask predictor.
        prompt: A tensor of shape (1, L).
        steps: Sampling steps, less than or equal to gen_length.
        gen_length: Generated answer length.
        block_length: Block length, less than or equal to gen_length. If less than gen_length, it means using semi_autoregressive remasking.
        temperature: Categorical distribution sampling temperature.
        cfg_scale: Unsupervised classifier-free guidance scale.
        remasking: Remasking strategy. 'low_confidence' or 'random'.
        mask_id: The toke id of [MASK]. If None, will be obtained from model config.
    '''
    # mask_idã‚’é©åˆ‡ã«å–å¾—
    if mask_id is None:
        if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'mask_token_id') and model.tokenizer.mask_token_id is not None:
            mask_id = model.tokenizer.mask_token_id
        else:
            mask_id = model.config.mask_token_id

    x = torch.full((1, prompt.shape[1] + gen_length),
                   mask_id, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length

    assert steps % num_blocks == 0
    steps = steps // num_blocks

    nfe = 0
    for num_block in range(num_blocks):
        current_block_start = prompt.shape[1] + num_block * block_length
        current_block_end = current_block_start + block_length

        block_mask_index = (
            x[:, current_block_start:current_block_end] == mask_id)
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)

        # cache init and update
        output = model(x, use_cache=True)
        past_key_values = output.past_key_values
        mask_index = (x == mask_id)
        mask_index[:, current_block_end:] = 0
        x0, transfer_index = get_transfer_index(
            output.logits, temperature, remasking, mask_index, x, num_transfer_tokens[:, 0] if threshold is None else None, threshold)
        x[transfer_index] = x0[transfer_index]
        nfe += 1

        i = 1
        replace_position = torch.zeros_like(x, dtype=torch.bool)
        replace_position[:, current_block_start:current_block_end] = 1
        while True:
            nfe += 1
            mask_index = (
                x[:, current_block_start:current_block_end] == mask_id)
            # cache position is the position between current_block_start and current_block_end
            logits = model(x[:, current_block_start:current_block_end], past_key_values=past_key_values,
                           use_cache=True, replace_position=replace_position).logits

            x0, transfer_index = get_transfer_index(logits, temperature, remasking, mask_index,
                                                    x[:, current_block_start:current_block_end], num_transfer_tokens[:, i] if threshold is None else None, threshold)
            x[:, current_block_start:current_block_end][transfer_index] = x0[transfer_index]
            if (x[:, current_block_start:current_block_end] == mask_id).sum() == 0:
                break
            i += 1

    return x, nfe


def get_transfer_index(logits, temperature, remasking, mask_index, x, num_transfer_tokens, threshold=None):
    logits_with_noise = add_gumbel_noise(logits, temperature=temperature)
    x0 = torch.argmax(logits_with_noise, dim=-1)  # b, l

    if remasking == 'low_confidence':
        p = F.softmax(logits.to(torch.float64), dim=-1)
        x0_p = torch.squeeze(
            torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1)  # b, l
    elif remasking == 'random':
        x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)
    else:
        raise NotImplementedError(remasking)

    x0 = torch.where(mask_index, x0, x)
    confidence = torch.where(mask_index, x0_p, -np.inf)

    transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)
    if threshold is not None:
        num_transfer_tokens = mask_index.sum(dim=1, keepdim=True)
    for j in range(confidence.shape[0]):
        _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j])
        transfer_index[j, select_index] = True
        if threshold is not None:
            for k in range(1, num_transfer_tokens[j]):
                if confidence[j, select_index[k]] < threshold:
                    transfer_index[j, select_index[k]] = False
    return x0, transfer_index


def get_transfer_index_with_confidence(logits, temperature, remasking, mask_index, x, num_transfer_tokens):
    """
    Modified version of get_transfer_index that also returns confidence scores.

    Returns:
        (x0, transfer_index, confidence_scores)
    """
    logits_with_noise = add_gumbel_noise(logits, temperature=temperature)
    x0 = torch.argmax(logits_with_noise, dim=-1)

    if remasking == 'low_confidence':
        p = F.softmax(logits.to(torch.float64), dim=-1)
        x0_p = torch.squeeze(
            torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1)
    elif remasking == 'random':
        x0_p = torch.rand((x0.shape[0], x0.shape[1]), device=x0.device)
    else:
        raise NotImplementedError(remasking)

    x0 = torch.where(mask_index, x0, x)
    confidence = torch.where(mask_index, x0_p, -np.inf)

    transfer_index = torch.zeros_like(x0, dtype=torch.bool, device=x0.device)
    for j in range(confidence.shape[0]):
        _, select_index = torch.topk(confidence[j], k=num_transfer_tokens[j])
        transfer_index[j, select_index] = True

    return x0, transfer_index, x0_p


def calculate_block_ambiguity_improved(confidence_scores, threshold, mask_id, remaining_masks):
    """
    æ”¹å–„ã•ã‚ŒãŸæ›–æ˜§åº¦è¨ˆç®—: å®Œæˆãƒ–ãƒ­ãƒƒã‚¯ã§ã‚‚ä¿¡é ¼åº¦æƒ…å ±ã‚’æ´»ç”¨

    Args:
        confidence_scores: Tensor of confidence scores for block tokens
        threshold: Confidence threshold Ï„
        mask_id: Mask token ID to exclude from calculation
        remaining_masks: Number of remaining mask tokens

    Returns:
        Ambiguity score (float): fraction of tokens below threshold
    """
    # ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãªã„æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã®ä¿¡é ¼åº¦ã®ã¿ã‚’è€ƒæ…®
    valid_scores = confidence_scores[confidence_scores != -np.inf]
    if len(valid_scores) == 0:
        return 0.0

    # é–¾å€¤æœªæº€ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å‰²åˆã‚’è¨ˆç®—
    low_confidence_tokens = (valid_scores < threshold).float()
    ambiguity_score = low_confidence_tokens.mean().item()

    # ğŸ”‘ é‡è¦ä¿®æ­£: å®Œæˆãƒ–ãƒ­ãƒƒã‚¯ã§ã‚‚å®Ÿéš›ã®ä¿¡é ¼åº¦æƒ…å ±ã‚’ä½¿ç”¨
    # ï¼ˆå¾“æ¥ã®å¼·åˆ¶0è¨­å®šã‚’å‰Šé™¤ï¼‰
    return ambiguity_score


def allocate_refinement_budget(block_ambiguities, total_refinement_budget):
    """
    Allocate refinement budget proportionally to block ambiguity scores.

    Args:
        block_ambiguities: List of ambiguity scores per block
        total_refinement_budget: Total additional steps to distribute

    Returns:
        List of additional steps per block
    """
    total_ambiguity = sum(block_ambiguities)

    if total_ambiguity == 0:
        # No ambiguity, distribute equally
        steps_per_block = total_refinement_budget // len(block_ambiguities)
        return [steps_per_block] * len(block_ambiguities)

    # Proportional allocation
    additional_steps = []
    allocated_total = 0

    for i, ambiguity in enumerate(block_ambiguities):
        if i == len(block_ambiguities) - 1:
            # Last block gets remaining budget
            steps = total_refinement_budget - allocated_total
        else:
            # Proportional allocation
            steps = int((ambiguity / total_ambiguity)
                        * total_refinement_budget)
            allocated_total += steps

        additional_steps.append(max(0, steps))

    return additional_steps


@torch.no_grad()
def generate_with_drs_improved(model, prompt, steps=128, gen_length=128, block_length=128,
                               temperature=0., remasking='low_confidence', mask_id=None,
                               threshold=0.8, t_base=8):
    """
    æ”¹å–„ç‰ˆDRS: ä¿¡é ¼åº¦æƒ…å ±ã‚’é©åˆ‡ã«ä¿å­˜ãƒ»æ´»ç”¨

    ä¿®æ­£ç‚¹:
    1. å®Œæˆå‰ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜
    2. å®Œæˆå¾Œã§ã‚‚å®Ÿéš›ã®æ›–æ˜§åº¦ã‚’è¨ˆç®—
    3. çœŸã®å‹•çš„é…åˆ†ä¾¡å€¤ã‚’æ¤œè¨¼å¯èƒ½
    4. ãƒªãƒ•ã‚¡ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆãƒ­ã‚¸ãƒƒã‚¯ã‚’çµ±ä¸€ã—ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ´»ç”¨ã—ã¦åŠ¹ç‡åŒ–
    """
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‹ã‚‰ãƒã‚¹ã‚¯IDã‚’å–å¾—ï¼ˆæ¨å¥¨ï¼‰
    if mask_id is None:
        if hasattr(model, 'tokenizer') and model.tokenizer.mask_token_id is not None:
            mask_id = model.tokenizer.mask_token_id
        else:
            mask_id = model.config.mask_token_id
            print(f"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®mask_token_idãŒNoneã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«è¨­å®šã‹ã‚‰å–å¾—: {mask_id}")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦126336ã‚’ç¢ºä¿ï¼ˆLLaDAã®æ­£å¼ãªãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³IDï¼‰
    if mask_id is None:
        mask_id = 126336
        print(f"ãƒ¢ãƒ‡ãƒ«è¨­å®šã§ã‚‚Noneã®ãŸã‚ã€LLaDAãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨: {mask_id}")

    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒã‚¹ã‚¯ã§åˆæœŸåŒ–
    x = torch.full((1, prompt.shape[1] + gen_length),
                   mask_id, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length

    # Phase 1: ç²—ã„åˆæœŸãƒ‘ã‚¹ï¼ˆä¿¡é ¼åº¦æƒ…å ±ã‚’é©åˆ‡ã«ä¿å­˜ï¼‰
    block_confidences = []
    block_remaining_masks = []
    block_confidence_histories = []
    nfe = 0

    print(f"Phase 1: æ”¹å–„ç‰ˆåˆæœŸãƒ‘ã‚¹ - {t_base}ã‚¹ãƒ†ãƒƒãƒ— x {num_blocks}ãƒ–ãƒ­ãƒƒã‚¯")
    for num_block in range(num_blocks):
        current_block_start = prompt.shape[1] + num_block * block_length
        current_block_end = current_block_start + block_length

        # å„ãƒ–ãƒ­ãƒƒã‚¯ã§t_baseã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œ
        block_mask_index = (
            x[:, current_block_start:current_block_end] == mask_id)
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, t_base)

        # ğŸ”‘ ä¿®æ­£: å…ƒã®generate_with_dual_cacheã¨åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¡ç”¨
        # æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—: ãƒ•ãƒ«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã—
        output = model(x, use_cache=True)
        past_key_values = output.past_key_values
        mask_index = (x == mask_id)
        mask_index[:, current_block_end:] = 0
        x0, transfer_index, confidence_scores_initial = get_transfer_index_with_confidence(
            output.logits, temperature, remasking, mask_index, x, num_transfer_tokens[:, 0] if len(num_transfer_tokens[0]) > 0 else None)
        x[transfer_index] = x0[transfer_index]
        nfe += 1

        block_confidence_scores = None
        confidence_history = []  # ã“ã®ãƒ–ãƒ­ãƒƒã‚¯ã®ä¿¡é ¼åº¦å±¥æ­´

        # æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã®ä¿¡é ¼åº¦ã‚’è¨˜éŒ²
        if confidence_scores_initial is not None:
            step_confidence = confidence_scores_initial[0,
                                                        current_block_start:current_block_end]
            confidence_history.append(step_confidence.clone())

        # æ®‹ã‚Šã®ã‚¹ãƒ†ãƒƒãƒ—: ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§ã®å‡¦ç†
        replace_position = torch.zeros_like(x, dtype=torch.bool)
        replace_position[:, current_block_start:current_block_end] = 1

        for i in range(1, t_base):
            nfe += 1
            mask_index_block = (
                x[:, current_block_start:current_block_end] == mask_id)

            if mask_index_block.sum() == 0:
                break  # ãƒ–ãƒ­ãƒƒã‚¯ãŒå®Œæˆ

            logits = model(x[:, current_block_start:current_block_end], past_key_values=past_key_values,
                           use_cache=True, replace_position=replace_position).logits

            x0_block, transfer_index_block, confidence_scores_block = get_transfer_index_with_confidence(
                logits, temperature, remasking, mask_index_block, x[:, current_block_start:current_block_end], num_transfer_tokens[:, i] if i < len(num_transfer_tokens[0]) else None)

            x[:, current_block_start:current_block_end][transfer_index_block] = x0_block[transfer_index_block]

            # ä¿¡é ¼åº¦ã‚’è¨˜éŒ²
            step_confidence = confidence_scores_block[0]
            confidence_history.append(step_confidence.clone())
            block_confidence_scores = step_confidence

        # ãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†å¾Œã€æ¬¡ã®ãƒ–ãƒ­ãƒƒã‚¯ã®ãŸã‚ã«KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
        if num_block < num_blocks - 1:  # æœ€å¾Œã®ãƒ–ãƒ­ãƒƒã‚¯ã§ãªã„å ´åˆ
            output = model(x[:, :current_block_end], use_cache=True)
            past_key_values = output.past_key_values

        # æ®‹ã‚Šãƒã‚¹ã‚¯æ•°ã‚’è¨˜éŒ²
        remaining_masks = (
            x[:, current_block_start:current_block_end] == mask_id).sum().item()
        block_remaining_masks.append(remaining_masks)
        block_confidence_histories.append(confidence_history)

        # ğŸ”‘ æ”¹å–„ã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯æ›–æ˜§åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        if block_confidence_scores is not None:
            ambiguity_score = calculate_block_ambiguity_improved(
                block_confidence_scores, threshold, mask_id, remaining_masks)
        else:
            ambiguity_score = 1.0 if remaining_masks > 0 else 0.0

        block_confidences.append(ambiguity_score)

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
        if block_confidence_scores is not None:
            valid_scores = block_confidence_scores[block_confidence_scores != -
                                                   np.inf]
            if len(valid_scores) > 0:
                below_threshold = (valid_scores < threshold).sum().item()
                print(f"ãƒ–ãƒ­ãƒƒã‚¯ {num_block}: æ®‹ã‚Šãƒã‚¹ã‚¯={remaining_masks}, "
                      f"ä¿¡é ¼åº¦ç¯„å›²=[{valid_scores.min():.3f}, {valid_scores.max():.3f}], "
                      f"æ›–æ˜§åº¦ã‚¹ã‚³ã‚¢={ambiguity_score:.3f} "
                      f"(é–¾å€¤æœªæº€: {below_threshold}/{len(valid_scores)})")
            else:
                print(
                    f"ãƒ–ãƒ­ãƒƒã‚¯ {num_block}: æ®‹ã‚Šãƒã‚¹ã‚¯={remaining_masks}, ä¿¡é ¼åº¦æƒ…å ±ãªã—, æ›–æ˜§åº¦ã‚¹ã‚³ã‚¢={ambiguity_score:.3f}")
        else:
            print(f"ãƒ–ãƒ­ãƒƒã‚¯ {num_block}: ä¿¡é ¼åº¦æƒ…å ±ãªã—, æ›–æ˜§åº¦ã‚¹ã‚³ã‚¢=0.0")

    # Phase 2: å‹•çš„äºˆç®—å†é…åˆ†ï¼ˆæ”¹å–„ç‰ˆï¼‰
    t_used_base = t_base * num_blocks
    t_refine = max(0, steps - t_used_base)

    print(f"\nPhase 2: æ”¹å–„ç‰ˆå‹•çš„äºˆç®—å†é…åˆ†")
    print(f"  ä½¿ç”¨æ¸ˆã¿äºˆç®—: {nfe}")
    print(f"  æ®‹ã‚Šäºˆç®—: {t_refine}")
    print(f"  ãƒ–ãƒ­ãƒƒã‚¯æ›–æ˜§åº¦ã‚¹ã‚³ã‚¢: {[f'{s:.3f}' for s in block_confidences]}")
    print(f"  ãƒ–ãƒ­ãƒƒã‚¯æ®‹ã‚Šãƒã‚¹ã‚¯æ•°: {block_remaining_masks}")

    if t_refine <= 0:
        print(f"  â†’ äºˆç®—ä¸è¶³ã€‚ç¾åœ¨ã®çŠ¶æ…‹ã§çµ‚äº†")
        return x, nfe, block_confidences

    # æ›–æ˜§åº¦ã«åŸºã¥ãè¿½åŠ ã‚¹ãƒ†ãƒƒãƒ—é…åˆ†
    additional_steps = allocate_refinement_budget(block_confidences, t_refine)
    print(f"  â†’ è¿½åŠ ã‚¹ãƒ†ãƒƒãƒ—é…åˆ†: {additional_steps}")

    # Phase 3: æ¨™çš„ç²¾éŒ¬ï¼ˆçµ±ä¸€ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    # ğŸ”‘ ä¸€æ™‚çš„ã«ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ: ã‚¨ãƒ©ãƒ¼åŸå› ã®ç‰¹å®šã®ãŸã‚
    """
    if any(steps > 0 for steps in additional_steps):
        print(f"\nPhase 3: æ”¹å–„ç‰ˆæ¨™çš„ç²¾éŒ¬é–‹å§‹")

        for num_block, steps_to_add in enumerate(additional_steps):
            if steps_to_add == 0:
                continue

            current_block_start = prompt.shape[1] + num_block * block_length
            current_block_end = current_block_start + block_length

            # --- æ–°ã—ã„çµ±ä¸€ãƒªãƒ•ã‚¡ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆãƒ­ã‚¸ãƒƒã‚¯ ---
            # Step 1: ç²¾éŒ¬å¯¾è±¡ã®ãƒã‚¹ã‚¯ã‚’æ±ºå®š
            block_x = x[:, current_block_start:current_block_end]
            is_masked = (block_x == mask_id)

            if is_masked.sum().item() == 0:
                # å®Œæˆãƒ–ãƒ­ãƒƒã‚¯ã ãŒæ›–æ˜§åº¦ãŒé«˜ã„å ´åˆã€ä½ä¿¡é ¼åº¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å†ãƒã‚¹ã‚¯
                print(f"  ç²¾éŒ¬ç†ç”±: ãƒ–ãƒ­ãƒƒã‚¯ {num_block} ã®ä¿¡é ¼åº¦ãƒ™ãƒ¼ã‚¹å“è³ªå‘ä¸Š")
                # æœ€æ–°ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å–å¾—
                output = model(x[:, :current_block_start], use_cache=True)
                past_key_values_refine = output.past_key_values
                replace_pos_refine = torch.zeros_like(x, dtype=torch.bool)
                replace_pos_refine[:, current_block_start:current_block_end] = 1

                logits = model(x[:, current_block_start:current_block_end], past_key_values=past_key_values_refine,
                               use_cache=True, replace_position=replace_pos_refine).logits
                nfe += 1

                p = F.softmax(logits.to(torch.float64), dim=-1)
                current_tokens = x[:, current_block_start:current_block_end]
                current_confidence = torch.gather(
                    p, dim=-1, index=current_tokens.unsqueeze(-1)).squeeze(-1)

                low_conf_mask = current_confidence < threshold
                if low_conf_mask.sum().item() == 0:
                    print(
                        f"    ãƒ–ãƒ­ãƒƒã‚¯ {num_block} ã¯ä¿¡é ¼åº¦ãŒé«˜ã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ (æœ€ä½ä¿¡é ¼åº¦: {current_confidence.min().item():.3f})ã€‚")
                    continue

                x[:, current_block_start:current_block_end][low_conf_mask] = mask_id
                print(
                    f"    â†’ ãƒ–ãƒ­ãƒƒã‚¯ {num_block} ã® {low_conf_mask.sum().item()} å€‹ã®ä½ä¿¡é ¼åº¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å†ãƒã‚¹ã‚¯ã—ã¾ã—ãŸã€‚")
            else:
                print(f"  ç²¾éŒ¬ç†ç”±: ãƒ–ãƒ­ãƒƒã‚¯ {num_block} ã®æ®‹å­˜ãƒã‚¹ã‚¯ã®å“è³ªå‘ä¸Š")

            # Step 2: ãƒã‚¹ã‚¯ã‚’å«ã‚€ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç²¾éŒ¬
            block_mask_index = (
                x[:, current_block_start:current_block_end] == mask_id)
            if block_mask_index.sum().item() == 0:
                continue

            num_transfer_tokens_refine = get_num_transfer_tokens(
                block_mask_index, steps_to_add)

            print(
                f"  â†’ ãƒ–ãƒ­ãƒƒã‚¯ {num_block} ã‚’ {steps_to_add} ã‚¹ãƒ†ãƒƒãƒ—ã§ç²¾éŒ¬...")

            # ãƒªãƒ•ã‚¡ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã®ãŸã‚ã®KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æº–å‚™
            output = model(x[:, :current_block_start], use_cache=True)
            past_key_values_refine = output.past_key_values

            for i in range(steps_to_add):
                if (x[:, current_block_start:current_block_end] == mask_id).sum().item() == 0:
                    print(f"    ãƒ–ãƒ­ãƒƒã‚¯ {num_block} ã®ç²¾éŒ¬å®Œäº† (æ—©æœŸçµ‚äº†)")
                    break

                nfe += 1
                replace_pos_refine = torch.zeros_like(x, dtype=torch.bool)
                replace_pos_refine[:, current_block_start:current_block_end] = 1

                logits = model(x[:, current_block_start:current_block_end], past_key_values=past_key_values_refine,
                               use_cache=True, replace_position=replace_pos_refine).logits

                refine_mask_index = (
                    x[:, current_block_start:current_block_end] == mask_id)

                x0_block, transfer_index_block, _ = get_transfer_index_with_confidence(
                    logits, temperature, remasking, refine_mask_index, x[:, current_block_start:current_block_end], num_transfer_tokens_refine[:, i])

                x[:, current_block_start:current_block_end][transfer_index_block] = x0_block[transfer_index_block]
    """
    print(f"\nPhase 3: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ï¼‰")

    # æœ€çµ‚çµæœ
    final_masks = (x[:, prompt.shape[1]:] == mask_id).sum().item()
    completion_rate = ((gen_length - final_masks) / gen_length) * 100

    print(f"\næ”¹å–„ç‰ˆDRSç”Ÿæˆå®Œäº†:")
    print(f"  ç·NFE: {nfe}/{steps} ({(nfe/steps*100):.1f}%)")
    print(
        f"  å®Œæˆç‡: {completion_rate:.1f}% ({gen_length - final_masks}/{gen_length})")
    print(f"  æ¤œå‡ºã•ã‚ŒãŸçœŸã®æ›–æ˜§åº¦: {any(score > 0.01 for score in block_confidences)}")
    print(f"  æ›–æ˜§åº¦åˆ†æ•£: {np.var(block_confidences):.3f}")

    return x, nfe, block_confidences


# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã€æ—¢å­˜ã®é–¢æ•°åã‚’ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¨ã—ã¦æ®‹ã™
def generate_with_drs_fixed(model, prompt, steps=128, gen_length=128, block_length=128,
                            temperature=0., remasking='low_confidence', mask_id=None,
                            threshold=0.8, t_base=8):
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ - æ–°ã—ã„ç ”ç©¶ç‰ˆã‚’å‘¼ã³å‡ºã—"""
    return generate_with_drs_improved(model, prompt, steps, gen_length, block_length,
                                      temperature, remasking, mask_id, threshold, t_base)


def main():
    device = 'cuda'

    model = LLaDAModelLM.from_pretrained(
        'GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True, torch_dtype=torch.bfloat16).to(device).eval()
    tokenizer = AutoTokenizer.from_pretrained(
        'GSAI-ML/LLaDA-8B-Instruct', trust_remote_code=True)
    model.tokenizer = tokenizer  # ãƒ¢ãƒ‡ãƒ«ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ã‚»ãƒƒãƒˆ

    # mask_idã‚’ç¢ºå®Ÿã«å–å¾—
    mask_id = tokenizer.mask_token_id
    if mask_id is None:
        mask_id = model.config.mask_token_id
        print(f"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®mask_token_idãŒNoneã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«è¨­å®šã‹ã‚‰å–å¾—: {mask_id}")

    prompt = "Lily can run 12 kilometers per hour for 4 hours. After that, she runs 6 kilometers per hour. How many kilometers can she run in 8 hours?"

    # Add special tokens for the Instruct model. The Base model does not require the following two lines.
    m = [{"role": "user", "content": prompt}, ]
    prompt = tokenizer.apply_chat_template(
        m, add_generation_prompt=True, tokenize=False)

    input_ids = tokenizer(prompt)['input_ids']
    input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)

    out, nfe = generate_with_dual_cache(model, input_ids, steps=128, gen_length=128,
                                        block_length=32, temperature=0., remasking='low_confidence', mask_id=mask_id)
    print(tokenizer.batch_decode(
        out[0][:, input_ids.shape[1]:], skip_special_tokens=True)[0])


if __name__ == '__main__':
    main()
