# å¤§å¹…æ”¹å–„ç‰ˆDRSæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  - è©•ä¾¡ãƒã‚¤ã‚¢ã‚¹ä¿®æ­£ç‰ˆ
import logging
import re
from collections import Counter

import numpy as np
import torch
import torch.nn.functional as F
from generate import generate_with_drs, generate_with_dual_cache
from model.modeling_llada import LLaDAModelLM
from transformers import AutoTokenizer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnhancedQualityEvaluator:
    """å¤§å¹…æ”¹å–„ã•ã‚ŒãŸå“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒã‚¤ã‚¢ã‚¹ä¿®æ­£ç‰ˆ"""

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def evaluate_text_coherence(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆä¸€è²«æ€§ã®è©•ä¾¡ï¼ˆç‹¬ç«‹æŒ‡æ¨™ï¼‰"""
        if not text or len(text) < 10:
            return 0.0

        # æ–‡ã®æ§‹é€ è©•ä¾¡
        sentences = re.split(r'[.!?]+', text)
        valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 3]

        if len(valid_sentences) == 0:
            return 0.0

        # å¹³å‡æ–‡é•·ã®å¦¥å½“æ€§
        avg_sentence_length = np.mean(
            [len(s.split()) for s in valid_sentences])
        length_score = min(1.0, avg_sentence_length / 15)  # é©åº¦ãªæ–‡é•·ã‚’è©•ä¾¡

        # èªå½™å¤šæ§˜æ€§
        words = text.lower().split()
        unique_words = set(words)
        diversity_score = len(unique_words) / max(len(words), 1)

        return 0.6 * length_score + 0.4 * diversity_score

    def evaluate_content_completeness(self, prompt, text):
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å›ç­”å®Œå…¨æ€§ã®è©•ä¾¡"""
        prompt_lower = prompt.lower()
        text_lower = text.lower()

        if "rectangular" in prompt_lower and "garden" in prompt_lower:
            # é•·æ–¹å½¢ã®åº­å•é¡Œ
            required_elements = ["area", "length", "width", "15", "8", "120"]
            found = sum(1 for elem in required_elements if elem in text_lower)
            return found / len(required_elements)

        elif "python" in prompt_lower and "function" in prompt_lower and "factorial" in prompt_lower:
            # éšä¹—é–¢æ•°å•é¡Œ
            required_elements = ["def", "factorial",
                                 "return", "recursion", "if", "else"]
            found = sum(1 for elem in required_elements if elem in text_lower)
            return found / len(required_elements)

        # ä¸€èˆ¬çš„ãªå®Œå…¨æ€§è©•ä¾¡
        words_in_prompt = set(prompt_lower.split())
        words_in_text = set(text_lower.split())
        coverage = len(words_in_prompt & words_in_text) / \
            max(len(words_in_prompt), 1)
        return min(1.0, coverage * 2)  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸

    def evaluate_mathematical_accuracy(self, prompt, text):
        """æ•°å­¦çš„æ­£ç¢ºæ€§ã®è©•ä¾¡"""
        if "rectangular" in prompt.lower() and "15" in prompt and "8" in prompt:
            # æ­£ç¢ºãªç­”ãˆï¼š120å¹³æ–¹ãƒ¡ãƒ¼ãƒˆãƒ«
            if "120" in text:
                return 1.0
            elif any(calc in text.lower() for calc in ["15 * 8", "15*8", "8 * 15", "8*15"]):
                return 0.8  # è¨ˆç®—å¼ã¯æ­£ã—ã„
            elif "area" in text.lower() and ("length" in text.lower() or "width" in text.lower()):
                return 0.5  # æ¦‚å¿µã¯ç†è§£ã—ã¦ã„ã‚‹
            else:
                return 0.0
        return 0.5  # æ•°å­¦å•é¡Œã§ãªã„å ´åˆã¯ä¸­ç«‹

    def comprehensive_quality_assessment(self, prompt, text):
        """åŒ…æ‹¬çš„å“è³ªè©•ä¾¡ï¼ˆç‹¬ç«‹æŒ‡æ¨™ï¼‰"""
        coherence = self.evaluate_text_coherence(text)
        completeness = self.evaluate_content_completeness(prompt, text)
        math_accuracy = self.evaluate_mathematical_accuracy(prompt, text)

        # é‡ã¿ä»˜ãçµ±åˆã‚¹ã‚³ã‚¢
        final_score = 0.4 * coherence + 0.4 * completeness + 0.2 * math_accuracy

        return final_score, {
            'coherence': coherence,
            'completeness': completeness,
            'math_accuracy': math_accuracy
        }

    def evaluate_efficiency_quality_tradeoff(self, baseline_nfe, drs_nfe, baseline_quality, drs_quality):
        """åŠ¹ç‡æ€§ã¨å“è³ªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•è©•ä¾¡"""
        nfe_ratio = drs_nfe / max(baseline_nfe, 1)
        quality_gain = drs_quality - baseline_quality

        # åŠ¹ç‡æ€§ã‚¹ã‚³ã‚¢ï¼ˆNFEå‰Šæ¸›ã‚’è©•ä¾¡ï¼‰
        efficiency_score = max(0, 2 - nfe_ratio)  # NFEåŠæ¸›ã§æœ€é«˜è©•ä¾¡

        # å“è³ªå‘ä¸Šã‚¹ã‚³ã‚¢
        quality_improvement_score = max(0, quality_gain * 10)  # 0.1ã®æ”¹å–„ã§1ç‚¹

        # çµ±åˆãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚¹ã‚³ã‚¢
        if quality_gain > 0.02:  # 2%ä»¥ä¸Šã®å“è³ªå‘ä¸Š
            if nfe_ratio <= 1.1:  # NFEå¢—åŠ 10%ä»¥å†…
                return "EXCELLENT"
            elif nfe_ratio <= 1.2:  # NFEå¢—åŠ 20%ä»¥å†…
                return "GOOD"
            else:
                return "MODERATE"
        elif quality_gain > -0.02:  # å“è³ªç¶­æŒï¼ˆÂ±2%ï¼‰
            if nfe_ratio <= 0.9:  # NFEå‰Šæ¸›10%ä»¥ä¸Š
                return "GOOD"
            elif nfe_ratio <= 1.0:  # NFEç¶­æŒ
                return "MODERATE"
            else:
                return "POOR"
        else:  # å“è³ªä½ä¸‹
            return "POOR"


def enhanced_drs_validation():
    """æ”¹å–„ç‰ˆDRSä»®èª¬æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”¬ æ”¹å–„ç‰ˆDRSä»®èª¬æ¤œè¨¼é–‹å§‹ (ãƒ‡ãƒã‚¤ã‚¹: {device})")

    # ã‚ˆã‚Šå¤šæ§˜ãªãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    test_prompts = [
        {
            "text": "Calculate the area of a rectangular garden with length 15 meters and width 8 meters.",
            "type": "math",
            "expected_answer": "120"
        },
        {
            "text": "Write a Python function to find the factorial of a number using recursion.",
            "type": "code",
            "expected_elements": ["def", "factorial", "recursion"]
        },
        {
            "text": "Explain the concept of photosynthesis in plants.",
            "type": "explanation",
            "expected_elements": ["chlorophyll", "sunlight", "carbon dioxide"]
        }
    ]

    try:
        print("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
        model = LLaDAModelLM.from_pretrained(
            'GSAI-ML/LLaDA-8B-Instruct',
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        ).to(device).eval()

        tokenizer = AutoTokenizer.from_pretrained(
            'GSAI-ML/LLaDA-8B-Instruct',
            trust_remote_code=True
        )
        model.tokenizer = tokenizer

        mask_id = tokenizer.mask_token_id or model.config.mask_token_id or 126336
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† (mask_id={mask_id})")

        evaluator = EnhancedQualityEvaluator(tokenizer)

        gen_length = 128
        block_length = 32
        total_steps = 128

        all_results = []

        for i, prompt_data in enumerate(test_prompts):
            prompt = prompt_data["text"]
            print(f"\n{'='*80}")
            print(f"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i+1} ({prompt_data['type']}): {prompt}")
            print(f"{'='*80}")

            m = [{"role": "user", "content": prompt}]
            prompt_formatted = tokenizer.apply_chat_template(
                m, add_generation_prompt=True, tokenize=False)
            input_ids = tokenizer(prompt_formatted)['input_ids']
            input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)

            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç”Ÿæˆ
            print("\nğŸ¯ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç”Ÿæˆä¸­...")
            baseline_out, baseline_nfe = generate_with_dual_cache(
                model, input_ids, steps=total_steps, gen_length=gen_length,
                block_length=block_length, temperature=0., remasking='low_confidence',
                mask_id=mask_id
            )
            baseline_text = tokenizer.batch_decode(
                baseline_out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]

            baseline_quality, baseline_details = evaluator.comprehensive_quality_assessment(
                prompt, baseline_text)

            # DRSè¨­å®šã‚’ã‚ˆã‚Šå¹…åºƒããƒ†ã‚¹ãƒˆ
            drs_configs = [
                {'t_base': 4, 'threshold': 0.7, 'name': 'DRS-Aggressive'},
                {'t_base': 8, 'threshold': 0.8, 'name': 'DRS-Balanced'},
                {'t_base': 12, 'threshold': 0.9, 'name': 'DRS-Conservative'},
            ]

            for config in drs_configs:
                print(f"\n{'-'*60}")
                print(
                    f"ğŸ§ª {config['name']} (t_base={config['t_base']}, threshold={config['threshold']})")
                print(f"{'-'*60}")

                # DRSç”Ÿæˆ
                drs_out, drs_nfe, ambiguity_scores = generate_with_drs(
                    model, input_ids, steps=total_steps, gen_length=gen_length,
                    block_length=block_length, temperature=0.,
                    threshold=config['threshold'], t_base=config['t_base'],
                    mask_id=mask_id
                )
                drs_text = tokenizer.batch_decode(
                    drs_out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]

                # ç‹¬ç«‹å“è³ªè©•ä¾¡
                drs_quality, drs_details = evaluator.comprehensive_quality_assessment(
                    prompt, drs_text)

                # ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•è©•ä¾¡
                tradeoff_rating = evaluator.evaluate_efficiency_quality_tradeoff(
                    baseline_nfe, drs_nfe, baseline_quality, drs_quality
                )

                # è©³ç´°å‡ºåŠ›
                print(f"\nğŸ“Š çµæœæ¯”è¼ƒ:")
                print(f"  ğŸ¯ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:")
                print(f"     NFE: {baseline_nfe}")
                print(
                    f"     å“è³ª: {baseline_quality:.3f} (coherence:{baseline_details['coherence']:.3f}, completeness:{baseline_details['completeness']:.3f}, math:{baseline_details['math_accuracy']:.3f})")
                print(f"     ãƒ†ã‚­ã‚¹ãƒˆ: {baseline_text[:150]}...")

                print(f"  âš¡ {config['name']}:")
                print(
                    f"     NFE: {drs_nfe} (å·®åˆ†: {drs_nfe-baseline_nfe:+d}, æ¯”ç‡: {drs_nfe/baseline_nfe:.2f}x)")
                print(
                    f"     å“è³ª: {drs_quality:.3f} (coherence:{drs_details['coherence']:.3f}, completeness:{drs_details['completeness']:.3f}, math:{drs_details['math_accuracy']:.3f})")
                print(f"     å“è³ªå·®åˆ†: {drs_quality-baseline_quality:+.3f}")
                print(f"     ãƒ†ã‚­ã‚¹ãƒˆ: {drs_text[:150]}...")
                print(f"     ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•è©•ä¾¡: {tradeoff_rating}")
                print(f"     æ›–æ˜§åº¦ã‚¹ã‚³ã‚¢: {[f'{s:.3f}' for s in ambiguity_scores]}")

                # çµæœè¨˜éŒ²
                all_results.append({
                    'prompt_id': i+1,
                    'prompt_type': prompt_data['type'],
                    'config': config['name'],
                    'baseline_nfe': baseline_nfe,
                    'drs_nfe': drs_nfe,
                    'nfe_ratio': drs_nfe / baseline_nfe,
                    'baseline_quality': baseline_quality,
                    'drs_quality': drs_quality,
                    'quality_gain': drs_quality - baseline_quality,
                    'tradeoff_rating': tradeoff_rating,
                    'avg_ambiguity': np.mean(ambiguity_scores)
                })

        # çµ±åˆåˆ†æ
        print(f"\n{'='*80}")
        print("ğŸ¯ DRSä»®èª¬æ¤œè¨¼çµæœ - çµ±åˆåˆ†æ")
        print(f"{'='*80}")

        excellent_cases = sum(
            1 for r in all_results if r['tradeoff_rating'] == 'EXCELLENT')
        good_cases = sum(
            1 for r in all_results if r['tradeoff_rating'] == 'GOOD')
        moderate_cases = sum(
            1 for r in all_results if r['tradeoff_rating'] == 'MODERATE')
        poor_cases = sum(
            1 for r in all_results if r['tradeoff_rating'] == 'POOR')
        total_cases = len(all_results)

        avg_nfe_ratio = np.mean([r['nfe_ratio'] for r in all_results])
        avg_quality_gain = np.mean([r['quality_gain'] for r in all_results])
        avg_ambiguity = np.mean([r['avg_ambiguity'] for r in all_results])

        print(f"ğŸ“Š æ€§èƒ½åˆ†å¸ƒ:")
        print(
            f"  ğŸŒŸ EXCELLENT: {excellent_cases}/{total_cases} ({100*excellent_cases/total_cases:.1f}%)")
        print(
            f"  âœ… GOOD: {good_cases}/{total_cases} ({100*good_cases/total_cases:.1f}%)")
        print(
            f"  âš ï¸ MODERATE: {moderate_cases}/{total_cases} ({100*moderate_cases/total_cases:.1f}%)")
        print(
            f"  âŒ POOR: {poor_cases}/{total_cases} ({100*poor_cases/total_cases:.1f}%)")

        print(f"\nğŸ“ˆ å¹³å‡æŒ‡æ¨™:")
        print(f"  NFEæ¯”ç‡: {avg_nfe_ratio:.3f}x")
        print(f"  å“è³ªå‘ä¸Š: {avg_quality_gain:+.3f}")
        print(f"  å¹³å‡æ›–æ˜§åº¦: {avg_ambiguity:.3f}")

        # DRSä»®èª¬ã®æœ€çµ‚åˆ¤å®š
        success_rate = (excellent_cases + good_cases) / total_cases

        if success_rate >= 0.7:
            final_conclusion = f"âœ… DRSä»®èª¬ã¯å¼·ãæ”¯æŒã•ã‚Œã‚‹ (æˆåŠŸç‡: {100*success_rate:.1f}%)"
        elif success_rate >= 0.5:
            final_conclusion = f"âš ï¸ DRSä»®èª¬ã¯éƒ¨åˆ†çš„ã«æ”¯æŒã•ã‚Œã‚‹ (æˆåŠŸç‡: {100*success_rate:.1f}%)"
        else:
            final_conclusion = f"âŒ DRSä»®èª¬ã¯æ”¯æŒã•ã‚Œãªã„ (æˆåŠŸç‡: {100*success_rate:.1f}%)"

        print(f"\nğŸ¯ æœ€çµ‚çµè«–: {final_conclusion}")

        # è¨­å®šåˆ¥åˆ†æ
        print(f"\nğŸ“‹ è¨­å®šåˆ¥æ€§èƒ½åˆ†æ:")
        for config_name in ['DRS-Aggressive', 'DRS-Balanced', 'DRS-Conservative']:
            config_results = [
                r for r in all_results if r['config'] == config_name]
            config_success = sum(1 for r in config_results if r['tradeoff_rating'] in [
                                 'EXCELLENT', 'GOOD'])
            config_avg_quality = np.mean(
                [r['quality_gain'] for r in config_results])
            config_avg_nfe = np.mean([r['nfe_ratio'] for r in config_results])

            print(f"  {config_name}: æˆåŠŸç‡ {config_success}/{len(config_results)}, å“è³ªå‘ä¸Š {config_avg_quality:+.3f}, NFEæ¯”ç‡ {config_avg_nfe:.3f}x")

        return all_results

    except Exception as e:
        logger.error(f"æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    results = enhanced_drs_validation()
    if results:
        print(f"\nâœ… æ”¹å–„ç‰ˆDRSä»®èª¬æ¤œè¨¼å®Œäº†")
    else:
        print(f"\nâŒ æ¤œè¨¼å¤±æ•—")
