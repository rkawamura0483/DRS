# æ”¹å–„ç‰ˆDRSæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
import logging
import re
from collections import Counter

import numpy as np
import torch
import torch.nn.functional as F
from generate import generate, generate_with_conservative_drs
from model.modeling_llada import LLaDAModelLM
from transformers import AutoTokenizer

# æ—¥æœ¬èªã®ã‚³ãƒ¡ãƒ³ãƒˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ä¿æŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QualityEvaluator:
    """æ”¹å–„ã•ã‚ŒãŸå“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def evaluate_semantic_quality(self, text):
        """æ„å‘³çš„å“è³ªã®è©•ä¾¡"""
        if not text or len(text.strip()) == 0:
            return 0.0

        # 1. åŸºæœ¬çš„ãªæ–‡æ³•æ§‹é€ ãƒã‚§ãƒƒã‚¯
        sentences = re.split(r'[.!?]+', text)
        valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 3]

        if len(valid_sentences) == 0:
            return 0.0

        # 2. åå¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
        words = text.lower().split()
        if len(words) == 0:
            return 0.0

        # é€£ç¶šã™ã‚‹åŒä¸€èªå¥ã®æ¤œå‡º
        consecutive_repetition_penalty = 0
        for i in range(len(words) - 2):
            if words[i] == words[i+1] == words[i+2]:
                consecutive_repetition_penalty += 1

        # æ–‡ãƒ¬ãƒ™ãƒ«ã§ã®åå¾©æ¤œå‡º
        sentence_repetition_penalty = 0
        sentence_counter = Counter(valid_sentences)
        for count in sentence_counter.values():
            if count > 1:
                sentence_repetition_penalty += count - 1

        # 3. èªå½™å¤šæ§˜æ€§ã‚¹ã‚³ã‚¢
        unique_words = len(set(words))
        vocabulary_diversity = unique_words / \
            len(words) if len(words) > 0 else 0

        # 4. è«–ç†çš„ä¸€è²«æ€§ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¤‰åŒ–ã®æ¤œå‡ºï¼‰
        consistency_score = 1.0
        # é‡è¦ãªåè©ãŒå¤‰åŒ–ã—ãŸå ´åˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£
        key_nouns = ['garden', 'problem', 'function', 'algorithm']
        found_nouns = [noun for noun in key_nouns if noun in text.lower()]
        if len(found_nouns) > 1:
            # è¤‡æ•°ã®ç«¶åˆã™ã‚‹æ¦‚å¿µãŒå«ã¾ã‚Œã‚‹å ´åˆ
            consistency_score *= 0.8

        # 5. æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—
        base_score = vocabulary_diversity * consistency_score
        repetition_penalty = (
            consecutive_repetition_penalty + sentence_repetition_penalty) * 0.1

        final_score = max(0, base_score - repetition_penalty)
        return min(1.0, final_score)

    def evaluate_coherence(self, text):
        """æ–‡è„ˆã®ä¸€è²«æ€§è©•ä¾¡"""
        if not text:
            return 0.0

        sentences = re.split(r'[.!?]+', text)
        valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 5]

        if len(valid_sentences) < 2:
            return 0.5  # çŸ­ã™ãã‚‹å ´åˆã¯ä¸­ç«‹çš„ã‚¹ã‚³ã‚¢

        # æ–‡é–“ã®èªå½™çš„ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        coherence_score = 0.0
        for i in range(len(valid_sentences) - 1):
            words1 = set(valid_sentences[i].lower().split())
            words2 = set(valid_sentences[i+1].lower().split())
            overlap = len(words1 & words2) / max(len(words1), len(words2), 1)
            coherence_score += overlap

        return coherence_score / max(1, len(valid_sentences) - 1)

    def comprehensive_quality_score(self, text):
        """åŒ…æ‹¬çš„å“è³ªã‚¹ã‚³ã‚¢"""
        semantic_score = self.evaluate_semantic_quality(text)
        coherence_score = self.evaluate_coherence(text)

        # é‡ã¿ä»˜ãå¹³å‡
        final_score = 0.7 * semantic_score + 0.3 * coherence_score
        return final_score


def improved_ambiguity_calculation(confidence_scores, threshold, baseline_confidence=None):
    """æ”¹å–„ã•ã‚ŒãŸæ›–æ˜§åº¦è¨ˆç®—"""
    valid_scores = confidence_scores[confidence_scores != -np.inf]
    if len(valid_scores) == 0:
        return 0.0

    # 1. ç›¸å¯¾çš„æ›–æ˜§åº¦è¨ˆç®—ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒï¼‰
    if baseline_confidence is not None:
        baseline_valid = baseline_confidence[baseline_confidence != -np.inf]
        if len(baseline_valid) > 0:
            relative_confidence_drop = (
                baseline_valid.mean() - valid_scores.mean()).item()
            relative_ambiguity = max(0, relative_confidence_drop)
        else:
            relative_ambiguity = 0
    else:
        relative_ambiguity = 0

    # 2. åˆ†æ•£ãƒ™ãƒ¼ã‚¹ã®æ›–æ˜§åº¦
    variance_ambiguity = valid_scores.var().item()

    # 3. é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®æ›–æ˜§åº¦
    threshold_ambiguity = (valid_scores < threshold).float().mean().item()

    # çµ±åˆæ›–æ˜§åº¦ã‚¹ã‚³ã‚¢
    combined_ambiguity = (
        0.4 * threshold_ambiguity +
        0.3 * variance_ambiguity +
        0.3 * relative_ambiguity
    )

    return combined_ambiguity


def test_improved_drs_validation():
    """æ”¹å–„ç‰ˆDRSä»®èª¬æ¤œè¨¼"""
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”¬ æ”¹å–„ç‰ˆDRSæ¤œè¨¼é–‹å§‹ (ãƒ‡ãƒã‚¤ã‚¹: {device})")

    # ã‚ˆã‚Šé©åˆ‡ãªãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    test_prompts = [
        "Calculate the area of a rectangular garden with length 15 meters and width 8 meters.",
        "Write a Python function to find the factorial of a number using recursion.",
    ]

    try:
        print("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
        model = LLaDAModelLM.from_pretrained(
            'GSAI-ML/LLaDA-8B-Instruct',
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        ).to(device).eval()

        tokenizer = AutoTokenizer.from_pretrained(
            'GSAI-ML/LLaDA-8B-Instruct',
            trust_remote_code=True
        )
        model.tokenizer = tokenizer

        # ãƒã‚¹ã‚¯IDå–å¾—
        mask_id = tokenizer.mask_token_id or model.config.mask_token_id or 126336
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† (mask_id={mask_id})")

        # å“è³ªè©•ä¾¡å™¨ã®åˆæœŸåŒ–
        evaluator = QualityEvaluator(tokenizer)

        # æ”¹å–„ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆè¨­å®š
        gen_length = 256
        block_length = 32
        total_steps = 128

        results = []

        for i, prompt in enumerate(test_prompts):
            print(f"\n{'='*80}")
            print(f"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i+1}: {prompt}")
            print(f"{'='*80}")

            m = [{"role": "user", "content": prompt}]
            prompt_formatted = tokenizer.apply_chat_template(
                m, add_generation_prompt=True, tokenize=False)
            input_ids = tokenizer(prompt_formatted)['input_ids']
            input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)

            # ã‚ˆã‚Šé©åˆ‡ãªãƒ†ã‚¹ãƒˆæ¡ä»¶
            test_conditions = [
                {'t_base': 4, 'threshold': 0.7, 'name': 'é©åº¦ãªæ¡ä»¶'},
            ]

            for condition in test_conditions:
                print(f"\n{'-'*60}")
                print(
                    f"ğŸ§ª {condition['name']} (t_base={condition['t_base']}, threshold={condition['threshold']})")
                print(f"{'-'*60}")

                # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç”Ÿæˆ
                print("ğŸ¯ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç”Ÿæˆä¸­...")
                baseline_out, baseline_nfe = generate(
                    model, input_ids, steps=total_steps, gen_length=gen_length,
                    block_length=block_length, temperature=0., remasking='low_confidence',
                    mask_id=mask_id
                )

                # ä¿å®ˆçš„DRSç”Ÿæˆ
                print("âš¡ ä¿å®ˆçš„DRSç”Ÿæˆä¸­...")
                drs_out, drs_nfe, ambiguity_scores = generate_with_conservative_drs(
                    model, input_ids, steps=total_steps, gen_length=gen_length,
                    block_length=block_length, temperature=0.,
                    threshold=condition['threshold'], t_base=condition['t_base'],
                    mask_id=mask_id
                )

                # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                baseline_text = tokenizer.batch_decode(
                    baseline_out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]
                drs_text = tokenizer.batch_decode(
                    drs_out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]

                # æ”¹å–„ã•ã‚ŒãŸå“è³ªè©•ä¾¡
                baseline_quality = evaluator.comprehensive_quality_score(
                    baseline_text)
                drs_quality = evaluator.comprehensive_quality_score(drs_text)
                quality_retention = (
                    drs_quality / baseline_quality) * 100 if baseline_quality > 0 else 0

                # NFEåŠ¹ç‡
                nfe_reduction = ((baseline_nfe - drs_nfe) / baseline_nfe) * 100

                # æ›–æ˜§åº¦åˆ†æ
                max_ambiguity = max(
                    ambiguity_scores) if ambiguity_scores else 0
                ambiguity_variance = np.var(
                    ambiguity_scores) if ambiguity_scores else 0
                meaningful_blocks = sum(
                    1 for score in ambiguity_scores if score > 0.1)

                # çµæœå‡ºåŠ›
                print(f"\nğŸ“Š è©³ç´°çµæœ:")
                print(f"  ğŸ¯ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:")
                print(
                    f"     NFE: {baseline_nfe}, å“è³ªã‚¹ã‚³ã‚¢: {baseline_quality:.3f}")
                print(f"     ãƒ†ã‚­ã‚¹ãƒˆ: {baseline_text[:100]}...")
                print(f"  âš¡ ä¿å®ˆçš„DRS:")
                print(f"     NFE: {drs_nfe}, å“è³ªã‚¹ã‚³ã‚¢: {drs_quality:.3f}")
                print(f"     ãƒ†ã‚­ã‚¹ãƒˆ: {drs_text[:100]}...")

                print(f"\nğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ:")
                print(f"  ğŸ”„ NFEå‰Šæ¸›: {nfe_reduction:.1f}%")
                print(f"  ğŸ’ å“è³ªä¿æŒ: {quality_retention:.1f}%")
                print(f"  ğŸ­ æœ€å¤§æ›–æ˜§åº¦: {max_ambiguity:.3f}")
                print(f"  ğŸ“Š æ›–æ˜§åº¦åˆ†æ•£: {ambiguity_variance:.3f}")
                print(
                    f"  ğŸ” æ„å‘³ã‚ã‚‹ãƒ–ãƒ­ãƒƒã‚¯: {meaningful_blocks}/{len(ambiguity_scores)}")

                # DRSä¾¡å€¤ã®è©•ä¾¡ï¼ˆæ”¹å–„ç‰ˆï¼‰
                if (nfe_reduction > 20 and quality_retention > 70 and
                        meaningful_blocks >= 2 and ambiguity_variance > 0.01):
                    drs_value = "âœ… TRUE - æœ‰åŠ¹ãªå‹•çš„é…åˆ†"
                elif (nfe_reduction > 15 and quality_retention > 50):
                    drs_value = "âš ï¸ PARTIAL - é™å®šçš„åŠ¹æœ"
                else:
                    drs_value = "âŒ FALSE - åŠ¹æœä¸æ˜ã¾ãŸã¯å“è³ªåŠ£åŒ–"

                print(f"  ğŸ¯ DRSä¾¡å€¤è©•ä¾¡: {drs_value}")

                # çµæœä¿å­˜
                results.append({
                    'prompt_id': i+1,
                    'condition': condition['name'],
                    'nfe_reduction': nfe_reduction,
                    'quality_retention': quality_retention,
                    'baseline_quality': baseline_quality,
                    'drs_quality': drs_quality,
                    'max_ambiguity': max_ambiguity,
                    'ambiguity_variance': ambiguity_variance,
                    'meaningful_blocks': meaningful_blocks,
                    'drs_value': drs_value
                })

        # å…¨ä½“çš„ãªçµè«–
        print(f"\n{'='*80}")
        print("ğŸ¯ æ”¹å–„ç‰ˆæ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
        print(f"{'='*80}")

        successful_cases = sum(1 for r in results if 'TRUE' in r['drs_value'])
        partial_cases = sum(1 for r in results if 'PARTIAL' in r['drs_value'])
        total_cases = len(results)

        avg_nfe_reduction = np.mean([r['nfe_reduction'] for r in results])
        avg_quality_retention = np.mean(
            [r['quality_retention'] for r in results])

        print(f"ğŸ“Š çµ±è¨ˆ:")
        print(f"  âœ… å®Œå…¨æˆåŠŸ: {successful_cases}/{total_cases}")
        print(f"  âš ï¸ éƒ¨åˆ†æˆåŠŸ: {partial_cases}/{total_cases}")
        print(f"  ğŸ“‰ å¹³å‡NFEå‰Šæ¸›: {avg_nfe_reduction:.1f}%")
        print(f"  ğŸ’ å¹³å‡å“è³ªä¿æŒ: {avg_quality_retention:.1f}%")

        if successful_cases >= total_cases * 0.5:
            final_conclusion = "âœ… DRSä»®èª¬ã¯æ¤œè¨¼ã•ã‚ŒãŸ - é©åˆ‡ãªæ¡ä»¶ä¸‹ã§æœ‰åŠ¹"
        elif (successful_cases + partial_cases) >= total_cases * 0.6:
            final_conclusion = "âš ï¸ DRSä»®èª¬ã¯éƒ¨åˆ†çš„ã«æ¤œè¨¼ - ã•ã‚‰ãªã‚‹èª¿æ•´ãŒå¿…è¦"
        else:
            final_conclusion = "âŒ DRSä»®èª¬ã¯æ¤œè¨¼ã•ã‚Œãš - æ ¹æœ¬çš„è¦‹ç›´ã—ãŒå¿…è¦"

        print(f"\nğŸ¯ æœ€çµ‚çµè«–: {final_conclusion}")

        return results

    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    results = test_improved_drs_validation()
    if results:
        print(f"\nâœ… æ”¹å–„ç‰ˆæ¤œè¨¼å®Œäº†")
    else:
        print(f"\nâŒ æ¤œè¨¼å¤±æ•—")
