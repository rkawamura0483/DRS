# æ”¹å–„ç‰ˆDRSæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
import logging
import re
from collections import Counter

import numpy as np
import torch
import torch.nn.functional as F
from generate import generate_with_drs, generate_with_dual_cache
from model.modeling_llada import LLaDAModelLM
from transformers import AutoTokenizer

# æ—¥æœ¬èªã®ã‚³ãƒ¡ãƒ³ãƒˆã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ä¿æŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ImprovedQualityEvaluator:
    """å¤§å¹…æ”¹å–„ã•ã‚ŒãŸå“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, tokenizer):
        self.tokenizer = tokenizer

    def check_semantic_consistency(self, prompt, baseline_text, drs_text):
        """æ„å‘³çš„ä¸€è²«æ€§ã®è©•ä¾¡ - æœ€é‡è¦æŒ‡æ¨™"""
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        prompt_lower = prompt.lower()
        baseline_lower = baseline_text.lower()
        drs_lower = drs_text.lower()

        # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸€è‡´åº¦ãƒã‚§ãƒƒã‚¯
        if "rectangular" in prompt_lower and "garden" in prompt_lower:
            # é•·æ–¹å½¢ã®åº­ã®å•é¡Œ
            baseline_has_rectangle = any(word in baseline_lower for word in [
                                         "rectangular", "rectangle", "length", "width"])
            drs_has_rectangle = any(word in drs_lower for word in [
                                    "rectangular", "rectangle", "length", "width"])

            # å††ã®æ¦‚å¿µãŒæ··å…¥ã—ã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            baseline_has_circle = any(word in baseline_lower for word in [
                                      "circle", "radius", "Ï€", "pi"])
            drs_has_circle = any(word in drs_lower for word in [
                                 "circle", "radius", "Ï€", "pi"])

            if baseline_has_rectangle and not baseline_has_circle:
                if drs_has_rectangle and not drs_has_circle:
                    return 1.0  # å®Œå…¨ä¸€è‡´
                elif drs_has_circle:
                    return 0.0  # é‡å¤§ãªæ„å‘³çš„ã‚¨ãƒ©ãƒ¼
                else:
                    return 0.5  # éƒ¨åˆ†çš„

        elif "python" in prompt_lower and "function" in prompt_lower:
            # ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°å•é¡Œ
            baseline_has_code = "def " in baseline_lower and "return" in baseline_lower
            drs_has_code = "def " in drs_lower and "return" in drs_lower

            if baseline_has_code:
                if drs_has_code:
                    return 1.0
                else:
                    return 0.0  # ã‚³ãƒ¼ãƒ‰ãŒç”Ÿæˆã•ã‚Œã¦ã„ãªã„

        # ä¸€èˆ¬çš„ãªä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
        baseline_words = set(baseline_lower.split())
        drs_words = set(drs_lower.split())
        overlap = len(baseline_words & drs_words) / max(len(baseline_words), 1)

        return min(1.0, overlap)

    def evaluate_repetition_penalty(self, text):
        """åå¾©ãƒšãƒŠãƒ«ãƒ†ã‚£ã®è©•ä¾¡"""
        if not text:
            return 1.0

        sentences = re.split(r'[.!?]+', text)
        valid_sentences = [s.strip() for s in sentences if len(s.strip()) > 5]

        if len(valid_sentences) <= 1:
            return 1.0

        # å®Œå…¨ã«åŒä¸€ã®æ–‡ã®æ¤œå‡º
        sentence_counts = Counter(valid_sentences)
        max_repetition = max(sentence_counts.values())

        if max_repetition >= 3:
            return 0.1  # é‡å¤§ãªåå¾©
        elif max_repetition == 2:
            return 0.5  # è»½åº¦ã®åå¾©

        # é€£ç¶šã™ã‚‹åŒä¸€èªå¥ã®æ¤œå‡º
        words = text.split()
        consecutive_count = 0
        for i in range(len(words) - 2):
            if words[i] == words[i+1] == words[i+2]:
                consecutive_count += 1

        if consecutive_count > 5:
            return 0.1
        elif consecutive_count > 2:
            return 0.5

        return 1.0

    def evaluate_completeness(self, text, expected_elements):
        """å®Œå…¨æ€§ã®è©•ä¾¡"""
        if not text:
            return 0.0

        text_lower = text.lower()
        found_elements = sum(
            1 for element in expected_elements if element.lower() in text_lower)

        return found_elements / max(len(expected_elements), 1)

    def comprehensive_quality_score(self, prompt, baseline_text, drs_text):
        """åŒ…æ‹¬çš„å“è³ªã‚¹ã‚³ã‚¢ - æ„å‘³çš„ä¸€è²«æ€§ã‚’æœ€é‡è¦è¦–"""
        # 1. æ„å‘³çš„ä¸€è²«æ€§ (æœ€é‡è¦ - 60%ã®é‡ã¿)
        semantic_consistency = self.check_semantic_consistency(
            prompt, baseline_text, drs_text)

        # 2. åå¾©ãƒšãƒŠãƒ«ãƒ†ã‚£ (30%ã®é‡ã¿)
        repetition_score = self.evaluate_repetition_penalty(drs_text)

        # 3. åŸºæœ¬çš„ãªå®Œå…¨æ€§ (10%ã®é‡ã¿)
        drs_length = len(drs_text.split())
        baseline_length = len(baseline_text.split())
        length_ratio = min(
            1.0, drs_length / max(baseline_length, 1)) if baseline_length > 0 else 0

        # é‡ã¿ä»˜ãæœ€çµ‚ã‚¹ã‚³ã‚¢
        final_score = (
            0.6 * semantic_consistency +
            0.3 * repetition_score +
            0.1 * length_ratio
        )

        return final_score, {
            'semantic_consistency': semantic_consistency,
            'repetition_score': repetition_score,
            'length_ratio': length_ratio
        }


def enhanced_ambiguity_calculation_with_context(confidence_scores, threshold, prompt_context=None):
    """æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸæ”¹å–„ã•ã‚ŒãŸæ›–æ˜§åº¦è¨ˆç®—"""
    valid_scores = confidence_scores[confidence_scores != -np.inf]
    if len(valid_scores) == 0:
        return 0.0

    # åŸºæœ¬çš„ãªé–¾å€¤ãƒ™ãƒ¼ã‚¹æ›–æ˜§åº¦
    threshold_ambiguity = (valid_scores < threshold).float().mean().item()

    # åˆ†æ•£ãƒ™ãƒ¼ã‚¹ã®æ›–æ˜§åº¦ï¼ˆä¸å®‰å®šæ€§æŒ‡æ¨™ï¼‰
    variance_ambiguity = min(1.0, valid_scores.var().item() * 10)  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´

    # çµ±åˆæ›–æ˜§åº¦ã‚¹ã‚³ã‚¢ï¼ˆã‚ˆã‚Šä¿å®ˆçš„ã«ï¼‰
    combined_ambiguity = 0.7 * threshold_ambiguity + 0.3 * variance_ambiguity

    return combined_ambiguity


def test_drs_validation():
    """DRSä»®èª¬æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”¬ DRSä»®èª¬æ¤œè¨¼é–‹å§‹ (ãƒ‡ãƒã‚¤ã‚¹: {device})")

    # æ¤œè¨¼ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    test_prompts = [
        "Calculate the area of a rectangular garden with length 15 meters and width 8 meters.",
        "Write a Python function to find the factorial of a number using recursion.",
    ]

    try:
        print("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ä¸­...")
        model = LLaDAModelLM.from_pretrained(
            'GSAI-ML/LLaDA-8B-Instruct',
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        ).to(device).eval()

        tokenizer = AutoTokenizer.from_pretrained(
            'GSAI-ML/LLaDA-8B-Instruct',
            trust_remote_code=True
        )
        model.tokenizer = tokenizer

        # ãƒã‚¹ã‚¯IDå–å¾—
        mask_id = tokenizer.mask_token_id or model.config.mask_token_id or 126336
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº† (mask_id={mask_id})")

        evaluator = ImprovedQualityEvaluator(tokenizer)

        gen_length = 256
        block_length = 32
        total_steps = 128

        results = []

        for i, prompt in enumerate(test_prompts):
            print(f"\n{'='*80}")
            print(f"ğŸ“ ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i+1}: {prompt}")
            print(f"{'='*80}")

            m = [{"role": "user", "content": prompt}]
            prompt_formatted = tokenizer.apply_chat_template(
                m, add_generation_prompt=True, tokenize=False)
            input_ids = tokenizer(prompt_formatted)['input_ids']
            input_ids = torch.tensor(input_ids).to(device).unsqueeze(0)

            # --- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (fast-dLLM) ---
            print("\nğŸ¯ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (fast-dLLM: generate_with_dual_cache) ç”Ÿæˆä¸­...")
            baseline_out, baseline_nfe = generate_with_dual_cache(
                model, input_ids, steps=total_steps, gen_length=gen_length,
                block_length=block_length, temperature=0., remasking='low_confidence',
                mask_id=mask_id
            )
            baseline_text = tokenizer.batch_decode(
                baseline_out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]

            # --- ææ¡ˆæ‰‹æ³• (DRS) ---
            # DRSã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šã‚’å¤‰ãˆã¦å®Ÿé¨“
            test_conditions = [
                {'t_base': 16, 'threshold': 0.9,
                    'name': 'DRS (t_base=16, å“è³ªé‡è¦–)'},
                {'t_base': 8, 'threshold': 0.8,
                    'name': 'DRS (t_base=8, ãƒãƒ©ãƒ³ã‚¹)'},
                {'t_base': 4, 'threshold': 0.7,
                    'name': 'DRS (t_base=4, é€Ÿåº¦é‡è¦–)'},
            ]

            for condition in test_conditions:
                print(f"\n{'-'*60}")
                print(f"ğŸ§ª {condition['name']}")
                print(f"{'-'*60}")

                print("âš¡ DRS (generate_with_drs) ç”Ÿæˆä¸­...")
                drs_out, drs_nfe, ambiguity_scores = generate_with_drs(
                    model, input_ids, steps=total_steps, gen_length=gen_length,
                    block_length=block_length, temperature=0.,
                    threshold=condition['threshold'], t_base=condition['t_base'],
                    mask_id=mask_id
                )
                drs_text = tokenizer.batch_decode(
                    drs_out[:, input_ids.shape[1]:], skip_special_tokens=True)[0]

                # å“è³ªè©•ä¾¡
                drs_quality, quality_details = evaluator.comprehensive_quality_score(
                    prompt, baseline_text, drs_text)

                # NFEåŠ¹ç‡
                # NFEã¯å¿…ãšã—ã‚‚å‰Šæ¸›ã•ã‚Œã‚‹ã‚ã‘ã§ã¯ãªãã€åŒç¨‹åº¦ã®äºˆç®—ã§å“è³ªå‘ä¸Šã‚’ç›®æŒ‡ã™
                nfe_diff = drs_nfe - baseline_nfe

                # è©³ç´°ãªçµæœå‡ºåŠ›
                print(f"\nğŸ“Š è©³ç´°çµæœ:")
                print(f"  ğŸ¯ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ (fast-dLLM):")
                print(f"     NFE: {baseline_nfe}")
                print(f"     ãƒ†ã‚­ã‚¹ãƒˆ: {baseline_text[:200]}...")
                print(f"  âš¡ ææ¡ˆæ‰‹æ³• ({condition['name']}):")
                print(f"     NFE: {drs_nfe} (ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”: {nfe_diff:+d})")
                print(f"     ãƒ†ã‚­ã‚¹ãƒˆ: {drs_text[:200]}...")

                print(f"\nğŸ“ˆ å“è³ªåˆ†æ:")
                print(f"  ğŸ’ ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {drs_quality:.3f}")
                print(
                    f"     - æ„å‘³çš„ä¸€è²«æ€§: {quality_details['semantic_consistency']:.3f}")
                print(
                    f"     - åå¾©ãƒšãƒŠãƒ«ãƒ†ã‚£: {quality_details['repetition_score']:.3f}")
                print(f"     - é•·ã•æ¯”ç‡: {quality_details['length_ratio']:.3f}")

                # ä»®èª¬æ¤œè¨¼
                # DRSã¯ã€NFEã‚’ã‚ãšã‹ã«å¢—åŠ ã•ã›ã‚‹ã“ã¨ã§ã€å“è³ªã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™
                quality_improved = drs_quality > evaluator.comprehensive_quality_score(
                    prompt, baseline_text, baseline_text)[0] * 1.05  # 5%ä»¥ä¸Šã®æ”¹å–„
                nfe_efficient = drs_nfe < total_steps * 1.2  # NFEã®å¢—åŠ ãŒ20%ä»¥å†…ã«åã¾ã£ã¦ã„ã‚‹

                if quality_improved and nfe_efficient:
                    drs_value = "âœ… TRUE - å°‘ãªã„NFEå¢—åŠ ã§å“è³ªãŒå¤§å¹…ã«å‘ä¸Š"
                elif quality_improved:
                    drs_value = "âš ï¸ PARTIAL - å“è³ªã¯å‘ä¸Šã—ãŸãŒã€NFEãŒå¢—ãˆã™ããŸ"
                elif not quality_improved and nfe_efficient:
                    drs_value = "âŒ FALSE - å“è³ªãŒå‘ä¸Šã›ãšã€DRSã®åŠ¹æœãŒè¦‹ã‚‰ã‚Œãªã„"
                else:
                    drs_value = "âŒ FALSE - å“è³ªãŒå‘ä¸Šã›ãšã€NFEã‚‚å¢—åŠ ã—ãŸ"

                print(f"  ğŸ¯ DRSä»®èª¬è©•ä¾¡: {drs_value}")
                print(f"  - æ›–æ˜§åº¦ã‚¹ã‚³ã‚¢: {[f'{s:.3f}' for s in ambiguity_scores]}")

                # çµæœä¿å­˜
                results.append({
                    'prompt_id': i+1,
                    'condition': condition['name'],
                    'nfe_diff': nfe_diff,
                    'total_quality': drs_quality,
                    'semantic_consistency': quality_details['semantic_consistency'],
                    'repetition_score': quality_details['repetition_score'],
                    'length_ratio': quality_details['length_ratio'],
                    'drs_value': drs_value
                })

        # å…¨ä½“çš„ãªçµè«–
        print(f"\n{'='*80}")
        print("ğŸ¯ DRSä»®èª¬æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
        print(f"{'='*80}")

        successful_cases = sum(1 for r in results if 'TRUE' in r['drs_value'])
        partial_cases = sum(1 for r in results if 'PARTIAL' in r['drs_value'])
        total_cases = len(results)

        avg_nfe_diff = np.mean([r['nfe_diff'] for r in results])
        avg_total_quality = np.mean([r['total_quality'] for r in results])
        avg_semantic_consistency = np.mean(
            [r['semantic_consistency'] for r in results])

        print(f"ğŸ“Š çµ±è¨ˆ:")
        print(f"  âœ… å®Œå…¨æˆåŠŸ: {successful_cases}/{total_cases}")
        print(f"  âš ï¸ éƒ¨åˆ†æˆåŠŸ: {partial_cases}/{total_cases}")
        print(f"  ğŸ“‰ å¹³å‡NFEå·®: {avg_nfe_diff:.1f}")
        print(f"  ğŸ’ å¹³å‡ç·åˆå“è³ª: {avg_total_quality:.3f}")
        print(f"  ğŸ­ å¹³å‡æ„å‘³çš„ä¸€è²«æ€§: {avg_semantic_consistency:.3f}")

        # ç ”ç©¶ä¾¡å€¤ã®æœ€çµ‚åˆ¤å®šï¼ˆã‚ˆã‚Šå³æ ¼ãªåŸºæº–ï¼‰
        semantic_success_rate = sum(
            1 for r in results if r['semantic_consistency'] >= 0.8) / total_cases

        if successful_cases >= total_cases * 0.5 and semantic_success_rate >= 0.8:
            final_conclusion = "âœ… DRSä»®èª¬ã¯æ¤œè¨¼ã•ã‚ŒãŸ - é«˜å“è³ªç¶­æŒ&åŠ¹ç‡å‘ä¸Šå®Ÿç¾"
        elif semantic_success_rate >= 0.6:
            final_conclusion = "âš ï¸ DRSä»®èª¬ã¯éƒ¨åˆ†çš„ã«æ¤œè¨¼ - æ„å‘³ä¿æŒã¯è‰¯å¥½ã ãŒåŠ¹ç‡è¦æ”¹å–„"
        else:
            final_conclusion = "âŒ DRSä»®èª¬ã¯æ¤œè¨¼ã•ã‚Œãš - æ„å‘³çš„ä¸€è²«æ€§ã«é‡å¤§ãªå•é¡Œ"

        print(f"\nğŸ¯ æœ€çµ‚çµè«–: {final_conclusion}")

        # æ”¹å–„ææ¡ˆ
        if semantic_success_rate < 0.8:
            print(f"\nğŸ’¡ æ”¹å–„ææ¡ˆ:")
            print(f"  1. å†ãƒã‚¹ã‚¯é–¾å€¤ã‚’ã•ã‚‰ã«ä¿å®ˆçš„ã«è¨­å®š (0.95+)")
            print(f"  2. å®Œæˆãƒ–ãƒ­ãƒƒã‚¯ã®å†ãƒã‚¹ã‚¯ã‚’å®Œå…¨ã«ç¦æ­¢")
            print(f"  3. ãƒ–ãƒ­ãƒƒã‚¯é–“ã®æ–‡è„ˆç¶™ç¶šæ€§ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’è¿½åŠ ")
            print(f"  4. æ„å‘³çš„æ¤œè¨¼ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã«çµ„ã¿è¾¼ã¿")

        return results

    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    results = test_drs_validation()
    if results:
        print(f"\nâœ… DRSä»®èª¬æ¤œè¨¼å®Œäº†")
    else:
        print(f"\nâŒ æ¤œè¨¼å¤±æ•—")
