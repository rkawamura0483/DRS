# Copyright 2025 NVIDIA CORPORATION & AFFILIATES
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

import torch
import torch.nn.functional as F
import numpy as np
from typing import List, Tuple, Optional, Dict, Any
from tqdm import tqdm
import time

from .adaptive_scheduler import AdaptiveInferenceScheduler
from .cache_manager import TieredCacheManager, CacheTier
from .generate import (
    add_gumbel_noise,
    get_num_transfer_tokens,
    get_transfer_index_with_confidence
)


@torch.no_grad()
def generate_with_adaptive_scheduling(
    model,
    prompt: torch.Tensor,
    gen_length: int = 128,
    base_block_size: int = 16,
    base_confidence_threshold: float = 0.8,
    adaptation_rate: float = 0.2,
    enable_tiered_cache: bool = True,
    temperature: float = 0.0,
    remasking: str = 'low_confidence',
    mask_id: Optional[int] = None,
    scheduler_config: Optional[Dict] = None,
    cache_config: Optional[Dict] = None,
    verbose: bool = True
) -> Tuple[torch.Tensor, Dict[str, Any]]:
    """
    Self-Correcting Adaptive Inference Scheduling for Diffusion LLMs

    ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆé–¢æ•°ã€‚å‹•çš„ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºèª¿æ•´ã€
    é©å¿œçš„ä¿¡é ¼åº¦é–¾å€¤ã€éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚’çµ±åˆã€‚

    Args:
        model: LLaDAãƒ¢ãƒ‡ãƒ«
        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (1, prompt_length)
        gen_length: ç”Ÿæˆã™ã‚‹é•·ã•
        base_block_size: åˆæœŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º
        base_confidence_threshold: åˆæœŸä¿¡é ¼åº¦é–¾å€¤
        adaptation_rate: é©å¿œç‡
        enable_tiered_cache: éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
        remasking: ãƒªãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
        mask_id: ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ID
        scheduler_config: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
        cache_config: ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        verbose: è©³ç´°å‡ºåŠ›

    Returns:
        (ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³, è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
    """

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    scheduler_config = scheduler_config or {}
    cache_config = cache_config or {}

    if verbose:
        print(f"\nğŸš€ Adaptive Scheduling é–‹å§‹")
        print(f"   ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {prompt.shape[1]}")
        print(f"   ç”Ÿæˆé•·: {gen_length}")
        print(f"   åˆæœŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {base_block_size}")
        print(f"   åˆæœŸä¿¡é ¼åº¦é–¾å€¤: {base_confidence_threshold}")
        print(f"   éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {enable_tiered_cache}")

    # ãƒã‚¹ã‚¯IDã®å–å¾—
    if mask_id is None:
        if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'mask_token_id'):
            mask_id = model.tokenizer.mask_token_id
        else:
            mask_id = model.config.mask_token_id

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–
    scheduler = AdaptiveInferenceScheduler(
        min_block_size=max(4, base_block_size // 4),
        max_block_size=min(64, base_block_size * 4),
        base_confidence_threshold=base_confidence_threshold,
        adaptation_sensitivity=adaptation_rate,
        **scheduler_config
    )

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
    cache_manager = None
    if enable_tiered_cache:
        cache_manager = TieredCacheManager(**cache_config)

    # åˆæœŸè¨­å®š
    x = torch.full((1, prompt.shape[1] + gen_length),
                   mask_id, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    # åˆæœŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã§é–‹å§‹
    current_block_size = base_block_size
    current_threshold = base_confidence_threshold

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    metrics = {
        'nfe': 0,
        'total_adaptations': 0,
        'block_size_history': [],
        'threshold_history': [],
        'confidence_history': [],
        'entropy_history': [],
        'cache_efficiency': {},
        'timing': {
            'total_time': 0,
            'adaptation_time': 0,
            'generation_time': 0,
            'cache_time': 0
        },
        'blocks_processed': 0,
        'tier_usage': {'tier1': 0, 'tier2': 0, 'tier3': 0}
    }

    start_time = time.time()

    # Phase 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š
    if enable_tiered_cache:
        cache_start = time.time()
        output = model(x[:, :prompt.shape[1]], use_cache=True)
        cache_manager.set_prompt_cache(prompt.shape[1], output.past_key_values)
        metrics['timing']['cache_time'] += time.time() - cache_start
        metrics['nfe'] += 1

        if verbose:
            print(f"âœ… Tier1ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šå®Œäº† (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {prompt.shape[1]})")

    # Phase 2: ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ç”Ÿæˆ
    generated_tokens = 0
    block_id = 0

    with tqdm(total=gen_length, desc="Adaptive Generation", disable=not verbose) as pbar:
        while generated_tokens < gen_length:
            block_start_time = time.time()

            # ç¾åœ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ç¯„å›²ã‚’æ±ºå®š
            block_start = prompt.shape[1] + generated_tokens
            remaining_tokens = gen_length - generated_tokens
            actual_block_size = min(current_block_size, remaining_tokens)
            block_end = block_start + actual_block_size

            if verbose and block_id % 5 == 0:
                print(
                    f"\nğŸ“¦ ãƒ–ãƒ­ãƒƒã‚¯ {block_id}: ã‚µã‚¤ã‚º={actual_block_size}, é–¾å€¤={current_threshold:.3f}")

            # ãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆ
            block_generated, block_metrics = _generate_block_adaptive(
                model=model,
                x=x,
                block_start=block_start,
                block_end=block_end,
                current_threshold=current_threshold,
                temperature=temperature,
                remasking=remasking,
                cache_manager=cache_manager,
                block_id=block_id,
                mask_id=mask_id
            )

            metrics['nfe'] += block_metrics['nfe']
            metrics['timing']['generation_time'] += block_metrics['generation_time']

            # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–èª¿æ•´
            if block_metrics['confidence_scores'] is not None:
                adaptation_start = time.time()

                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã«ã‚ˆã‚‹é©å¿œ
                next_block_size, adapted_threshold, step_metrics = scheduler.step(
                    logits=block_metrics['final_logits'],
                    tokens=x[:, block_start:block_end],
                    mask_index=block_metrics['final_mask_index'],
                    step_num=block_id,
                    total_steps=gen_length // base_block_size
                )

                # é©å¿œæ¤œå‡º
                if (next_block_size != current_block_size or
                        abs(adapted_threshold - current_threshold) > 0.01):
                    metrics['total_adaptations'] += 1
                    if verbose:
                        print(f"ğŸ”„ é©å¿œ: ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º {current_block_size}â†’{next_block_size}, "
                              f"é–¾å€¤ {current_threshold:.3f}â†’{adapted_threshold:.3f}")

                # æ›´æ–°
                current_block_size = next_block_size
                current_threshold = adapted_threshold

                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                metrics['block_size_history'].append(actual_block_size)
                metrics['threshold_history'].append(current_threshold)
                metrics['confidence_history'].append(
                    step_metrics['confidence'])
                metrics['entropy_history'].append(step_metrics['entropy'])

                metrics['timing']['adaptation_time'] += time.time() - \
                    adaptation_start

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨çŠ¶æ³ã®è¨˜éŒ²
            if enable_tiered_cache and block_metrics['cache_tier']:
                if block_metrics['cache_tier'] == CacheTier.FROZEN:
                    metrics['tier_usage']['tier1'] += 1
                elif block_metrics['cache_tier'] == CacheTier.STABLE:
                    metrics['tier_usage']['tier2'] += 1
                elif block_metrics['cache_tier'] == CacheTier.ACTIVE:
                    metrics['tier_usage']['tier3'] += 1

            # é€²æ—æ›´æ–°
            generated_tokens += actual_block_size
            metrics['blocks_processed'] += 1
            block_id += 1

            pbar.update(actual_block_size)
            pbar.set_postfix({
                'B': actual_block_size,
                'Ï„': f"{current_threshold:.2f}",
                'NFE': metrics['nfe'],
                'Adapt': metrics['total_adaptations']
            })

            metrics['timing']['generation_time'] += time.time() - \
                block_start_time

    # æœ€çµ‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    metrics['timing']['total_time'] = time.time() - start_time

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    scheduler_metrics = scheduler.get_adaptation_metrics()
    metrics.update({
        'avg_block_size': scheduler_metrics['current_block_size'],
        'final_threshold': scheduler_metrics['current_threshold'],
        'adaptation_rate': scheduler_metrics['adaptation_count'] / max(1, scheduler_metrics['total_blocks'])
    })

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    if enable_tiered_cache:
        cache_metrics = cache_manager.get_cache_efficiency_metrics()
        metrics['cache_efficiency'] = cache_metrics

        if verbose:
            cache_manager.print_cache_status()

    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
    if verbose:
        print(f"\nğŸ‰ Adaptive Scheduling å®Œäº†!")
        print(f"   ç·æ™‚é–“: {metrics['timing']['total_time']:.2f}ç§’")
        print(f"   ç·NFE: {metrics['nfe']}")
        print(f"   é©å¿œå›æ•°: {metrics['total_adaptations']}")
        print(f"   å¹³å‡ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {np.mean(metrics['block_size_history']):.1f}")
        print(f"   æœ€çµ‚ä¿¡é ¼åº¦é–¾å€¤: {metrics['final_threshold']:.3f}")
        if enable_tiered_cache:
            print(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {cache_metrics['cache_hit_rate']:.2%}")

    return x, metrics


def _generate_block_adaptive(
    model,
    x: torch.Tensor,
    block_start: int,
    block_end: int,
    current_threshold: float,
    temperature: float,
    remasking: str,
    cache_manager: Optional[TieredCacheManager],
    block_id: int,
    mask_id: int
) -> Tuple[bool, Dict[str, Any]]:
    """
    ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ç”¨ãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆ

    Args:
        model: LLaDAãƒ¢ãƒ‡ãƒ«
        x: ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ãƒ³ã‚½ãƒ«
        block_start: ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹ä½ç½®
        block_end: ãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†ä½ç½®
        current_threshold: ç¾åœ¨ã®ä¿¡é ¼åº¦é–¾å€¤
        temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
        remasking: ãƒªãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
        cache_manager: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        block_id: ãƒ–ãƒ­ãƒƒã‚¯ID
        mask_id: ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ID

    Returns:
        (ç”ŸæˆæˆåŠŸãƒ•ãƒ©ã‚°, ãƒ–ãƒ­ãƒƒã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
    """
    block_start_time = time.time()
    nfe = 0
    confidence_scores = None
    final_logits = None
    final_mask_index = None
    cache_tier = None

    # ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ãƒã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    block_mask_index = (x[:, block_start:block_end] == mask_id)
    if not block_mask_index.any():
        # ã™ã§ã«ç”Ÿæˆæ¸ˆã¿
        return True, {
            'nfe': 0,
            'generation_time': 0,
            'confidence_scores': None,
            'final_logits': None,
            'final_mask_index': None,
            'cache_tier': None
        }

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å–å¾—
    past_key_values = None
    if cache_manager is not None:
        past_key_values = cache_manager.get_cache_for_block(block_id)
        if past_key_values is None:
            past_key_values = cache_manager.get_base_cache()

    # åˆæœŸæ¨è«–
    if past_key_values is not None:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šã®å ´åˆã¯éƒ¨åˆ†çš„ã«æ¨è«–
        output = model(x[:, block_start:block_end],
                       past_key_values=past_key_values, use_cache=True)
    else:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã®å ´åˆã¯å…¨ä½“ã‚’æ¨è«–
        output = model(x, use_cache=True)

    nfe += 1
    final_logits = output.logits

    # ãƒã‚¹ã‚¯ä½ç½®ã®æ›´æ–°
    mask_index = (x == mask_id)
    mask_index[:, block_end:] = 0  # ãƒ–ãƒ­ãƒƒã‚¯ç¯„å›²å¤–ã¯ãƒã‚¹ã‚¯ã—ãªã„
    final_mask_index = mask_index

    # ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰
    x0, transfer_index, confidence_probs = get_transfer_index_with_confidence(
        final_logits, temperature, remasking, mask_index, x,
        mask_index.sum(dim=1, keepdim=True)
    )

    # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if confidence_probs is not None:
        confidence_mask = confidence_probs >= current_threshold
        final_transfer_index = transfer_index & confidence_mask.unsqueeze(0)
    else:
        final_transfer_index = transfer_index

    # ãƒˆãƒ¼ã‚¯ãƒ³æ›´æ–°
    x[final_transfer_index] = x0[final_transfer_index]

    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    if confidence_probs is not None:
        block_confidences = confidence_probs[:, block_start:block_end]
        confidence_scores = block_confidences[block_mask_index[0]]

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
    if cache_manager is not None and confidence_scores is not None:
        cache_tier = cache_manager.update_cache(
            block_id=block_id,
            start_pos=block_start,
            end_pos=block_end,
            past_key_values=output.past_key_values,
            confidence_scores=confidence_scores
        )

    return True, {
        'nfe': nfe,
        'generation_time': time.time() - block_start_time,
        'confidence_scores': confidence_scores,
        'final_logits': final_logits,
        'final_mask_index': final_mask_index,
        'cache_tier': cache_tier
    }


@torch.no_grad()
def generate_with_custom_scheduler(
    model,
    prompt: torch.Tensor,
    scheduler: AdaptiveInferenceScheduler,
    cache_manager: Optional[TieredCacheManager] = None,
    gen_length: int = 128,
    temperature: float = 0.0,
    remasking: str = 'low_confidence',
    mask_id: Optional[int] = None,
    verbose: bool = True
) -> Tuple[torch.Tensor, Dict[str, Any]]:
    """
    ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆé–¢æ•°

    Args:
        model: LLaDAãƒ¢ãƒ‡ãƒ«
        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        scheduler: ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        cache_manager: ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        gen_length: ç”Ÿæˆé•·
        temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
        remasking: ãƒªãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
        mask_id: ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ID
        verbose: è©³ç´°å‡ºåŠ›

    Returns:
        (ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³, è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
    """
    return generate_with_adaptive_scheduling(
        model=model,
        prompt=prompt,
        gen_length=gen_length,
        base_block_size=scheduler.current_block_size,
        base_confidence_threshold=scheduler.current_threshold,
        enable_tiered_cache=cache_manager is not None,
        temperature=temperature,
        remasking=remasking,
        mask_id=mask_id,
        verbose=verbose
    )
