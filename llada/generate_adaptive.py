# Copyright 2025 NVIDIA CORPORATION & AFFILIATES
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

import torch
import torch.nn.functional as F
import numpy as np
from typing import List, Tuple, Optional, Dict, Any
from tqdm import tqdm
import time

from adaptive_scheduler import AdaptiveInferenceScheduler
from cache_manager import TieredCacheManager, CacheTier

# å¿…è¦ãªé–¢æ•°ã‚’import
try:
    from generate import get_transfer_index
except ImportError:
    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦è©¦è¡Œ
    from .generate import get_transfer_index


def add_gumbel_noise(logits, temperature):
    """
    Gumbel Max ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ã®ãƒã‚¤ã‚ºè¿½åŠ é–¢æ•°
    """
    if temperature == 0:
        return logits
    logits = logits.to(torch.float64)
    noise = torch.rand_like(logits, dtype=torch.float64)
    gumbel_noise = (- torch.log(noise)) ** temperature
    return logits.exp() / gumbel_noise


def get_num_transfer_tokens(mask_index, steps):
    """
    å„ã‚¹ãƒ†ãƒƒãƒ—ã§é·ç§»ã™ã¹ããƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’äº‹å‰è¨ˆç®—
    """
    mask_num = mask_index.sum(dim=1, keepdim=True)
    base = mask_num // steps
    remainder = mask_num % steps

    num_transfer_tokens = torch.zeros(mask_num.size(0), steps,
                                      device=mask_index.device, dtype=torch.int64) + base

    for i in range(mask_num.size(0)):
        num_transfer_tokens[i, :remainder[i]] += 1

    return num_transfer_tokens


def get_transfer_index_with_confidence(logits, temperature, remasking, mask_index, x, num_transfer_tokens):
    """
    ä¿¡é ¼åº¦ã‚’è€ƒæ…®ã—ãŸé·ç§»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®å–å¾—
    """
    logits_with_noise = add_gumbel_noise(logits, temperature=temperature)
    x0 = torch.argmax(logits_with_noise, dim=-1)

    # ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ç¢ºç‡ã®æœ€å¤§å€¤ï¼‰
    confidence = torch.softmax(logits, dim=-1).max(dim=-1)[0]

    if remasking == 'low_confidence':
        # ä½ä¿¡é ¼åº¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å„ªå…ˆçš„ã«é·ç§»
        mask_logits = logits.clone()
        mask_logits[~mask_index] = -float('inf')

        # ä¿¡é ¼åº¦ã®é€†é †ã§ã‚½ãƒ¼ãƒˆ
        confidence_masked = confidence.clone()
        confidence_masked[~mask_index] = float('inf')
        _, sorted_indices = torch.sort(confidence_masked, descending=False)

        transfer_index = torch.zeros_like(mask_index)
        for i in range(num_transfer_tokens.item()):
            if i < sorted_indices.size(-1):
                transfer_index.view(-1)[sorted_indices.view(-1)[i]] = True

    else:  # random
        # ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ
        masked_indices = torch.where(mask_index.view(-1))[0]
        if len(masked_indices) > 0:
            num_to_select = min(num_transfer_tokens.item(),
                                len(masked_indices))
            selected = torch.randperm(len(masked_indices))[:num_to_select]
            transfer_index = torch.zeros_like(mask_index).view(-1)
            transfer_index[masked_indices[selected]] = True
            transfer_index = transfer_index.view(mask_index.shape)
        else:
            transfer_index = torch.zeros_like(mask_index)

    return x0, transfer_index, confidence


@torch.no_grad()
def generate_with_adaptive_scheduling(
    model,
    prompt: torch.Tensor,
    gen_length: int = 128,
    base_block_size: int = 16,
    base_confidence_threshold: float = 0.8,
    adaptation_rate: float = 0.2,
    enable_tiered_cache: bool = True,
    temperature: float = 0.0,
    remasking: str = 'low_confidence',
    mask_id: Optional[int] = None,
    scheduler_config: Optional[Dict] = None,
    cache_config: Optional[Dict] = None,
    verbose: bool = True
) -> Tuple[torch.Tensor, Dict[str, Any]]:
    """
    Self-Correcting Adaptive Inference Scheduling for Diffusion LLMs

    ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆé–¢æ•°ã€‚å‹•çš„ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºèª¿æ•´ã€
    é©å¿œçš„ä¿¡é ¼åº¦é–¾å€¤ã€éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚’çµ±åˆã€‚

    Args:
        model: LLaDAãƒ¢ãƒ‡ãƒ«
        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (1, prompt_length)
        gen_length: ç”Ÿæˆã™ã‚‹é•·ã•
        base_block_size: åˆæœŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º
        base_confidence_threshold: åˆæœŸä¿¡é ¼åº¦é–¾å€¤
        adaptation_rate: é©å¿œç‡
        enable_tiered_cache: éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
        remasking: ãƒªãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
        mask_id: ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ID
        scheduler_config: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
        cache_config: ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        verbose: è©³ç´°å‡ºåŠ›

    Returns:
        (ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³, è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
    """

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    scheduler_config = scheduler_config or {}
    cache_config = cache_config or {}

    # ãƒã‚¹ã‚¯IDã®å–å¾— - LLaDAã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
    if mask_id is None:
        if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'mask_token_id'):
            mask_id = model.tokenizer.mask_token_id
        elif hasattr(model.config, 'mask_token_id'):
            mask_id = model.config.mask_token_id
        else:
            mask_id = 126336  # LLaDAã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ID

    if verbose:
        print(f"\nğŸš€ Adaptive Scheduling é–‹å§‹")
        print(f"   ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {prompt.shape[1]}")
        print(f"   ç”Ÿæˆé•·: {gen_length}")
        print(f"   åˆæœŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {base_block_size}")
        print(f"   åˆæœŸä¿¡é ¼åº¦é–¾å€¤: {base_confidence_threshold}")
        print(f"   ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ID: {mask_id}")
        print(f"   éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {enable_tiered_cache}")

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–
    scheduler = AdaptiveInferenceScheduler(
        min_block_size=max(4, base_block_size // 2),
        max_block_size=min(64, base_block_size * 3),
        base_confidence_threshold=base_confidence_threshold,
        adaptation_sensitivity=adaptation_rate,
        **scheduler_config
    )

    # åˆæœŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’è¨­å®šï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®å†…éƒ¨çŠ¶æ…‹ã«å¾“ã†ï¼‰
    current_block_size = scheduler.current_block_size
    current_threshold = scheduler.current_threshold

    if verbose:
        print(
            f"ğŸ¯ åˆæœŸè¨­å®š: ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º={current_block_size}, é–¾å€¤={current_threshold:.3f}")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®åˆæœŸåŒ–
    cache_manager = None
    if enable_tiered_cache:
        cache_manager = TieredCacheManager(**cache_config)

    # åˆæœŸè¨­å®š
    x = torch.full((1, prompt.shape[1] + gen_length),
                   mask_id, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
    metrics = {
        'nfe': 0,
        'total_adaptations': 0,
        'block_size_history': [],
        'threshold_history': [],
        'confidence_history': [],
        'entropy_history': [],
        'cache_efficiency': {},
        'timing': {
            'total_time': 0,
            'adaptation_time': 0,
            'generation_time': 0,
            'cache_time': 0
        },
        'blocks_processed': 0,
        'tier_usage': {'tier1': 0, 'tier2': 0, 'tier3': 0}
    }

    start_time = time.time()

    # Phase 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è¨­å®š
    if enable_tiered_cache:
        cache_start = time.time()
        output = model(x[:, :prompt.shape[1]], use_cache=True)
        cache_manager.set_prompt_cache(prompt.shape[1], output.past_key_values)
        metrics['timing']['cache_time'] += time.time() - cache_start
        metrics['nfe'] += 1

        if verbose:
            print(f"âœ… Tier1ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šå®Œäº† (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {prompt.shape[1]})")

    # Phase 2: ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ç”Ÿæˆ
    generated_tokens = 0
    block_id = 0

    with tqdm(total=gen_length, desc="Adaptive Generation", disable=not verbose) as pbar:
        while generated_tokens < gen_length:
            block_start_time = time.time()

            # ç¾åœ¨ã®ãƒ–ãƒ­ãƒƒã‚¯ç¯„å›²ã‚’æ±ºå®š
            block_start = prompt.shape[1] + generated_tokens
            remaining_tokens = gen_length - generated_tokens
            actual_block_size = min(current_block_size, remaining_tokens)
            block_end = block_start + actual_block_size

            if verbose and block_id % 5 == 0:
                print(
                    f"\nğŸ“¦ ãƒ–ãƒ­ãƒƒã‚¯ {block_id}: ã‚µã‚¤ã‚º={actual_block_size}, é–¾å€¤={current_threshold:.3f}")

            # ãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆ - å®Œå…¨ãªåå¾©çš„ç”Ÿæˆã‚’å®Ÿè¡Œ
            block_generated, block_metrics = _generate_block_adaptive_complete(
                model=model,
                x=x,
                block_start=block_start,
                block_end=block_end,
                current_threshold=current_threshold,
                temperature=temperature,
                remasking=remasking,
                cache_manager=cache_manager,
                block_id=block_id,
                mask_id=mask_id,
                steps=8  # ãƒ–ãƒ­ãƒƒã‚¯å†…ã§ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°
            )

            metrics['nfe'] += block_metrics['nfe']
            metrics['timing']['generation_time'] += block_metrics['generation_time']

            if verbose:
                print(f"ğŸ“ˆ ãƒ–ãƒ­ãƒƒã‚¯ {block_id} å®Œäº†: NFE={block_metrics['nfe']}, "
                      f"ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢æœ‰ç„¡={block_metrics['confidence_scores'] is not None}")
                if block_metrics['confidence_scores'] is not None:
                    avg_conf = block_metrics['confidence_scores'].mean().item() if len(
                        block_metrics['confidence_scores']) > 0 else 0.0
                    print(f"   å¹³å‡ä¿¡é ¼åº¦: {avg_conf:.3f}")

            # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–èª¿æ•´
            if (block_metrics['confidence_scores'] is not None and
                block_metrics['final_logits'] is not None and
                    block_metrics['final_mask_index'] is not None):
                adaptation_start = time.time()

                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã«ã‚ˆã‚‹é©å¿œï¼ˆãƒ–ãƒ­ãƒƒã‚¯åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã™ï¼‰
                next_block_size, adapted_threshold, step_metrics = scheduler.step(
                    logits=block_metrics['final_logits'],
                    tokens=x[:, block_start:block_end],
                    mask_index=block_metrics['final_mask_index'],
                    step_num=block_id,
                    total_steps=gen_length // base_block_size
                )

                # é©å¿œæ¤œå‡º
                if (next_block_size != current_block_size or
                        abs(adapted_threshold - current_threshold) > 0.01):
                    metrics['total_adaptations'] += 1
                    if verbose:
                        print(f"ğŸ”„ é©å¿œ: ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º {current_block_size}â†’{next_block_size}, "
                              f"é–¾å€¤ {current_threshold:.3f}â†’{adapted_threshold:.3f}")

                # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‹ã‚‰ã®é©å¿œçµæœã‚’ä½¿ç”¨ï¼ˆé‡è¦ãªä¿®æ­£ï¼‰
                current_block_size = next_block_size
                current_threshold = adapted_threshold

                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²ï¼ˆå®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’è¨˜éŒ²ï¼‰
                metrics['block_size_history'].append(actual_block_size)
                metrics['threshold_history'].append(adapted_threshold)
                metrics['confidence_history'].append(
                    step_metrics['confidence'])
                metrics['entropy_history'].append(step_metrics['entropy'])

                metrics['timing']['adaptation_time'] += time.time() - \
                    adaptation_start
            else:
                # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if verbose:
                    print(f"âš ï¸  ãƒ–ãƒ­ãƒƒã‚¯ {block_id}: é©å¿œãƒ‡ãƒ¼ã‚¿ä¸è¶³ã€é©å¿œã‚’ã‚¹ã‚­ãƒƒãƒ—")
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§è¨˜éŒ²
                metrics['block_size_history'].append(actual_block_size)
                metrics['threshold_history'].append(current_threshold)
                metrics['confidence_history'].append(0.0)
                metrics['entropy_history'].append(0.0)

            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨çŠ¶æ³ã®è¨˜éŒ²
            if enable_tiered_cache and block_metrics['cache_tier']:
                if block_metrics['cache_tier'] == CacheTier.FROZEN:
                    metrics['tier_usage']['tier1'] += 1
                elif block_metrics['cache_tier'] == CacheTier.STABLE:
                    metrics['tier_usage']['tier2'] += 1
                elif block_metrics['cache_tier'] == CacheTier.ACTIVE:
                    metrics['tier_usage']['tier3'] += 1

            # é€²æ—æ›´æ–°
            generated_tokens += actual_block_size
            metrics['blocks_processed'] += 1
            block_id += 1

            pbar.update(actual_block_size)
            pbar.set_postfix({
                'B': actual_block_size,
                'Ï„': f"{current_threshold:.2f}",
                'NFE': metrics['nfe'],
                'Adapt': metrics['total_adaptations']
            })

            metrics['timing']['generation_time'] += time.time() - \
                block_start_time

    # æœ€çµ‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    metrics['timing']['total_time'] = time.time() - start_time

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    scheduler_metrics = scheduler.get_adaptation_metrics()
    metrics.update({
        'avg_block_size': np.mean(metrics['block_size_history']) if metrics['block_size_history'] else base_block_size,
        'final_threshold': scheduler_metrics['current_threshold'],
        'adaptation_rate': scheduler_metrics['adaptation_count'] / max(1, scheduler_metrics['total_blocks'])
    })

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    if enable_tiered_cache:
        cache_metrics = cache_manager.get_cache_efficiency_metrics()
        metrics['cache_efficiency'] = cache_metrics

        if verbose:
            cache_manager.print_cache_status()

    # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
    if verbose:
        print(f"\nğŸ‰ Adaptive Scheduling å®Œäº†!")
        print(f"   ç·æ™‚é–“: {metrics['timing']['total_time']:.2f}ç§’")
        print(f"   ç·NFE: {metrics['nfe']}")
        print(f"   é©å¿œå›æ•°: {metrics['total_adaptations']}")
        print(f"   å¹³å‡ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {np.mean(metrics['block_size_history']):.1f}")
        print(f"   æœ€çµ‚ä¿¡é ¼åº¦é–¾å€¤: {metrics['final_threshold']:.3f}")
        if enable_tiered_cache:
            print(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {cache_metrics['cache_hit_rate']:.2%}")

    return x, metrics


def _generate_block_adaptive_complete(
    model,
    x: torch.Tensor,
    block_start: int,
    block_end: int,
    current_threshold: float,
    temperature: float,
    remasking: str,
    cache_manager: Optional[TieredCacheManager],
    block_id: int,
    mask_id: int,
    steps: int = 8
) -> Tuple[bool, Dict[str, Any]]:
    """
    å®Œå…¨ãªãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆï¼ˆdual_cacheãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã†ï¼‰

    Args:
        model: LLaDAãƒ¢ãƒ‡ãƒ«
        x: ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ãƒ³ã‚½ãƒ«
        block_start: ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹ä½ç½®
        block_end: ãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†ä½ç½®
        current_threshold: ç¾åœ¨ã®ä¿¡é ¼åº¦é–¾å€¤
        temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
        remasking: ãƒªãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
        cache_manager: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        block_id: ãƒ–ãƒ­ãƒƒã‚¯ID
        mask_id: ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ID
        steps: ãƒ–ãƒ­ãƒƒã‚¯å†…ã§ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°

    Returns:
        (ç”ŸæˆæˆåŠŸãƒ•ãƒ©ã‚°, ãƒ–ãƒ­ãƒƒã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
    """
    block_start_time = time.time()
    nfe = 0
    confidence_scores = None
    final_logits = None
    final_mask_index = None
    cache_tier = None

    # ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ãƒã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    block_mask_index = (x[:, block_start:block_end] == mask_id)
    if not block_mask_index.any():
        # ã™ã§ã«ç”Ÿæˆæ¸ˆã¿
        return True, {
            'nfe': 0,
            'generation_time': 0,
            'confidence_scores': None,
            'final_logits': None,
            'final_mask_index': None,
            'cache_tier': None
        }

    # ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®è¨ˆç®—
    num_transfer_tokens = get_num_transfer_tokens(block_mask_index, steps)

    # åˆæœŸæ¨è«–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®šã¨ç¬¬1ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
    output = model(x, use_cache=True)
    past_key_values = output.past_key_values
    nfe += 1

    # å…¨ä½“ã®ãƒã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ–ãƒ­ãƒƒã‚¯ç¯„å›²å¤–ã¯ãƒã‚¹ã‚¯ã—ãªã„ï¼‰
    mask_index = (x == mask_id)
    mask_index[:, block_end:] = 0

    # ç¬¬1ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ
    if current_threshold is not None:
        # é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®ç”Ÿæˆ - dual_cacheã¨åŒæ§˜ã®å‡¦ç†
        x0, transfer_index = get_transfer_index(
            output.logits, temperature, remasking, mask_index, x,
            None, current_threshold)
    else:
        # é€šå¸¸ã®ã‚¹ãƒ†ãƒƒãƒ—ãƒ™ãƒ¼ã‚¹ç”Ÿæˆ
        x0, transfer_index = get_transfer_index(
            output.logits, temperature, remasking, mask_index, x,
            num_transfer_tokens[:, 0], None)

    x[transfer_index] = x0[transfer_index]

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã‚’è¨­å®šï¼ˆdual_cacheãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
    replace_position = torch.zeros_like(x, dtype=torch.bool)
    replace_position[:, block_start:block_end] = 1

    # åå¾©çš„ç”Ÿæˆï¼ˆæ®‹ã‚Šã®ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
    i = 1
    while True:
        nfe += 1

        # ç¾åœ¨ã®ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ãƒã‚¹ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
        current_mask_index = (x[:, block_start:block_end] == mask_id)

        if not current_mask_index.any():
            # ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ã™ã¹ã¦ã®ãƒã‚¹ã‚¯ãŒè§£æ±ºã•ã‚ŒãŸ
            break

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¦ãƒ–ãƒ­ãƒƒã‚¯ç¯„å›²ã®ã¿æ¨è«–
        logits = model(x[:, block_start:block_end],
                       past_key_values=past_key_values,
                       use_cache=True,
                       replace_position=replace_position).logits

        # ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ - dual_cacheãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã†
        if current_threshold is not None:
            # é–¾å€¤ãƒ™ãƒ¼ã‚¹ã®ç”Ÿæˆ
            x0, transfer_index = get_transfer_index(
                logits, temperature, remasking, current_mask_index,
                x[:, block_start:block_end], None, current_threshold)
        else:
            # ã‚¹ãƒ†ãƒƒãƒ—ãƒ™ãƒ¼ã‚¹ç”Ÿæˆ
            if i < steps:
                x0, transfer_index = get_transfer_index(
                    logits, temperature, remasking, current_mask_index,
                    x[:, block_start:block_end], num_transfer_tokens[:, i], None)
            else:
                # æœ€çµ‚ã‚¹ãƒ†ãƒƒãƒ—ï¼šæ®‹ã‚Šã™ã¹ã¦ã‚’ç”Ÿæˆ
                x0, transfer_index = get_transfer_index(
                    logits, temperature, remasking, current_mask_index,
                    x[:, block_start:block_end], current_mask_index.sum(dim=1, keepdim=True), None)

        # ãƒ–ãƒ­ãƒƒã‚¯ç¯„å›²ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ›´æ–°
        x[:, block_start:block_end][transfer_index] = x0[transfer_index]

        # æœ€å¾Œã®ãƒ­ã‚¸ãƒƒãƒˆã¨ä¿¡é ¼åº¦ã‚’ä¿å­˜
        final_logits = logits
        final_mask_index = current_mask_index

        # ä¿¡é ¼åº¦è¨ˆç®—
        if remasking == 'low_confidence':
            p = F.softmax(logits.to(torch.float64), dim=-1)
            x0_p = torch.squeeze(
                torch.gather(p, dim=-1, index=torch.unsqueeze(x0, -1)), -1)
            confidence_scores = torch.where(current_mask_index, x0_p, -np.inf)

        i += 1

        # å®‰å…¨æ©Ÿæ§‹ï¼šç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
        if i > steps * 2:
            break

    # æœ€çµ‚çš„ãªä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆãƒ–ãƒ­ãƒƒã‚¯å…¨ä½“ã®å¹³å‡ï¼‰
    if final_logits is not None and final_mask_index is not None:
        actual_block_size = block_end - block_start
        if remasking == 'low_confidence':
            p = F.softmax(final_logits.to(torch.float64), dim=-1)
            # æœ€çµ‚çš„ã«ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—
            final_generated_tokens = x[:, block_start:block_end]
            x0_p = torch.squeeze(
                torch.gather(p, dim=-1, index=torch.unsqueeze(final_generated_tokens, -1)), -1)
            # ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ãŸä½ç½®ã®ä¿¡é ¼åº¦ã®ã¿ã‚’å–å¾—
            block_mask_was_generated = (x[:, block_start:block_end] != mask_id)
            if block_mask_was_generated.any():
                confidence_scores = x0_p[block_mask_was_generated]
            else:
                # ãƒã‚¹ã‚¯ã•ã‚ŒãŸä½ç½®ãŒãªã„å ´åˆã§ã‚‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
                confidence_scores = torch.tensor([0.5])
        else:
            # ãƒ©ãƒ³ãƒ€ãƒ æˆ¦ç•¥ã®å ´åˆã¯ä¸€å¾‹ã®ä¿¡é ¼åº¦
            confidence_scores = torch.ones(max(1, actual_block_size)) * 0.5
    else:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ­ã‚¸ãƒƒãƒˆã‚„ãƒã‚¹ã‚¯ãŒãªã„å ´åˆ
        confidence_scores = torch.tensor([0.5])

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
    if cache_manager is not None and confidence_scores is not None and len(confidence_scores) > 0:
        cache_tier = cache_manager.update_cache(
            block_id=block_id,
            start_pos=block_start,
            end_pos=block_end,
            past_key_values=output.past_key_values,
            confidence_scores=confidence_scores
        )

    return True, {
        'nfe': nfe,
        'generation_time': time.time() - block_start_time,
        'confidence_scores': confidence_scores,
        'final_logits': final_logits,
        'final_mask_index': final_mask_index,
        'cache_tier': cache_tier
    }


def _generate_block_adaptive(
    model,
    x: torch.Tensor,
    block_start: int,
    block_end: int,
    current_threshold: float,
    temperature: float,
    remasking: str,
    cache_manager: Optional[TieredCacheManager],
    block_id: int,
    mask_id: int
) -> Tuple[bool, Dict[str, Any]]:
    """
    ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ç”¨ãƒ–ãƒ­ãƒƒã‚¯ç”Ÿæˆ

    Args:
        model: LLaDAãƒ¢ãƒ‡ãƒ«
        x: ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ†ãƒ³ã‚½ãƒ«
        block_start: ãƒ–ãƒ­ãƒƒã‚¯é–‹å§‹ä½ç½®
        block_end: ãƒ–ãƒ­ãƒƒã‚¯çµ‚äº†ä½ç½®
        current_threshold: ç¾åœ¨ã®ä¿¡é ¼åº¦é–¾å€¤
        temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
        remasking: ãƒªãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
        cache_manager: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        block_id: ãƒ–ãƒ­ãƒƒã‚¯ID
        mask_id: ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ID

    Returns:
        (ç”ŸæˆæˆåŠŸãƒ•ãƒ©ã‚°, ãƒ–ãƒ­ãƒƒã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
    """
    block_start_time = time.time()
    nfe = 0
    confidence_scores = None
    final_logits = None
    final_mask_index = None
    cache_tier = None

    # ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ãƒã‚¹ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    block_mask_index = (x[:, block_start:block_end] == mask_id)
    if not block_mask_index.any():
        # ã™ã§ã«ç”Ÿæˆæ¸ˆã¿
        return True, {
            'nfe': 0,
            'generation_time': 0,
            'confidence_scores': None,
            'final_logits': None,
            'final_mask_index': None,
            'cache_tier': None
        }

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å–å¾—
    past_key_values = None
    if cache_manager is not None:
        past_key_values = cache_manager.get_cache_for_block(block_id)
        if past_key_values is None:
            past_key_values = cache_manager.get_base_cache()

    # åˆæœŸæ¨è«–
    if past_key_values is not None:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šã®å ´åˆã¯éƒ¨åˆ†çš„ã«æ¨è«–
        output = model(x[:, block_start:block_end],
                       past_key_values=past_key_values, use_cache=True)
    else:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ã®å ´åˆã¯å…¨ä½“ã‚’æ¨è«–
        output = model(x, use_cache=True)

    nfe += 1
    final_logits = output.logits

    # ãƒã‚¹ã‚¯ä½ç½®ã®æ›´æ–°
    mask_index = (x == mask_id)
    mask_index[:, block_end:] = 0  # ãƒ–ãƒ­ãƒƒã‚¯ç¯„å›²å¤–ã¯ãƒã‚¹ã‚¯ã—ãªã„
    final_mask_index = mask_index

    # ãƒ–ãƒ­ãƒƒã‚¯ç¯„å›²ã«é™å®šã—ãŸãƒã‚¹ã‚¯ã¨ãƒ­ã‚¸ãƒƒãƒˆã‚’ä½¿ç”¨
    actual_block_size = block_end - block_start
    block_logits = final_logits[:, -actual_block_size:]  # ãƒ–ãƒ­ãƒƒã‚¯åˆ†ã®ãƒ­ã‚¸ãƒƒãƒˆã®ã¿
    block_mask_index = mask_index[:, block_start:block_end]  # ãƒ–ãƒ­ãƒƒã‚¯åˆ†ã®ãƒã‚¹ã‚¯ã®ã¿

    # ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆï¼ˆä¿¡é ¼åº¦ä»˜ãï¼‰
    x0, transfer_index, confidence_probs = get_transfer_index_with_confidence(
        block_logits, temperature, remasking, block_mask_index, x[:,
                                                                  block_start:block_end],
        block_mask_index.sum(dim=1, keepdim=True)
    )

    # ä¿¡é ¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    if confidence_probs is not None:
        confidence_mask = confidence_probs >= current_threshold
        final_transfer_index = transfer_index & confidence_mask
    else:
        final_transfer_index = transfer_index

    # ãƒ–ãƒ­ãƒƒã‚¯ç¯„å›²ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³æ›´æ–°
    block_x0 = x0  # ãƒ–ãƒ­ãƒƒã‚¯åˆ†ã®ãƒˆãƒ¼ã‚¯ãƒ³
    # final_transfer_indexã‚’ä½¿ã£ã¦ãƒ–ãƒ­ãƒƒã‚¯å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ›´æ–°
    if final_transfer_index.any():
        x[:, block_start:block_end][final_transfer_index] = block_x0[final_transfer_index]

    # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    if confidence_probs is not None:
        # confidence_probsã¯ã™ã§ã«ãƒ–ãƒ­ãƒƒã‚¯ç¯„å›²ãªã®ã§ã€ãã®ã¾ã¾ä½¿ç”¨
        # block_mask_indexãŒ2æ¬¡å…ƒãªã®ã§ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸéƒ¨åˆ†ã®ã¿æŠ½å‡º
        if block_mask_index.any():
            confidence_scores = confidence_probs[block_mask_index]
        else:
            confidence_scores = torch.tensor([])

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
    if cache_manager is not None and confidence_scores is not None:
        cache_tier = cache_manager.update_cache(
            block_id=block_id,
            start_pos=block_start,
            end_pos=block_end,
            past_key_values=output.past_key_values,
            confidence_scores=confidence_scores
        )

    return True, {
        'nfe': nfe,
        'generation_time': time.time() - block_start_time,
        'confidence_scores': confidence_scores,
        'final_logits': block_logits,  # ãƒ–ãƒ­ãƒƒã‚¯åˆ†ã®ãƒ­ã‚¸ãƒƒãƒˆã®ã¿è¿”ã™
        'final_mask_index': block_mask_index,  # ãƒ–ãƒ­ãƒƒã‚¯åˆ†ã®ãƒã‚¹ã‚¯ã®ã¿è¿”ã™
        'cache_tier': cache_tier
    }


@torch.no_grad()
def generate_with_custom_scheduler(
    model,
    prompt: torch.Tensor,
    scheduler: AdaptiveInferenceScheduler,
    cache_manager: Optional[TieredCacheManager] = None,
    gen_length: int = 128,
    temperature: float = 0.0,
    remasking: str = 'low_confidence',
    mask_id: Optional[int] = None,
    verbose: bool = True
) -> Tuple[torch.Tensor, Dict[str, Any]]:
    """
    ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆé–¢æ•°

    Args:
        model: LLaDAãƒ¢ãƒ‡ãƒ«
        prompt: å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        scheduler: ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        cache_manager: ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        gen_length: ç”Ÿæˆé•·
        temperature: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
        remasking: ãƒªãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥
        mask_id: ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ID
        verbose: è©³ç´°å‡ºåŠ›

    Returns:
        (ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³, è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹)
    """
    return generate_with_adaptive_scheduling(
        model=model,
        prompt=prompt,
        gen_length=gen_length,
        base_block_size=scheduler.current_block_size,
        base_confidence_threshold=scheduler.current_threshold,
        enable_tiered_cache=cache_manager is not None,
        temperature=temperature,
        remasking=remasking,
        mask_id=mask_id,
        verbose=verbose
    )
