# Copyright 2025 NVIDIA CORPORATION & AFFILIATES
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

"""
Self-Correcting Adaptive Inference Scheduling ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ç”¨æ–¹æ³•ã‚’
å®Ÿéš›ã®ä¾‹ã¨ã¨ã‚‚ã«ç¤ºã—ã¾ã™ã€‚
"""

from cache_manager import TieredCacheManager
from adaptive_scheduler import AdaptiveInferenceScheduler
from generate_adaptive import generate_with_adaptive_scheduling, generate_with_custom_scheduler
from model.modeling_llada import LLaDAModelLM
import torch
import time
from transformers import AutoTokenizer
import sys
import os

# ãƒ‘ã‚¹ã®è¿½åŠ ï¼ˆè¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ï¼‰
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def demo_basic_usage():
    """åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ã®ãƒ‡ãƒ¢"""
    print("ğŸš€ åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•ã®ãƒ‡ãƒ¢")
    print("=" * 50)

    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
    print("ğŸ“¥ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    model_name = "GSAI-ML/LLaDA-8B-Instruct"
    model = LLaDAModelLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    if torch.cuda.is_available():
        model = model.cuda()
        device = "cuda"
    else:
        device = "cpu"

    model.eval()
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† (ãƒ‡ãƒã‚¤ã‚¹: {device})")

    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt_text = "Write a Python function to calculate the factorial of a number:"
    prompt = tokenizer.encode(prompt_text, return_tensors='pt').to(device)

    print(f"\nğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt_text}")
    print(f"   ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: {prompt.shape[1]} ãƒˆãƒ¼ã‚¯ãƒ³")

    # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã§ç”Ÿæˆ
    print(f"\nğŸ”§ ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    start_time = time.time()

    output, metrics = generate_with_adaptive_scheduling(
        model=model,
        prompt=prompt,
        gen_length=128,
        base_block_size=16,
        base_confidence_threshold=0.8,
        adaptation_rate=0.2,
        enable_tiered_cache=True,
        verbose=True
    )

    end_time = time.time()

    # çµæœã®è¡¨ç¤º
    generated_text = tokenizer.decode(
        output[0, prompt.shape[1]:], skip_special_tokens=True)

    print(f"\nğŸ“„ ç”Ÿæˆçµæœ:")
    print(f"   ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")
    print(f"   ç·NFE: {metrics['nfe']}")
    print(f"   é©å¿œå›æ•°: {metrics['total_adaptations']}")
    print(f"   å¹³å‡ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {metrics.get('avg_block_size', 'N/A')}")

    if metrics.get('cache_efficiency'):
        cache_metrics = metrics['cache_efficiency']
        print(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {cache_metrics['cache_hit_rate']:.2%}")
        print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {cache_metrics['memory_usage']['total_mb']:.1f} MB")

    print(f"\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:")
    print("-" * 40)
    print(generated_text)
    print("-" * 40)


def demo_configuration_comparison():
    """ç•°ãªã‚‹è¨­å®šã§ã®æ¯”è¼ƒãƒ‡ãƒ¢"""
    print("\nğŸ”¬ è¨­å®šæ¯”è¼ƒãƒ‡ãƒ¢")
    print("=" * 50)

    # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ï¼ˆç°¡ç•¥åŒ–ï¼‰
    model_name = "GSAI-ML/LLaDA-8B-Instruct"
    model = LLaDAModelLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device).eval()

    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt_text = "Solve this math problem step by step: If a car travels 240 km in 3 hours, what is its average speed?"
    prompt = tokenizer.encode(prompt_text, return_tensors='pt').to(device)

    print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt_text}")

    # è¨­å®š1: ä¿å®ˆçš„è¨­å®šï¼ˆå°ã•ãªãƒ–ãƒ­ãƒƒã‚¯ã€é«˜ã„é–¾å€¤ï¼‰
    print(f"\nğŸ›¡ï¸ ä¿å®ˆçš„è¨­å®š")
    conservative_start = time.time()
    conservative_output, conservative_metrics = generate_with_adaptive_scheduling(
        model=model,
        prompt=prompt,
        gen_length=96,
        base_block_size=8,
        base_confidence_threshold=0.9,
        adaptation_rate=0.1,
        verbose=False
    )
    conservative_time = time.time() - conservative_start

    # è¨­å®š2: ç©æ¥µçš„è¨­å®šï¼ˆå¤§ããªãƒ–ãƒ­ãƒƒã‚¯ã€ä½ã„é–¾å€¤ï¼‰
    print(f"âš¡ ç©æ¥µçš„è¨­å®š")
    aggressive_start = time.time()
    aggressive_output, aggressive_metrics = generate_with_adaptive_scheduling(
        model=model,
        prompt=prompt,
        gen_length=96,
        base_block_size=32,
        base_confidence_threshold=0.7,
        adaptation_rate=0.3,
        verbose=False
    )
    aggressive_time = time.time() - aggressive_start

    # è¨­å®š3: ãƒãƒ©ãƒ³ã‚¹è¨­å®š
    print(f"âš–ï¸ ãƒãƒ©ãƒ³ã‚¹è¨­å®š")
    balanced_start = time.time()
    balanced_output, balanced_metrics = generate_with_adaptive_scheduling(
        model=model,
        prompt=prompt,
        gen_length=96,
        base_block_size=16,
        base_confidence_threshold=0.8,
        adaptation_rate=0.2,
        verbose=False
    )
    balanced_time = time.time() - balanced_start

    # çµæœæ¯”è¼ƒ
    print(f"\nğŸ“Š çµæœæ¯”è¼ƒ:")
    print(f"{'è¨­å®š':<12} {'æ™‚é–“(s)':<10} {'NFE':<8} {'é©å¿œå›æ•°':<10} {'ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º':<12}")
    print("-" * 60)

    print(f"{'ä¿å®ˆçš„':<12} {conservative_time:<10.2f} {conservative_metrics['nfe']:<8} "
          f"{conservative_metrics['total_adaptations']:<10} {conservative_metrics.get('avg_block_size', 'N/A'):<12}")

    print(f"{'ç©æ¥µçš„':<12} {aggressive_time:<10.2f} {aggressive_metrics['nfe']:<8} "
          f"{aggressive_metrics['total_adaptations']:<10} {aggressive_metrics.get('avg_block_size', 'N/A'):<12}")

    print(f"{'ãƒãƒ©ãƒ³ã‚¹':<12} {balanced_time:<10.2f} {balanced_metrics['nfe']:<8} "
          f"{balanced_metrics['total_adaptations']:<10} {balanced_metrics.get('avg_block_size', 'N/A'):<12}")


def demo_custom_scheduler():
    """ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ãƒ‡ãƒ¢"""
    print("\nğŸ›ï¸ ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ãƒ‡ãƒ¢")
    print("=" * 50)

    # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
    model_name = "GSAI-ML/LLaDA-8B-Instruct"
    model = LLaDAModelLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device).eval()

    # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ä½œæˆ
    custom_scheduler = AdaptiveInferenceScheduler(
        min_block_size=4,
        max_block_size=48,
        base_confidence_threshold=0.85,
        adaptation_sensitivity=0.25,
        entropy_threshold_high=1.8,
        entropy_threshold_low=0.3,
        scale_up_factor=1.5,
        scale_down_factor=0.7
    )

    # ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®ä½œæˆ
    custom_cache_manager = TieredCacheManager(
        tier2_stability_threshold=0.9,
        tier2_update_interval=2,
        max_stable_blocks=16,
        memory_efficiency_mode=True
    )

    print(f"ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ è¨­å®š:")
    print(
        f"   ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºç¯„å›²: {custom_scheduler.min_block_size}-{custom_scheduler.max_block_size}")
    print(f"   åŸºæœ¬ä¿¡é ¼åº¦é–¾å€¤: {custom_scheduler.base_confidence_threshold}")
    print(f"   é©å¿œæ„Ÿåº¦: {custom_scheduler.adaptation_sensitivity}")
    print(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥å®‰å®šåŒ–é–¾å€¤: {custom_cache_manager.tier2_stability_threshold}")

    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt_text = "Explain the concept of machine learning in simple terms for a beginner:"
    prompt = tokenizer.encode(prompt_text, return_tensors='pt').to(device)

    print(f"\nğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt_text}")

    # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã§ç”Ÿæˆ
    start_time = time.time()
    output, metrics = generate_with_custom_scheduler(
        model=model,
        prompt=prompt,
        scheduler=custom_scheduler,
        cache_manager=custom_cache_manager,
        gen_length=128,
        verbose=True
    )
    end_time = time.time()

    # çµæœã®è¡¨ç¤º
    generated_text = tokenizer.decode(
        output[0, prompt.shape[1]:], skip_special_tokens=True)

    print(f"\nğŸ“Š ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼çµæœ:")
    print(f"   ç”Ÿæˆæ™‚é–“: {end_time - start_time:.2f}ç§’")
    print(f"   ç·NFE: {metrics['nfe']}")
    print(f"   é©å¿œå›æ•°: {metrics['total_adaptations']}")

    # é©å¿œãƒ¡ãƒˆãƒªã‚¯ã‚¹
    scheduler_metrics = custom_scheduler.get_adaptation_metrics()
    print(f"   æœ€çµ‚ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {scheduler_metrics['current_block_size']}")
    print(f"   æœ€çµ‚ä¿¡é ¼åº¦é–¾å€¤: {scheduler_metrics['current_threshold']:.3f}")
    print(
        f"   é©å¿œç‡: {scheduler_metrics['adaptation_count'] / max(1, scheduler_metrics['total_blocks']):.2%}")

    print(f"\nğŸ“ ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:")
    print("-" * 40)
    print(generated_text)
    print("-" * 40)


def demo_content_adaptation():
    """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹é©å¿œã®é•ã„ã®ãƒ‡ãƒ¢"""
    print("\nğŸ­ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é©å¿œãƒ‡ãƒ¢")
    print("=" * 50)

    # ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
    model_name = "GSAI-ML/LLaDA-8B-Instruct"
    model = LLaDAModelLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device).eval()

    # ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    test_prompts = [
        {
            "name": "æ•°å­¦å•é¡Œ",
            "text": "Calculate: (15 + 25) Ã— 3 - 18 Ã· 2 =",
            "expected": "äºˆæ¸¬å¯èƒ½ï¼ˆä½é©å¿œï¼‰"
        },
        {
            "name": "å‰µä½œæ–‡ç« ",
            "text": "Write a creative story about a time-traveling cat:",
            "expected": "ä¸ç¢ºå®Ÿï¼ˆé«˜é©å¿œï¼‰"
        },
        {
            "name": "äº‹å®Ÿè³ªå•",
            "text": "What is the capital of Japan?",
            "expected": "ç¢ºå®Ÿï¼ˆä½é©å¿œï¼‰"
        },
        {
            "name": "è¤‡é›‘æ¨è«–",
            "text": "Explain why quantum computers might be able to solve certain problems faster than classical computers:",
            "expected": "è¤‡é›‘ï¼ˆä¸­ã€œé«˜é©å¿œï¼‰"
        }
    ]

    results = []

    for prompt_info in test_prompts:
        print(f"\nğŸ“ {prompt_info['name']}: {prompt_info['text']}")
        print(f"   äºˆæƒ³: {prompt_info['expected']}")

        prompt = tokenizer.encode(
            prompt_info['text'], return_tensors='pt').to(device)

        start_time = time.time()
        output, metrics = generate_with_adaptive_scheduling(
            model=model,
            prompt=prompt,
            gen_length=64,
            base_block_size=16,
            base_confidence_threshold=0.8,
            adaptation_rate=0.2,
            verbose=False
        )
        end_time = time.time()

        # çµæœè¨˜éŒ²
        result = {
            'name': prompt_info['name'],
            'time': end_time - start_time,
            'nfe': metrics['nfe'],
            'adaptations': metrics['total_adaptations'],
            'avg_block_size': metrics.get('avg_block_size', 16),
            'avg_confidence': np.mean(metrics['confidence_history']) if metrics['confidence_history'] else 0,
            'avg_entropy': np.mean(metrics['entropy_history']) if metrics['entropy_history'] else 0
        }
        results.append(result)

        print(
            f"   çµæœ: æ™‚é–“={result['time']:.2f}s, NFE={result['nfe']}, é©å¿œ={result['adaptations']}")
        if result['avg_confidence'] > 0:
            print(
                f"   ãƒ¡ãƒˆãƒªã‚¯ã‚¹: å¹³å‡ä¿¡é ¼åº¦={result['avg_confidence']:.3f}, å¹³å‡ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼={result['avg_entropy']:.3f}")

    # é©å¿œãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ
    print(f"\nğŸ“Š é©å¿œãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ:")
    print(f"{'ã‚¿ã‚¤ãƒ—':<12} {'é©å¿œå›æ•°':<8} {'ä¿¡é ¼åº¦':<10} {'ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼':<10} {'ãƒ‘ã‚¿ãƒ¼ãƒ³'}")
    print("-" * 70)

    for result in results:
        if result['avg_confidence'] > 0.85 and result['adaptations'] <= 2:
            pattern = "å®‰å®šå‹"
        elif result['avg_entropy'] > 1.5 and result['adaptations'] >= 3:
            pattern = "é©å¿œå‹"
        else:
            pattern = "ä¸­é–“å‹"

        print(f"{result['name']:<12} {result['adaptations']:<8} {result['avg_confidence']:<10.3f} "
              f"{result['avg_entropy']:<10.3f} {pattern}")


def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("ğŸ¯ Self-Correcting Adaptive Inference Scheduling")
    print("   å®Ÿç”¨ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("="*60)

    try:
        # åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•
        demo_basic_usage()

        # è¨­å®šæ¯”è¼ƒ
        demo_configuration_comparison()

        # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        demo_custom_scheduler()

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é©å¿œ
        demo_content_adaptation()

        print(f"\nğŸ‰ å…¨ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†!")
        print(f"   ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®å„æ©Ÿèƒ½ãŒ")
        print(f"   æ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # numpy ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
    import numpy as np
    main()
