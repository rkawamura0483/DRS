#!/usr/bin/env python3
# Copyright 2025 NVIDIA CORPORATION & AFFILIATES
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

"""
Self-Correcting Adaptive Inference Scheduling å®Ÿè£…æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®
å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
"""

import torch
import sys
import traceback
from typing import Tuple, Dict, Any


def test_imports() -> bool:
    """å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
    try:
        print("ğŸ“¦ ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ...")

        # åŸºæœ¬ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        from adaptive_scheduler import AdaptiveInferenceScheduler
        from cache_manager import TieredCacheManager, CacheTier
        from generate_adaptive import generate_with_adaptive_scheduling

        # æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        from generate import generate_adaptive, compare_generation_methods

        print("âœ… å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
        return True

    except ImportError as e:
        print(f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def test_scheduler_creation() -> bool:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ"""
    try:
        print("\nğŸ”§ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ...")

        from adaptive_scheduler import AdaptiveInferenceScheduler

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        scheduler = AdaptiveInferenceScheduler()
        assert scheduler.min_block_size == 8
        assert scheduler.max_block_size == 64
        assert scheduler.base_confidence_threshold == 0.8

        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
        custom_scheduler = AdaptiveInferenceScheduler(
            min_block_size=4,
            max_block_size=32,
            base_confidence_threshold=0.9
        )
        assert custom_scheduler.min_block_size == 4
        assert custom_scheduler.max_block_size == 32
        assert custom_scheduler.base_confidence_threshold == 0.9

        print("âœ… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ä½œæˆæˆåŠŸ")
        return True

    except Exception as e:
        print(f"âŒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return False


def test_cache_manager_creation() -> bool:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ"""
    try:
        print("\nğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ä½œæˆãƒ†ã‚¹ãƒˆ...")

        from cache_manager import TieredCacheManager, CacheTier

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        cache_manager = TieredCacheManager()
        assert cache_manager.tier2_stability_threshold == 0.85
        assert cache_manager.tier2_update_interval == 3

        # éšå±¤åˆ—æŒ™å‹
        assert CacheTier.FROZEN.value == "frozen"
        assert CacheTier.STABLE.value == "stable"
        assert CacheTier.ACTIVE.value == "active"

        # çŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
        cache_manager.reset_cache()
        assert cache_manager.tier1_cache is None
        assert len(cache_manager.tier2_cache) == 0

        print("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ä½œæˆæˆåŠŸ")
        return True

    except Exception as e:
        print(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return False


def test_scheduler_adaptation() -> bool:
    """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é©å¿œãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
    try:
        print("\nğŸ¯ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é©å¿œãƒ­ã‚¸ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ...")

        from adaptive_scheduler import AdaptiveInferenceScheduler

        scheduler = AdaptiveInferenceScheduler(
            min_block_size=8,
            max_block_size=32,
            base_confidence_threshold=0.8
        )

        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ãƒ†ã‚¹ãƒˆ
        dummy_logits = torch.randn(1, 10, 1000)  # (batch, seq, vocab)
        dummy_tokens = torch.randint(0, 1000, (1, 10))  # (batch, seq)
        dummy_mask = torch.ones(1, 10, dtype=torch.bool)  # (batch, seq)

        # é©å¿œã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
        next_size, adapted_threshold, metrics = scheduler.step(
            logits=dummy_logits,
            tokens=dummy_tokens,
            mask_index=dummy_mask,
            step_num=1,
            total_steps=10
        )

        # çµæœæ¤œè¨¼
        assert isinstance(next_size, int)
        assert scheduler.min_block_size <= next_size <= scheduler.max_block_size
        assert isinstance(adapted_threshold, float)
        assert isinstance(metrics, dict)
        assert 'confidence' in metrics
        assert 'entropy' in metrics

        print("âœ… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é©å¿œãƒ­ã‚¸ãƒƒã‚¯æˆåŠŸ")
        return True

    except Exception as e:
        print(f"âŒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é©å¿œãƒ­ã‚¸ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return False


def test_cache_operations() -> bool:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œãƒ†ã‚¹ãƒˆ"""
    try:
        print("\nğŸ“š ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œãƒ†ã‚¹ãƒˆ...")

        from cache_manager import TieredCacheManager

        cache_manager = TieredCacheManager()

        # ãƒ€ãƒŸãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿
        dummy_cache = [
            (torch.randn(1, 8, 20, 64), torch.randn(1, 8, 20, 64))  # (key, value)
            for _ in range(4)  # 4å±¤åˆ†
        ]

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥è¨­å®š
        cache_manager.set_prompt_cache(20, dummy_cache)
        assert cache_manager.is_prompt_cached
        assert cache_manager.prompt_length == 20

        # ãƒ–ãƒ­ãƒƒã‚¯åˆ†é¡ãƒ†ã‚¹ãƒˆ
        confidence_scores = torch.tensor([0.9, 0.8, 0.85, 0.7])
        tier = cache_manager.classify_block(0, confidence_scores)
        assert tier in [cache_manager.CacheTier.STABLE,
                        cache_manager.CacheTier.ACTIVE]

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—
        metrics = cache_manager.get_cache_efficiency_metrics()
        assert isinstance(metrics, dict)
        assert 'cache_hit_rate' in metrics

        print("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œæˆåŠŸ")
        return True

    except Exception as e:
        print(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return False


def test_integration_imports() -> bool:
    """çµ±åˆã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ"""
    try:
        print("\nğŸ”— çµ±åˆã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ...")

        # çµ±åˆé–¢æ•°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        from generate import generate_adaptive, compare_generation_methods
        from generate import ADAPTIVE_SCHEDULING_AVAILABLE

        print(f"   ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°åˆ©ç”¨å¯èƒ½: {ADAPTIVE_SCHEDULING_AVAILABLE}")

        # é–¢æ•°ãŒå‘¼ã³å‡ºã—å¯èƒ½ã‹ç¢ºèª
        assert callable(generate_adaptive)
        assert callable(compare_generation_methods)

        print("âœ… çµ±åˆã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
        return True

    except Exception as e:
        print(f"âŒ çµ±åˆã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return False


def run_verification() -> Tuple[bool, Dict[str, bool]]:
    """å…¨ä½“æ¤œè¨¼ã®å®Ÿè¡Œ"""
    print("ğŸš€ Self-Correcting Adaptive Inference Scheduling å®Ÿè£…æ¤œè¨¼")
    print("=" * 70)

    tests = [
        ("ã‚¤ãƒ³ãƒãƒ¼ãƒˆ", test_imports),
        ("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ä½œæˆ", test_scheduler_creation),
        ("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ä½œæˆ", test_cache_manager_creation),
        ("ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é©å¿œãƒ­ã‚¸ãƒƒã‚¯", test_scheduler_adaptation),
        ("ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ“ä½œ", test_cache_operations),
        ("çµ±åˆã‚¤ãƒ³ãƒãƒ¼ãƒˆ", test_integration_imports)
    ]

    results = {}
    all_passed = True

    for test_name, test_func in tests:
        try:
            passed = test_func()
            results[test_name] = passed
            all_passed = all_passed and passed
        except Exception as e:
            print(f"âŒ {test_name}ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
            results[test_name] = False
            all_passed = False

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¤œè¨¼çµæœã‚µãƒãƒªãƒ¼")
    print("=" * 70)

    for test_name, passed in results.items():
        status = "âœ… æˆåŠŸ" if passed else "âŒ å¤±æ•—"
        print(f"  {test_name:<25} {status}")

    overall_status = "âœ… å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸ" if all_passed else "âŒ ä¸€éƒ¨ãƒ†ã‚¹ãƒˆå¤±æ•—"
    print(f"\nğŸ¯ ç·åˆçµæœ: {overall_status}")

    if all_passed:
        print("\nğŸ‰ Self-Correcting Adaptive Inference Schedulingã®å®Ÿè£…ã¯æ­£å¸¸ã§ã™ï¼")
        print("   ã™ã¹ã¦ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ã¾ã™ã€‚")
        print("\nğŸ“š æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("   1. QUICK_START_ADAPTIVE_SCHEDULING.md ã‚’å‚ç…§ã—ã¦ä½¿ç”¨é–‹å§‹")
        print("   2. test_adaptive_scheduling.py ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ")
        print("   3. examples/adaptive_scheduling_demo.py ã§ãƒ‡ãƒ¢ç¢ºèª")
    else:
        print("\nâš ï¸ ä¸€éƒ¨ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§å•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚")
        print("   ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç¢ºèªã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚")

    return all_passed, results


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    try:
        success, results = run_verification()
        return 0 if success else 1
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æ¤œè¨¼ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return 1
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
