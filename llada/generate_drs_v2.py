# ç†è«–çš„æ”¹å–„ç‰ˆDRSå®Ÿè£…
import numpy as np
import torch
import torch.nn.functional as F
from generate import add_gumbel_noise, get_num_transfer_tokens, get_transfer_index


@torch.no_grad()
def generate_with_improved_drs(model, prompt, steps=128, gen_length=128, block_length=128,
                               temperature=0., remasking='low_confidence', mask_id=None,
                               threshold=0.8, t_base=8, adaptive_threshold=True):
    """
    ç†è«–çš„æ”¹å–„ç‰ˆDRSå®Ÿè£…
    - å‹•çš„ä¿¡é ¼åº¦è©•ä¾¡ï¼ˆãƒ–ãƒ­ãƒƒã‚¯å®Œäº†å‰ï¼‰
    - æœªå®Œäº†ãƒ–ãƒ­ãƒƒã‚¯å„ªå…ˆäºˆç®—é…åˆ†
    - æ–‡è„ˆç¶™ç¶šæ€§ä¿æŒãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
    """

    if mask_id is None:
        if hasattr(model, 'tokenizer') and hasattr(model.tokenizer, 'mask_token_id') and model.tokenizer.mask_token_id is not None:
            mask_id = model.tokenizer.mask_token_id
        else:
            mask_id = model.config.mask_token_id

    x = torch.full((1, prompt.shape[1] + gen_length),
                   mask_id, dtype=torch.long).to(model.device)
    x[:, :prompt.shape[1]] = prompt.clone()

    assert gen_length % block_length == 0
    num_blocks = gen_length // block_length

    nfe = 0
    block_states = []  # ãƒ–ãƒ­ãƒƒã‚¯çŠ¶æ…‹ï¼šå®Œäº†åº¦ã€ä¿¡é ¼åº¦ã€ãƒã‚¹ã‚¯æ•°

    print(
        f"ğŸ“Š æ”¹å–„ç‰ˆDRSé–‹å§‹ (t_base={t_base}, adaptive_threshold={adaptive_threshold})")

    # ======== ãƒ•ã‚§ãƒ¼ã‚º1: å‹•çš„åˆæœŸç”Ÿæˆ ========
    print(f"\nPhase 1: å‹•çš„åˆæœŸç”Ÿæˆ")

    for num_block in range(num_blocks):
        current_block_start = prompt.shape[1] + num_block * block_length
        current_block_end = current_block_start + block_length

        block_mask_index = (
            x[:, current_block_start:current_block_end] == mask_id)
        num_transfer_tokens = get_num_transfer_tokens(block_mask_index, t_base)

        # ãƒ–ãƒ­ãƒƒã‚¯åˆæœŸåŒ–
        output = model(x, use_cache=True)
        past_key_values = output.past_key_values
        mask_index = (x == mask_id)
        mask_index[:, current_block_end:] = 0
        x0, transfer_index = get_transfer_index(
            output.logits, temperature, remasking, mask_index, x, num_transfer_tokens[:, 0])
        x[transfer_index] = x0[transfer_index]
        nfe += 1

        # ãƒ–ãƒ­ãƒƒã‚¯å†…é©å¿œçš„ç”Ÿæˆ
        replace_position = torch.zeros_like(x, dtype=torch.bool)
        replace_position[:, current_block_start:current_block_end] = 1

        confidence_history = []

        for i in range(1, t_base):
            nfe += 1
            mask_index = (
                x[:, current_block_start:current_block_end] == mask_id)
            if mask_index.sum() == 0:
                break  # ãƒ–ãƒ­ãƒƒã‚¯å®Œäº†

            logits = model(x[:, current_block_start:current_block_end],
                           past_key_values=past_key_values,
                           use_cache=True, replace_position=replace_position).logits

            # **æ”¹å–„ç‚¹1: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¿¡é ¼åº¦è©•ä¾¡**
            p = F.softmax(logits.to(torch.float64), dim=-1)
            current_tokens = x[:, current_block_start:current_block_end]
            confidence = torch.gather(
                p, dim=-1, index=current_tokens.unsqueeze(-1)).squeeze(-1)
            confidence_history.append(confidence[0])

            # **æ”¹å–„ç‚¹2: é©å¿œçš„é–¾å€¤**
            if adaptive_threshold and len(confidence_history) > 2:
                recent_confidence = torch.stack(confidence_history[-3:])
                dynamic_threshold = threshold * \
                    (1 + 0.1 * recent_confidence.std().item())
            else:
                dynamic_threshold = threshold

            x0, transfer_index = get_transfer_index(logits, temperature, remasking, mask_index,
                                                    x[:, current_block_start:current_block_end],
                                                    num_transfer_tokens[:, i])
            x[:, current_block_start:current_block_end][transfer_index] = x0[transfer_index]

        # ãƒ–ãƒ­ãƒƒã‚¯çŠ¶æ…‹è¨˜éŒ²
        remaining_masks = (
            x[:, current_block_start:current_block_end] == mask_id).sum().item()
        avg_confidence = torch.stack(confidence_history).mean(
        ).item() if confidence_history else 0.5

        block_states.append({
            'block_id': num_block,
            'completion': 1.0 - (remaining_masks / block_length),
            'avg_confidence': avg_confidence,
            'remaining_masks': remaining_masks,
            'confidence_history': confidence_history
        })

        print(f"  ãƒ–ãƒ­ãƒƒã‚¯ {num_block}: å®Œäº†åº¦ {block_states[-1]['completion']:.2f}, "
              f"å¹³å‡ä¿¡é ¼åº¦ {avg_confidence:.3f}, æ®‹ã‚Šãƒã‚¹ã‚¯ {remaining_masks}")

    # ======== ãƒ•ã‚§ãƒ¼ã‚º2: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆäºˆç®—é…åˆ† ========
    t_used = nfe
    t_remaining = max(0, steps - t_used)
    print(f"\nPhase 2: ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆäºˆç®—é…åˆ† (ä½¿ç”¨æ¸ˆã¿: {t_used}, æ®‹ã‚Š: {t_remaining})")

    if t_remaining <= 0:
        print("  â†’ äºˆç®—ãªã—ã€åˆæœŸç”Ÿæˆã§çµ‚äº†")
        return x, nfe, [bs['avg_confidence'] for bs in block_states]

    # **æ”¹å–„ç‚¹3: æœªå®Œäº†ãƒ–ãƒ­ãƒƒã‚¯å„ªå…ˆé…åˆ†**
    incomplete_blocks = [bs for bs in block_states if bs['completion'] < 1.0]

    if not incomplete_blocks:
        print("  â†’ å…¨ãƒ–ãƒ­ãƒƒã‚¯å®Œäº†æ¸ˆã¿ã€ä½ä¿¡é ¼åº¦ãƒ–ãƒ­ãƒƒã‚¯ã‚’å†å‡¦ç†")
        # å…¨å®Œäº†ã®å ´åˆã€ä¿¡é ¼åº¦æœ€ä½ãƒ–ãƒ­ãƒƒã‚¯ã‚’é¸æŠ
        target_blocks = sorted(block_states, key=lambda x: x['avg_confidence'])[
            :min(3, len(block_states))]
    else:
        print(f"  â†’ {len(incomplete_blocks)}å€‹ã®æœªå®Œäº†ãƒ–ãƒ­ãƒƒã‚¯ã‚’å„ªå…ˆå‡¦ç†")
        # æœªå®Œäº†ãƒ–ãƒ­ãƒƒã‚¯ã‚’åŠ¹ç‡é‡è¦–ã§é…åˆ†
        target_blocks = sorted(incomplete_blocks,
                               key=lambda x: x['remaining_masks'] /
                               max(x['avg_confidence'], 0.1),
                               reverse=True)

    # äºˆç®—é…åˆ†è¨ˆç®—
    budget_allocation = {}
    remaining_budget = t_remaining

    for i, block in enumerate(target_blocks):
        if remaining_budget <= 0:
            break

        # æœªå®Œäº†ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯å¤šã‚ã«é…åˆ†
        if block['completion'] < 1.0:
            allocated = min(remaining_budget, block['remaining_masks'] * 2)
        else:
            # å®Œäº†æ¸ˆã¿ä½ä¿¡é ¼åº¦ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯å°‘ãªã‚é…åˆ†
            allocated = min(remaining_budget // 2, max(1,
                            int(block_length * (1 - block['avg_confidence']))))

        budget_allocation[block['block_id']] = allocated
        remaining_budget -= allocated

    print(f"  â†’ äºˆç®—é…åˆ†: {budget_allocation}")

    # ======== ãƒ•ã‚§ãƒ¼ã‚º3: æ–‡è„ˆç¶™ç¶šæ€§è€ƒæ…®ãƒªãƒ•ã‚¡ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ ========
    if budget_allocation:
        print(f"\nPhase 3: æ–‡è„ˆç¶™ç¶šæ€§è€ƒæ…®ãƒªãƒ•ã‚¡ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ")

        for block_id, extra_steps in budget_allocation.items():
            if extra_steps <= 0:
                continue

            print(f"  â†’ ãƒ–ãƒ­ãƒƒã‚¯ {block_id}: {extra_steps} ã‚¹ãƒ†ãƒƒãƒ—ã§ãƒªãƒ•ã‚¡ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ")
            current_block_start = prompt.shape[1] + block_id * block_length
            current_block_end = current_block_start + block_length

            block_state = block_states[block_id]

            # **æ”¹å–„ç‚¹4: é¸æŠçš„å†ãƒã‚¹ã‚¯**ï¼ˆå®Œäº†ãƒ–ãƒ­ãƒƒã‚¯ã®ã¿ï¼‰
            if block_state['completion'] >= 1.0:
                # ä¿¡é ¼åº¦å±¥æ­´ã‹ã‚‰æœ€ã‚‚ä¸å®‰å®šãªä½ç½®ã‚’ç‰¹å®š
                if block_state['confidence_history']:
                    final_confidence = block_state['confidence_history'][-1]
                    remask_threshold = threshold + 0.1  # ã‚ˆã‚Šå³æ ¼ãªé–¾å€¤
                    remask_indices = final_confidence < remask_threshold
                    num_remasked = remask_indices.sum().item()

                    if num_remasked > 0:
                        print(f"    - {num_remasked}å€‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠçš„å†ãƒã‚¹ã‚¯")
                        x[:, current_block_start:current_block_end][remask_indices.unsqueeze(
                            0)] = mask_id
                    else:
                        print(f"    - å†ãƒã‚¹ã‚¯å¯¾è±¡ãªã—ã€ã‚¹ã‚­ãƒƒãƒ—")
                        continue

            # **æ”¹å–„ç‚¹5: æ–‡è„ˆè€ƒæ…®ãƒªãƒ•ã‚¡ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ**
            # éš£æ¥ãƒ–ãƒ­ãƒƒã‚¯ã¨ã®æ•´åˆæ€§ã‚’è€ƒæ…®
            context_start = max(
                prompt.shape[1], current_block_start - block_length // 2)
            context_end = min(
                x.shape[1], current_block_end + block_length // 2)

            output = model(x[:, context_start:context_end], use_cache=True)
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ–ãƒ­ãƒƒã‚¯éƒ¨åˆ†ã®logitsã®ã¿ã‚’ä½¿ç”¨
            target_offset = current_block_start - context_start
            target_logits = output.logits[:,
                                          target_offset:target_offset + block_length]

            past_key_values = output.past_key_values
            nfe += 1

            for step in range(extra_steps):
                mask_index = (
                    x[:, current_block_start:current_block_end] == mask_id)
                if mask_index.sum() == 0:
                    print(f"    - {step+1} ã‚¹ãƒ†ãƒƒãƒ—ã§å®Œäº†")
                    break

                nfe += 1

                # æ–‡è„ˆè€ƒæ…®ã§ã®äºˆæ¸¬
                context_output = model(x[:, context_start:current_block_end],
                                       past_key_values=past_key_values, use_cache=True)
                logits = context_output.logits[:, target_offset:]

                remaining_steps = extra_steps - step
                num_transfer_tokens = get_num_transfer_tokens(
                    mask_index, remaining_steps)[:, 0]

                x0, transfer_index = get_transfer_index(logits, temperature, remasking, mask_index,
                                                        x[:, current_block_start:current_block_end],
                                                        num_transfer_tokens)
                x[:, current_block_start:current_block_end][transfer_index] = x0[transfer_index]

    final_masks = (x[:, prompt.shape[1]:] == mask_id).sum().item()
    print(f"\næ”¹å–„ç‰ˆDRSå®Œäº†: ç·NFE={nfe}, æœªç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³={final_masks}")

    # æœ€çµ‚å“è³ªæŒ‡æ¨™
    final_confidences = []
    for bs in block_states:
        if bs['confidence_history']:
            final_confidences.append(
                bs['confidence_history'][-1].mean().item())
        else:
            final_confidences.append(bs['avg_confidence'])

    return x, nfe, final_confidences


def compare_drs_versions(model, input_ids, **kwargs):
    """DRSç‰ˆæ•°æ¯”è¼ƒå®Ÿé¨“"""
    print("ğŸ”¬ DRSç‰ˆæ•°æ¯”è¼ƒå®Ÿé¨“")

    from generate import generate_with_drs, generate_with_dual_cache

    # ã‚ªãƒªã‚¸ãƒŠãƒ«fast-dLLM
    print("\n1ï¸âƒ£ ã‚ªãƒªã‚¸ãƒŠãƒ«fast-dLLM")
    baseline_out, baseline_nfe = generate_with_dual_cache(
        model, input_ids, **kwargs)

    # å…ƒã®DRS
    print("\n2ï¸âƒ£ å…ƒã®DRS")
    original_drs_out, original_drs_nfe, original_ambiguity = generate_with_drs(
        model, input_ids, **kwargs)

    # æ”¹å–„ç‰ˆDRS
    print("\n3ï¸âƒ£ æ”¹å–„ç‰ˆDRS")
    improved_drs_out, improved_drs_nfe, improved_confidences = generate_with_improved_drs(
        model, input_ids, **kwargs)

    print(f"\nğŸ“Š æ¯”è¼ƒçµæœ:")
    print(f"  Baseline NFE: {baseline_nfe}")
    print(
        f"  å…ƒDRS NFE: {original_drs_nfe} ({original_drs_nfe/baseline_nfe:.2f}x)")
    print(
        f"  æ”¹å–„DRS NFE: {improved_drs_nfe} ({improved_drs_nfe/baseline_nfe:.2f}x)")

    return {
        'baseline': (baseline_out, baseline_nfe),
        'original_drs': (original_drs_out, original_drs_nfe, original_ambiguity),
        'improved_drs': (improved_drs_out, improved_drs_nfe, improved_confidences)
    }
