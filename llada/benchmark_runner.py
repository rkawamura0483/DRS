#!/usr/bin/env python3
# Copyright 2025 NVIDIA CORPORATION & AFFILIATES
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
import numpy as np
import time
import json
import re
import sys
import subprocess
import tempfile
import os
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from tqdm import tqdm
import argparse

from transformers import AutoTokenizer
from model.modeling_llada import LLaDAModelLM
from generate import generate_with_dual_cache
from generate_adaptive import generate_with_adaptive_scheduling
from datasets import load_dataset
import signal


class TimeoutError(Exception):
    """ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼"""
    pass


def timeout_handler(signum, frame):
    """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    raise TimeoutError("ã‚³ãƒ¼ãƒ‰å®Ÿè¡ŒãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")


class BenchmarkRunner:
    """
    ç ”ç©¶ç”¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ãƒ³ãƒŠãƒ¼

    GSM8K, MATH, HumanEval, MBPPã§ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡
    """

    def __init__(self, model_name: str = "GSAI-ML/LLaDA-8B-Instruct", device: str = "auto"):
        """
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ãƒ³ãƒŠãƒ¼ã®åˆæœŸåŒ–

        Args:
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
            device: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹
        """
        self.model_name = model_name
        self.device = device if device != "auto" else (
            "cuda" if torch.cuda.is_available() else "cpu")

        print(f"ğŸ”§ BenchmarkRunneråˆæœŸåŒ–")
        print(f"   ãƒ¢ãƒ‡ãƒ«: {model_name}")
        print(f"   ãƒ‡ãƒã‚¤ã‚¹: {self.device}")

        # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
        print("ğŸ“¦ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
        self.model = LLaDAModelLM.from_pretrained(model_name).to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model.eval()

        print("âœ… åˆæœŸåŒ–å®Œäº†")

    def load_datasets(self, samples_per_dataset: int = 25) -> Dict[str, List[Dict]]:
        """
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿

        Args:
            samples_per_dataset: å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å–å¾—ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°

        Returns:
            ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¾æ›¸
        """
        print(f"ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­... (å„{samples_per_dataset}ã‚µãƒ³ãƒ—ãƒ«)")

        datasets = {}

        # GSM8K
        print("   GSM8Kèª­ã¿è¾¼ã¿ä¸­...")
        try:
            gsm8k = load_dataset("gsm8k", "main", split="test")
            datasets["gsm8k"] = [
                {
                    "dataset": "gsm8k",
                    "id": i,
                    "question": item["question"],
                    "answer": item["answer"],
                    "type": "math"
                }
                for i, item in enumerate(gsm8k.select(range(min(samples_per_dataset, len(gsm8k)))))
            ]
            print(f"     âœ… GSM8K: {len(datasets['gsm8k'])}ã‚µãƒ³ãƒ—ãƒ«")
        except Exception as e:
            print(f"     âŒ GSM8Kèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            datasets["gsm8k"] = []

        # MATH ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        print("   MATHèª­ã¿è¾¼ã¿ä¸­...")
        try:
            math_dataset = load_dataset("competition_math", split="test")
            datasets["math"] = [
                {
                    "dataset": "math",
                    "id": i,
                    "question": item["problem"],
                    "answer": item["solution"],
                    "type": "math"
                }
                for i, item in enumerate(math_dataset.select(range(min(samples_per_dataset, len(math_dataset)))))
            ]
            print(f"     âœ… MATH: {len(datasets['math'])}ã‚µãƒ³ãƒ—ãƒ«")
        except Exception as e:
            print(f"     âŒ MATHèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            datasets["math"] = []

        # HumanEval
        print("   HumanEvalèª­ã¿è¾¼ã¿ä¸­...")
        try:
            humaneval = load_dataset("openai_humaneval", split="test")
            datasets["humaneval"] = [
                {
                    "dataset": "humaneval",
                    "id": i,
                    "task_id": item["task_id"],
                    "prompt": item["prompt"],
                    "canonical_solution": item["canonical_solution"],
                    "test": item["test"],
                    "entry_point": item["entry_point"],
                    "type": "code"
                }
                for i, item in enumerate(humaneval.select(range(min(samples_per_dataset, len(humaneval)))))
            ]
            print(f"     âœ… HumanEval: {len(datasets['humaneval'])}ã‚µãƒ³ãƒ—ãƒ«")
        except Exception as e:
            print(f"     âŒ HumanEvalèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            datasets["humaneval"] = []

        # MBPP
        print("   MBPPèª­ã¿è¾¼ã¿ä¸­...")
        try:
            mbpp = load_dataset("mbpp", "sanitized", split="test")
            datasets["mbpp"] = [
                {
                    "dataset": "mbpp",
                    "id": i,
                    "task_id": item["task_id"],
                    "text": item["text"],
                    "code": item["code"],
                    "test_list": item["test_list"],
                    "type": "code"
                }
                for i, item in enumerate(mbpp.select(range(min(samples_per_dataset, len(mbpp)))))
            ]
            print(f"     âœ… MBPP: {len(datasets['mbpp'])}ã‚µãƒ³ãƒ—ãƒ«")
        except Exception as e:
            print(f"     âŒ MBPPèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            datasets["mbpp"] = []

        total_samples = sum(len(dataset) for dataset in datasets.values())
        print(f"ğŸ“Š åˆè¨ˆ: {total_samples}ã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

        return datasets

    def format_prompt(self, sample: Dict) -> str:
        """
        ã‚µãƒ³ãƒ—ãƒ«ã«å¿œã˜ã¦ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

        Args:
            sample: ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«

        Returns:
            ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        if sample["dataset"] == "gsm8k":
            return f"å•é¡Œã‚’æ®µéšçš„ã«è§£ã„ã¦ãã ã•ã„:\n\n{sample['question']}\n\nç­”ãˆ:"

        elif sample["dataset"] == "math":
            return f"ä»¥ä¸‹ã®æ•°å­¦å•é¡Œã‚’è§£ã„ã¦ãã ã•ã„:\n\n{sample['question']}\n\nè§£ç­”:"

        elif sample["dataset"] == "humaneval":
            return f"{sample['prompt']}"

        elif sample["dataset"] == "mbpp":
            return f"ä»¥ä¸‹ã®å•é¡Œã«å¯¾ã™ã‚‹Pythoné–¢æ•°ã‚’æ›¸ã„ã¦ãã ã•ã„:\n\n{sample['text']}\n\n```python\n"

        else:
            return sample.get("question", sample.get("prompt", ""))

    def extract_answer(self, generated_text: str, sample: Dict) -> str:
        """
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç­”ãˆã‚’æŠ½å‡º

        Args:
            generated_text: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            sample: å…ƒã®ã‚µãƒ³ãƒ—ãƒ«

        Returns:
            æŠ½å‡ºã•ã‚ŒãŸç­”ãˆ
        """
        if sample["type"] == "math":
            # æ•°å­¦å•é¡Œ: æ•°å€¤ã‚’æŠ½å‡º
            # æœ€å¾Œã®æ•°å€¤ã‚’ç­”ãˆã¨ã—ã¦å–å¾—
            numbers = re.findall(r'-?\d+\.?\d*', generated_text)
            return numbers[-1] if numbers else ""

        elif sample["type"] == "code":
            # ã‚³ãƒ¼ãƒ‰å•é¡Œ: é–¢æ•°å®šç¾©ã‚’æŠ½å‡º
            if sample["dataset"] == "humaneval":
                # def ã§å§‹ã¾ã‚‹è¡Œã‹ã‚‰æ¬¡ã®def ã¾ãŸã¯æ–‡æœ«ã¾ã§
                lines = generated_text.split('\n')
                code_lines = []
                in_function = False

                for line in lines:
                    if line.strip().startswith('def '):
                        in_function = True
                        code_lines.append(line)
                    elif in_function:
                        if line.strip().startswith('def ') and len(code_lines) > 1:
                            break
                        code_lines.append(line)
                        if line.strip() == '' and len(code_lines) > 5:
                            # ç©ºè¡Œã§é–¢æ•°çµ‚äº†ã®å¯èƒ½æ€§
                            break

                return '\n'.join(code_lines)

            elif sample["dataset"] == "mbpp":
                # ```python ã¨ ``` ã®é–“ã€ã¾ãŸã¯æœ€åˆã®é–¢æ•°å®šç¾©
                code_match = re.search(
                    r'```python\n(.*?)\n```', generated_text, re.DOTALL)
                if code_match:
                    return code_match.group(1)

                # def ã§å§‹ã¾ã‚‹è¡Œã‹ã‚‰æ¨æ¸¬
                lines = generated_text.split('\n')
                for i, line in enumerate(lines):
                    if line.strip().startswith('def '):
                        return '\n'.join(lines[i:i+10])  # 10è¡Œã¾ã§å–å¾—

                return generated_text[:200]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        return generated_text.strip()

    def evaluate_math_answer(self, generated_answer: str, correct_answer: str) -> bool:
        """
        æ•°å­¦å•é¡Œã®ç­”ãˆã‚’è©•ä¾¡

        Args:
            generated_answer: ç”Ÿæˆã•ã‚ŒãŸç­”ãˆ
            correct_answer: æ­£è§£

        Returns:
            æ­£è§£ã‹ã©ã†ã‹
        """
        try:
            # æ•°å€¤æŠ½å‡º
            gen_nums = re.findall(r'-?\d+\.?\d*', generated_answer)
            correct_nums = re.findall(r'-?\d+\.?\d*', correct_answer)

            if not gen_nums or not correct_nums:
                return False

            # æœ€å¾Œã®æ•°å€¤ã‚’æ¯”è¼ƒ
            gen_val = float(gen_nums[-1])
            correct_val = float(correct_nums[-1])

            # æ•°å€¤ã®è¿‘ä¼¼æ¯”è¼ƒ
            return abs(gen_val - correct_val) < 1e-6

        except:
            # æ–‡å­—åˆ—ã®å®Œå…¨ä¸€è‡´ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return generated_answer.strip().lower() == correct_answer.strip().lower()

    def evaluate_code_execution(self, generated_code: str, sample: Dict) -> bool:
        """
        ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œã«ã‚ˆã‚‹è©•ä¾¡

        Args:
            generated_code: ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
            sample: ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«

        Returns:
            å®Ÿè¡ŒæˆåŠŸã‹ã©ã†ã‹
        """
        try:
            if sample["dataset"] == "humaneval":
                # HumanEvalã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                test_code = f"{generated_code}\n\n{sample['test']}"

                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                    f.write(test_code)
                    temp_file = f.name

                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ
                    result = subprocess.run([
                        sys.executable, temp_file
                    ], capture_output=True, timeout=5.0, text=True)

                    success = result.returncode == 0
                    return success

                except subprocess.TimeoutExpired:
                    return False
                except Exception:
                    return False
                finally:
                    if os.path.exists(temp_file):
                        os.unlink(temp_file)

            elif sample["dataset"] == "mbpp":
                # MBPPã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                test_cases = sample["test_list"]

                # å„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
                success_count = 0
                for test_case in test_cases[:3]:  # æœ€åˆã®3ã¤ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ã¿
                    try:
                        test_code = f"{generated_code}\n\n{test_case}"

                        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ
                        local_vars = {}

                        # ã‚·ã‚°ãƒŠãƒ«ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
                        signal.signal(signal.SIGALRM, timeout_handler)
                        signal.alarm(3)  # 3ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

                        try:
                            exec(test_code, {}, local_vars)
                            success_count += 1
                        except TimeoutError:
                            pass
                        except Exception:
                            pass
                        finally:
                            signal.alarm(0)  # ã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ

                    except Exception:
                        continue

                # 2/3ä»¥ä¸ŠæˆåŠŸã—ãŸã‚‰OK
                return success_count >= max(1, len(test_cases[:3]) * 0.67)

        except Exception as e:
            return False

        return False

    def run_single_evaluation(self, sample: Dict, method: str,
                              scheduler_config: Dict = None,
                              gen_length: int = 256) -> Dict[str, Any]:
        """
        å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®è©•ä¾¡ã‚’å®Ÿè¡Œ

        Args:
            sample: è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«
            method: è©•ä¾¡æ‰‹æ³• ("adaptive" or "static")
            scheduler_config: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
            gen_length: ç”Ÿæˆé•·

        Returns:
            è©•ä¾¡çµæœ
        """
        prompt_text = self.format_prompt(sample)
        prompt_ids = self.tokenizer.encode(
            prompt_text, return_tensors='pt').to(self.device)

        start_time = time.time()

        try:
            if method == "adaptive":
                # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
                output, metrics = generate_with_adaptive_scheduling(
                    model=self.model,
                    prompt=prompt_ids,
                    gen_length=gen_length,
                    enable_tiered_cache=True,
                    scheduler_config=scheduler_config,
                    verbose=False
                )

                nfe = metrics['nfe']
                adaptations = metrics['total_adaptations']
                mode_changes = metrics['total_adaptations']
                avg_block_size = metrics.get('avg_block_size', 0)

            else:  # static
                # é™çš„ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                output, nfe = generate_with_dual_cache(
                    model=self.model,
                    prompt=prompt_ids,
                    steps=128,
                    gen_length=gen_length,
                    block_length=32,
                    threshold=0.8
                )

                adaptations = 0
                mode_changes = 0
                avg_block_size = 32

        except Exception as e:
            print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e),
                "generation_time": time.time() - start_time
            }

        generation_time = time.time() - start_time

        # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        generated_text = self.tokenizer.decode(
            output[0, prompt_ids.shape[1]:],
            skip_special_tokens=True
        )

        # ç­”ãˆã‚’æŠ½å‡º
        extracted_answer = self.extract_answer(generated_text, sample)

        # æ­£ç¢ºæ€§ã‚’è©•ä¾¡
        if sample["type"] == "math":
            correct_answer = sample["answer"]
            is_correct = self.evaluate_math_answer(
                extracted_answer, correct_answer)
        elif sample["type"] == "code":
            is_correct = self.evaluate_code_execution(extracted_answer, sample)
        else:
            is_correct = False

        return {
            "success": True,
            "dataset": sample["dataset"],
            "sample_id": sample["id"],
            "method": method,
            "generation_time": generation_time,
            "nfe": nfe,
            "adaptations": adaptations,
            "mode_changes": mode_changes,
            "avg_block_size": avg_block_size,
            "is_correct": is_correct,
            "generated_text": generated_text[:500],  # æœ€åˆã®500æ–‡å­—ã®ã¿ä¿å­˜
            "extracted_answer": extracted_answer[:200]  # æœ€åˆã®200æ–‡å­—ã®ã¿ä¿å­˜
        }

    def run_comprehensive_benchmark(self,
                                    samples_per_dataset: int = 25,
                                    gen_length: int = 256,
                                    scheduler_config: Dict = None) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ

        Args:
            samples_per_dataset: å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°
            gen_length: ç”Ÿæˆé•·
            scheduler_config: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š

        Returns:
            ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        print(f"\nğŸš€ åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print(f"   å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {samples_per_dataset}ã‚µãƒ³ãƒ—ãƒ«")
        print(f"   ç”Ÿæˆé•·: {gen_length}")
        print(f"   åˆè¨ˆäºˆå®šã‚µãƒ³ãƒ—ãƒ«: {samples_per_dataset * 4}")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
        if scheduler_config is None:
            scheduler_config = {
                'to_quality_threshold': 0.80,
                'to_efficiency_threshold': 0.95,
                'confidence_window_size': 2,
                'high_efficiency_params': {'block_size': 32, 'threshold': 0.75},
                'high_quality_params': {'block_size': 8, 'threshold': 0.95}
            }

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
        datasets = self.load_datasets(samples_per_dataset)

        # å…¨ã‚µãƒ³ãƒ—ãƒ«ã‚’ã¾ã¨ã‚ã‚‹
        all_samples = []
        for dataset_name, samples in datasets.items():
            all_samples.extend(samples)

        print(f"ğŸ“Š å®Ÿéš›ã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(all_samples)}")

        # è©•ä¾¡æ‰‹æ³•
        methods = ["adaptive", "static"]

        results = {
            "summary": {},
            "detailed_results": [],
            "dataset_breakdown": {},
            "config": {
                "samples_per_dataset": samples_per_dataset,
                "gen_length": gen_length,
                "scheduler_config": scheduler_config,
                "total_samples": len(all_samples)
            }
        }

        # å„æ‰‹æ³•ã§è©•ä¾¡
        for method in methods:
            print(f"\nğŸ“Š {method.upper()}æ‰‹æ³•è©•ä¾¡ä¸­...")
            method_results = []

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§å®Ÿè¡Œ
            for sample in tqdm(all_samples, desc=f"{method.upper()}"):
                try:
                    result = self.run_single_evaluation(
                        sample=sample,
                        method=method,
                        scheduler_config=scheduler_config,
                        gen_length=gen_length
                    )
                    method_results.append(result)

                except Exception as e:
                    print(f"âŒ ã‚µãƒ³ãƒ—ãƒ«{sample['id']}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    error_result = {
                        "success": False,
                        "dataset": sample["dataset"],
                        "sample_id": sample["id"],
                        "method": method,
                        "error": str(e)
                    }
                    method_results.append(error_result)

            # çµæœã‚’è©³ç´°çµæœã«è¿½åŠ 
            results["detailed_results"].extend(method_results)

            # æ‰‹æ³•åˆ¥ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—
            successful_results = [
                r for r in method_results if r.get("success", False)]

            if successful_results:
                total_time = sum(r["generation_time"]
                                 for r in successful_results)
                total_nfe = sum(r["nfe"] for r in successful_results)
                total_correct = sum(r["is_correct"]
                                    for r in successful_results)

                if method == "adaptive":
                    total_adaptations = sum(r["adaptations"]
                                            for r in successful_results)
                    avg_block_size = np.mean(
                        [r["avg_block_size"] for r in successful_results])
                else:
                    total_adaptations = 0
                    avg_block_size = 32

                results["summary"][method] = {
                    "total_samples": len(successful_results),
                    "total_time": total_time,
                    "avg_time_per_sample": total_time / len(successful_results),
                    "total_nfe": total_nfe,
                    "avg_nfe_per_sample": total_nfe / len(successful_results),
                    "accuracy": total_correct / len(successful_results),
                    "total_adaptations": total_adaptations,
                    "avg_block_size": avg_block_size
                }

                print(f"âœ… {method.upper()}å®Œäº†:")
                print(f"   æˆåŠŸã‚µãƒ³ãƒ—ãƒ«: {len(successful_results)}")
                print(f"   å¹³å‡æ™‚é–“: {total_time/len(successful_results):.2f}ç§’")
                print(
                    f"   ç²¾åº¦: {total_correct/len(successful_results)*100:.1f}%")
                print(f"   å¹³å‡NFE: {total_nfe/len(successful_results):.1f}")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥åˆ†æ
        for dataset_name in datasets.keys():
            dataset_results = [r for r in results["detailed_results"]
                               if r.get("dataset") == dataset_name and r.get("success", False)]

            if not dataset_results:
                continue

            results["dataset_breakdown"][dataset_name] = {}

            for method in methods:
                method_dataset_results = [
                    r for r in dataset_results if r["method"] == method]

                if method_dataset_results:
                    total_time = sum(r["generation_time"]
                                     for r in method_dataset_results)
                    total_correct = sum(r["is_correct"]
                                        for r in method_dataset_results)

                    results["dataset_breakdown"][dataset_name][method] = {
                        "samples": len(method_dataset_results),
                        "avg_time": total_time / len(method_dataset_results),
                        "accuracy": total_correct / len(method_dataset_results)
                    }

        return results

    def print_results_summary(self, results: Dict[str, Any]):
        """
        çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º

        Args:
            results: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼")
        print(f"{'='*60}")

        summary = results["summary"]

        if "adaptive" in summary and "static" in summary:
            adaptive = summary["adaptive"]
            static = summary["static"]

            print(f"\nğŸ”„ ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ– vs é™çš„æ¯”è¼ƒ:")
            print(f"{'ãƒ¡ãƒˆãƒªãƒƒã‚¯':<20} {'ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–':<15} {'é™çš„':<15} {'æ”¹å–„ç‡':<10}")
            print("-" * 65)

            # æ™‚é–“æ¯”è¼ƒ
            time_improvement = (
                static["avg_time_per_sample"] - adaptive["avg_time_per_sample"]) / static["avg_time_per_sample"] * 100
            print(
                f"{'å¹³å‡æ™‚é–“(ç§’)':<20} {adaptive['avg_time_per_sample']:<15.2f} {static['avg_time_per_sample']:<15.2f} {time_improvement:>+8.1f}%")

            # NFEæ¯”è¼ƒ
            nfe_improvement = (
                static["avg_nfe_per_sample"] - adaptive["avg_nfe_per_sample"]) / static["avg_nfe_per_sample"] * 100
            print(
                f"{'å¹³å‡NFE':<20} {adaptive['avg_nfe_per_sample']:<15.1f} {static['avg_nfe_per_sample']:<15.1f} {nfe_improvement:>+8.1f}%")

            # ç²¾åº¦æ¯”è¼ƒ
            accuracy_improvement = (
                adaptive["accuracy"] - static["accuracy"]) * 100
            print(
                f"{'ç²¾åº¦':<20} {adaptive['accuracy']*100:<15.1f}% {static['accuracy']*100:<15.1f}% {accuracy_improvement:>+8.1f}%")

            print(f"{'é©å¿œå›æ•°':<20} {adaptive['total_adaptations']:<15}")
            print(
                f"{'å¹³å‡ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º':<20} {adaptive['avg_block_size']:<15.1f} {static['avg_block_size']:<15.1f}")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµæœ
        if "dataset_breakdown" in results:
            print(f"\nğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµæœ:")
            breakdown = results["dataset_breakdown"]

            for dataset_name, dataset_results in breakdown.items():
                if "adaptive" in dataset_results and "static" in dataset_results:
                    adaptive = dataset_results["adaptive"]
                    static = dataset_results["static"]

                    print(f"\n{dataset_name.upper()}:")
                    print(
                        f"  ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–: ç²¾åº¦={adaptive['accuracy']*100:.1f}%, æ™‚é–“={adaptive['avg_time']:.2f}s")
                    print(
                        f"  é™çš„:        ç²¾åº¦={static['accuracy']*100:.1f}%, æ™‚é–“={static['avg_time']:.2f}s")

    def save_results(self, results: Dict[str, Any], output_dir: str = "benchmark_results"):
        """
        çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

        Args:
            results: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        timestamp = time.strftime("%Y%m%d_%H%M%S")

        # è©³ç´°çµæœä¿å­˜
        results_file = output_path / f"benchmark_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        # CSVã‚µãƒãƒªãƒ¼ä¿å­˜
        summary_file = output_path / f"benchmark_summary_{timestamp}.csv"
        with open(summary_file, 'w') as f:
            f.write("method,dataset,samples,avg_time,accuracy,avg_nfe,adaptations\n")

            for dataset_name, dataset_results in results.get("dataset_breakdown", {}).items():
                for method, method_results in dataset_results.items():
                    nfe = results["summary"][method]["avg_nfe_per_sample"] if method in results["summary"] else 0
                    adaptations = results["summary"][method]["total_adaptations"] if method in results["summary"] else 0

                    f.write(f"{method},{dataset_name},{method_results['samples']},"
                            f"{method_results['avg_time']:.3f},{method_results['accuracy']:.3f},"
                            f"{nfe:.1f},{adaptations}\n")

        print(f"ğŸ’¾ çµæœä¿å­˜å®Œäº†:")
        print(f"   è©³ç´°çµæœ: {results_file}")
        print(f"   CSVã‚µãƒãƒªãƒ¼: {summary_file}")


def main():
    """
    ä½¿ç”¨ä¾‹:

    # åŸºæœ¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (å„25ã‚µãƒ³ãƒ—ãƒ«)
    python benchmark_runner.py --samples 25

    # é«˜é€Ÿãƒ†ã‚¹ãƒˆ (å„5ã‚µãƒ³ãƒ—ãƒ«)
    python benchmark_runner.py --samples 5 --gen-length 128

    # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
    python benchmark_runner.py --samples 25 --to-quality-threshold 0.75 --efficiency-block-size 16
    """
    parser = argparse.ArgumentParser(
        description="Adaptive Scheduling Benchmark Runner")

    # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument("--model", default="GSAI-ML/LLaDA-8B-Instruct",
                        help="ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--device", default="auto",
                        help="ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹")
    parser.add_argument("--samples", type=int, default=25,
                        help="å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°")
    parser.add_argument("--gen-length", type=int, default=256,
                        help="ç”Ÿæˆé•·")

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
    scheduler_group = parser.add_argument_group('scheduler', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š')
    scheduler_group.add_argument("--to-quality-threshold", type=float, default=0.80,
                                 help="å“è³ªãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæ›¿ãˆé–¾å€¤")
    scheduler_group.add_argument("--to-efficiency-threshold", type=float, default=0.95,
                                 help="åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæ›¿ãˆé–¾å€¤")
    scheduler_group.add_argument("--confidence-window-size", type=int, default=2,
                                 help="ä¿¡é ¼åº¦ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º")
    scheduler_group.add_argument("--efficiency-block-size", type=int, default=32,
                                 help="åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º")
    scheduler_group.add_argument("--quality-block-size", type=int, default=8,
                                 help="å“è³ªãƒ¢ãƒ¼ãƒ‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º")
    scheduler_group.add_argument("--efficiency-threshold", type=float, default=0.75,
                                 help="åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ã®ä¿¡é ¼åº¦é–¾å€¤")
    scheduler_group.add_argument("--quality-threshold", type=float, default=0.95,
                                 help="å“è³ªãƒ¢ãƒ¼ãƒ‰ã®ä¿¡é ¼åº¦é–¾å€¤")

    # å‡ºåŠ›è¨­å®š
    parser.add_argument("--output-dir", default="benchmark_results",
                        help="çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")

    args = parser.parse_args()

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šæ§‹ç¯‰
    scheduler_config = {
        'to_quality_threshold': args.to_quality_threshold,
        'to_efficiency_threshold': args.to_efficiency_threshold,
        'confidence_window_size': args.confidence_window_size,
        'high_efficiency_params': {
            'block_size': args.efficiency_block_size,
            'threshold': args.efficiency_threshold
        },
        'high_quality_params': {
            'block_size': args.quality_block_size,
            'threshold': args.quality_threshold
        }
    }

    print(f"ğŸ”§ è¨­å®š:")
    print(f"   å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {args.samples}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   ç”Ÿæˆé•·: {args.gen_length}")
    print(f"   ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š: {scheduler_config}")

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    runner = BenchmarkRunner(model_name=args.model, device=args.device)

    try:
        results = runner.run_comprehensive_benchmark(
            samples_per_dataset=args.samples,
            gen_length=args.gen_length,
            scheduler_config=scheduler_config
        )

        # çµæœè¡¨ç¤º
        runner.print_results_summary(results)

        # çµæœä¿å­˜
        runner.save_results(results, args.output_dir)

        print(f"\nğŸ‰ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†!")

    except Exception as e:
        print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        raise


if __name__ == "__main__":
    main()
