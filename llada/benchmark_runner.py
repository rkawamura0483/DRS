#!/usr/bin/env python3
# Copyright 2025 NVIDIA CORPORATION & AFFILIATES
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import torch
import numpy as np
import time
import json
import re
import sys
import subprocess
import tempfile
import os
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from tqdm import tqdm
import argparse

from transformers import AutoTokenizer
from model.modeling_llada import LLaDAModelLM
from generate import generate_with_dual_cache
from generate_adaptive import generate_with_adaptive_scheduling
from datasets import load_dataset
import signal


class TimeoutError(Exception):
    """ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼"""
    pass


def timeout_handler(signum, frame):
    """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    raise TimeoutError("ã‚³ãƒ¼ãƒ‰å®Ÿè¡ŒãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")


class BenchmarkRunner:
    """
    ç ”ç©¶ç”¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ãƒ³ãƒŠãƒ¼

    GSM8K, MATH, HumanEval, MBPPã§ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡
    """

    def __init__(self, model_name: str = "GSAI-ML/LLaDA-8B-Instruct", device: str = "auto"):
        """
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ©ãƒ³ãƒŠãƒ¼ã®åˆæœŸåŒ–

        Args:
            model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
            device: ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹
        """
        self.model_name = model_name
        self.device = device if device != "auto" else (
            "cuda" if torch.cuda.is_available() else "cpu")

        print(f"ğŸ”§ BenchmarkRunner initialization")
        print(f"   Model: {model_name}")
        print(f"   Device: {self.device}")

        # Load model and tokenizer
        print("ğŸ“¦ Loading model...")
        self.model = LLaDAModelLM.from_pretrained(model_name).to(self.device)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model.eval()

        print("âœ… Initialization complete")

    def load_datasets(self, samples_per_dataset: int = 25) -> Dict[str, List[Dict]]:
        """
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿

        Args:
            samples_per_dataset: å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å–å¾—ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°

        Returns:
            ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¾æ›¸
        """
        print(f"ğŸ“š Loading datasets... ({samples_per_dataset} samples each)")

        datasets = {}
        # å†ç¾æ€§ã®ãŸã‚ã€åŒã˜ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã™ã‚‹ãŸã‚ã®ã‚·ãƒ¼ãƒ‰
        seed = 42

        # GSM8K
        print("   Loading GSM8K...")
        try:
            gsm8k = load_dataset("gsm8k", "main", split="test")
            datasets["gsm8k"] = [
                {
                    "dataset": "gsm8k",
                    "id": i,
                    "question": item["question"],
                    "answer": item["answer"],
                    "type": "math"
                }
                for i, item in enumerate(gsm8k.shuffle(seed=seed).select(range(min(samples_per_dataset, len(gsm8k)))))
            ]
            print(f"     âœ… GSM8K: {len(datasets['gsm8k'])} samples")
        except Exception as e:
            print(f"     âŒ GSM8K loading error: {e}")
            datasets["gsm8k"] = []

        # MATH dataset
        print("   Loading MATH...")
        try:
            # è¤‡æ•°ã®ä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è©¦è¡Œ
            alternative_datasets = [
                ("lighteval/MATH-Hard", "test"),
                ("DigitalLearningGmbH/MATH-lighteval", "test"),
                ("EleutherAI/hendrycks_math", "algebra", "test")  # ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’æŒ‡å®š
            ]

            math_dataset = None
            for dataset_info in alternative_datasets:
                try:
                    if len(dataset_info) == 3:
                        # ã‚µãƒ–ã‚»ãƒƒãƒˆæŒ‡å®šãŒã‚ã‚‹å ´åˆ
                        dataset_name, subset, split = dataset_info
                        math_dataset = load_dataset(
                            dataset_name, subset, split=split)
                    else:
                        # ã‚µãƒ–ã‚»ãƒƒãƒˆæŒ‡å®šãŒãªã„å ´åˆ
                        dataset_name, split = dataset_info
                        math_dataset = load_dataset(dataset_name, split=split)
                    print(f"     ğŸ“Š ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")
                    break
                except Exception as sub_e:
                    print(f"     âš ï¸  {dataset_name} å¤±æ•—: {sub_e}")
                    continue

            if math_dataset is not None:
                datasets["math"] = [
                    {
                        "dataset": "math",
                        "id": i,
                        "question": item.get("problem", item.get("question", "")),
                        "answer": item.get("solution", item.get("answer", "")),
                        "type": "math"
                    }
                    for i, item in enumerate(math_dataset.shuffle(seed=seed).select(range(min(samples_per_dataset, len(math_dataset)))))
                ]
                print(f"     âœ… MATH: {len(datasets['math'])} samples")
            else:
                print(f"     âŒ MATH: No available dataset")
                datasets["math"] = []
        except Exception as e:
            print(f"     âŒ MATH loading error: {e}")
            datasets["math"] = []

        # HumanEval
        print("   Loading HumanEval...")
        try:
            humaneval = load_dataset("openai_humaneval", split="test")
            datasets["humaneval"] = [
                {
                    "dataset": "humaneval",
                    "id": i,
                    "task_id": item["task_id"],
                    "prompt": item["prompt"],
                    "canonical_solution": item["canonical_solution"],
                    "test": item["test"],
                    "entry_point": item["entry_point"],
                    "type": "code"
                }
                for i, item in enumerate(humaneval.shuffle(seed=seed).select(range(min(samples_per_dataset, len(humaneval)))))
            ]
            print(f"     âœ… HumanEval: {len(datasets['humaneval'])} samples")
        except Exception as e:
            print(f"     âŒ HumanEval loading error: {e}")
            datasets["humaneval"] = []

        # MBPP
        print("   Loading MBPP...")
        try:
            # Try different MBPP dataset configurations
            mbpp_configs = [
                ("mbpp", "sanitized", "test"),
                ("mbpp", None, "test"),
                ("google/mbpp", None, "test")
            ]

            mbpp_dataset = None
            for config in mbpp_configs:
                try:
                    dataset_name, subset, split = config
                    if subset:
                        mbpp_dataset = load_dataset(
                            dataset_name, subset, split=split)
                    else:
                        mbpp_dataset = load_dataset(dataset_name, split=split)
                    print(f"     ğŸ“Š ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {dataset_name}")
                    break
                except Exception as sub_e:
                    print(f"     âš ï¸  {dataset_name} å¤±æ•—: {sub_e}")
                    continue

            if mbpp_dataset is not None:
                # Check what fields are available and adapt accordingly
                sample_item = mbpp_dataset[0]
                available_fields = list(sample_item.keys())
                print(f"     ğŸ” åˆ©ç”¨å¯èƒ½ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {available_fields}")

                datasets["mbpp"] = []
                for i, item in enumerate(mbpp_dataset.shuffle(seed=seed).select(range(min(samples_per_dataset, len(mbpp_dataset))))):
                    # Adapt to different field names
                    text_field = item.get("text", item.get(
                        "prompt", item.get("description", "")))
                    code_field = item.get("code", item.get(
                        "solution", item.get("canonical_solution", "")))
                    test_field = item.get("test_list", item.get(
                        "tests", item.get("test", [])))
                    task_id_field = item.get("task_id", item.get("id", i))

                    # Ensure test_field is a list
                    if isinstance(test_field, str):
                        test_field = [test_field]
                    elif test_field is None:
                        test_field = []

                    datasets["mbpp"].append({
                        "dataset": "mbpp",
                        "id": i,
                        "task_id": task_id_field,
                        "text": text_field,
                        "code": code_field,
                        "test_list": test_field,
                        "type": "code"
                    })

                print(f"     âœ… MBPP: {len(datasets['mbpp'])} samples")
            else:
                print(f"     âŒ MBPP: No available dataset")
                datasets["mbpp"] = []
        except Exception as e:
            print(f"     âŒ MBPP loading error: {e}")
            datasets["mbpp"] = []

        total_samples = sum(len(dataset) for dataset in datasets.values())
        print(f"ğŸ“Š Total: {total_samples} samples loaded")

        return datasets

    def format_prompt(self, sample: Dict) -> str:
        """
        Format prompts according to sample type

        Args:
            sample: Data sample

        Returns:
            Formatted prompt
        """
        if sample["dataset"] == "gsm8k":
            return f"Solve this problem step by step:\n\n{sample['question']}\n\nAnswer:"

        elif sample["dataset"] == "math":
            return f"Solve the following math problem:\n\n{sample['question']}\n\nSolution:"

        elif sample["dataset"] == "humaneval":
            return f"{sample['prompt']}"

        elif sample["dataset"] == "mbpp":
            return f"Write a Python function for the following problem:\n\n{sample['text']}\n\n```python\n"

        else:
            return sample.get("question", sample.get("prompt", ""))

    def extract_answer(self, generated_text: str, sample: Dict) -> str:
        """
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç­”ãˆã‚’æŠ½å‡º

        Args:
            generated_text: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            sample: å…ƒã®ã‚µãƒ³ãƒ—ãƒ«

        Returns:
            æŠ½å‡ºã•ã‚ŒãŸç­”ãˆ
        """
        if sample["type"] == "math":
            # æ•°å­¦å•é¡Œ: ã‚ˆã‚Šå …ç‰¢ãªæ•°å€¤æŠ½å‡º
            # "Answer:" ã‚„ "=" ã®å¾Œã®æ•°å€¤ã€ã¾ãŸã¯æœ€å¾Œã®ç‹¬ç«‹ã—ãŸæ•°å€¤ã‚’æŠ½å‡º

            # ç­”ãˆã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å¾Œã®æ•°å€¤ã‚’æ¢ã™
            answer_patterns = [
                r'(?:answer|result|solution|equals?|=)\s*:?\s*([\d,]+(?:\.\d+)?)',
                r'\\boxed\{([\d,]+(?:\.\d+)?)\}',  # LaTeXå½¢å¼
                r'([\d,]+(?:\.\d+)?)\s*$',  # è¡Œæœ«ã®æ•°å€¤
            ]

            for pattern in answer_patterns:
                matches = re.findall(pattern, generated_text, re.IGNORECASE)
                if matches:
                    # ã‚«ãƒ³ãƒã‚’å‰Šé™¤ã—ã¦æ•°å€¤ã¨ã—ã¦å‡¦ç†
                    return matches[-1].replace(',', '')

            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å…¨ã¦ã®æ•°å€¤ã‹ã‚‰æœ€å¾Œã‚’é¸æŠï¼ˆã‚«ãƒ³ãƒå‡¦ç†ä»˜ãï¼‰
            numbers = re.findall(
                r'-?\d{1,3}(?:,\d{3})*(?:\.\d+)?', generated_text)
            if numbers:
                return numbers[-1].replace(',', '')

            return ""

        elif sample["type"] == "code":
            # ã‚³ãƒ¼ãƒ‰å•é¡Œ: é–¢æ•°å®šç¾©ã‚’æŠ½å‡º
            if sample["dataset"] == "humaneval":
                # def ã§å§‹ã¾ã‚‹è¡Œã‹ã‚‰é–¢æ•°ã®çµ‚ã‚ã‚Šã¾ã§ã‚’ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã«åŸºã¥ã„ã¦æŠ½å‡º
                lines = generated_text.split('\n')
                code_lines = []
                in_function = False
                start_indent = 0

                for line in lines:
                    stripped_line = line.strip()
                    if stripped_line.startswith('def '):
                        if not in_function:
                            in_function = True
                            # é–¢æ•°ã®æœ€åˆã®è¡Œã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’è¨˜éŒ²
                            start_indent = len(line) - len(line.lstrip())
                            code_lines.append(line)
                        elif in_function:
                            # åˆ¥ã®é–¢æ•°å®šç¾©ãŒå§‹ã¾ã£ãŸã‚‰çµ‚äº†
                            break
                    elif in_function:
                        # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãŒé–¢æ•°å®šç¾©ã¨åŒã˜ã‹ãã‚Œã‚ˆã‚Šå°ã•ã„ã€ç©ºã§ãªã„è¡ŒãŒè¦‹ã¤ã‹ã£ãŸã‚‰çµ‚äº†
                        current_indent = len(line) - len(line.lstrip())
                        if stripped_line and current_indent <= start_indent:
                            break
                        code_lines.append(line)

                return '\n'.join(code_lines)

            elif sample["dataset"] == "mbpp":
                # ```python ã¨ ``` ã®é–“ã€ã¾ãŸã¯æœ€åˆã®é–¢æ•°å®šç¾©
                code_match = re.search(
                    r'```python\n(.*?)\n```', generated_text, re.DOTALL)
                if code_match:
                    return code_match.group(1)

                # ã‚ˆã‚Šå …ç‰¢ãªMBPPé–¢æ•°æŠ½å‡º: å®Œå…¨ãªé–¢æ•°å®šç¾©ã‚’æŠ½å‡º
                lines = generated_text.split('\n')
                for i, line in enumerate(lines):
                    if line.strip().startswith('def '):
                        # é–¢æ•°ã®é–‹å§‹ã‚’è¦‹ã¤ã‘ãŸ
                        code_lines = [line]
                        indent_level = len(line) - len(line.lstrip())

                        # é–¢æ•°ã®æœ¬ä½“ã‚’æŠ½å‡ºï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã«åŸºã¥ãï¼‰
                        j = i + 1
                        while j < len(lines):
                            current_line = lines[j]

                            # ç©ºè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                            if current_line.strip() == '':
                                code_lines.append(current_line)
                                j += 1
                                continue

                            # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
                            current_indent = len(
                                current_line) - len(current_line.lstrip())

                            # é–¢æ•°ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ä»¥ä¸‹ã®è¡ŒãŒå‡ºã¦ããŸã‚‰çµ‚äº†
                            if current_indent <= indent_level and current_line.strip():
                                # ãŸã ã—ã€æ¬¡ã®é–¢æ•°å®šç¾©ã§ãªã„å ´åˆã¯å«ã‚ã‚‹å¯èƒ½æ€§
                                if not current_line.strip().startswith('def '):
                                    break
                                else:
                                    break

                            code_lines.append(current_line)
                            j += 1

                            # å®‰å…¨è£…ç½®: 50è¡Œã‚’è¶…ãˆãŸã‚‰çµ‚äº†
                            if len(code_lines) > 50:
                                break

                        return '\n'.join(code_lines)

                return generated_text[:200]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

        return generated_text.strip()

    def evaluate_math_answer(self, generated_answer: str, correct_answer: str) -> bool:
        """
        æ•°å­¦å•é¡Œã®ç­”ãˆã‚’è©•ä¾¡

        Args:
            generated_answer: ç”Ÿæˆã•ã‚ŒãŸç­”ãˆ
            correct_answer: æ­£è§£

        Returns:
            æ­£è§£ã‹ã©ã†ã‹
        """
        try:
            # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šæ•°å€¤ã‚’å‡¦ç†ã™ã‚‹æ”¹è‰¯ã•ã‚ŒãŸæ­£è¦è¡¨ç¾
            number_pattern = r'-?\d{1,3}(?:,\d{3})*(?:\.\d+)?'

            # ç”Ÿæˆã•ã‚ŒãŸç­”ãˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡ºï¼ˆã‚«ãƒ³ãƒã‚’å‰Šé™¤ï¼‰
            gen_nums_raw = re.findall(number_pattern, generated_answer)
            gen_nums = [num.replace(',', '') for num in gen_nums_raw]

            # æ­£è§£ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡ºï¼ˆã‚«ãƒ³ãƒã‚’å‰Šé™¤ï¼‰
            correct_nums_raw = re.findall(number_pattern, correct_answer)
            correct_nums = [num.replace(',', '') for num in correct_nums_raw]

            if not gen_nums or not correct_nums:
                # æ•°å€¤ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€æ–‡å­—åˆ—ã®å®Œå…¨ä¸€è‡´ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                return generated_answer.strip().lower() == correct_answer.strip().lower()

            # æœ€å¾Œã®æ•°å€¤ã‚’æ¯”è¼ƒ
            gen_val = float(gen_nums[-1])
            correct_val = float(correct_nums[-1])

            # æ•°å€¤ã®è¿‘ä¼¼æ¯”è¼ƒï¼ˆç›¸å¯¾èª¤å·®ã‚‚è€ƒæ…®ï¼‰
            if correct_val == 0:
                return abs(gen_val) < 1e-6
            else:
                relative_error = abs(gen_val - correct_val) / abs(correct_val)
                absolute_error = abs(gen_val - correct_val)
                return relative_error < 1e-6 or absolute_error < 1e-6

        except Exception as e:
            # æ•°å€¤å¤‰æ›ã«å¤±æ•—ã—ãŸå ´åˆã€æ–‡å­—åˆ—ã®å®Œå…¨ä¸€è‡´ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            try:
                # æ­£è¦åŒ–ã•ã‚ŒãŸæ–‡å­—åˆ—æ¯”è¼ƒ
                gen_clean = re.sub(
                    r'\s+', ' ', generated_answer.strip().lower())
                correct_clean = re.sub(
                    r'\s+', ' ', correct_answer.strip().lower())
                return gen_clean == correct_clean
            except:
                return False

    def evaluate_code_execution(self, generated_code: str, sample: Dict) -> bool:
        """
        ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œã«ã‚ˆã‚‹è©•ä¾¡

        Args:
            generated_code: ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰
            sample: ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«

        Returns:
            å®Ÿè¡ŒæˆåŠŸã‹ã©ã†ã‹
        """
        try:
            if sample["dataset"] == "humaneval":
                # HumanEvalã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                test_code = f"{generated_code}\n\n{sample['test']}"

                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                    f.write(test_code)
                    temp_file = f.name

                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ
                    result = subprocess.run([
                        sys.executable, temp_file
                    ], capture_output=True, timeout=5.0, text=True)

                    success = result.returncode == 0
                    return success

                except subprocess.TimeoutExpired:
                    return False
                except Exception:
                    return False
                finally:
                    if os.path.exists(temp_file):
                        os.unlink(temp_file)

            elif sample["dataset"] == "mbpp":
                # MBPPã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
                test_cases = sample["test_list"]

                # å„ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
                success_count = 0
                for test_case in test_cases[:3]:  # æœ€åˆã®3ã¤ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®ã¿
                    try:
                        test_code = f"{generated_code}\n\n{test_case}"

                        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§å®Ÿè¡Œ
                        local_vars = {}

                        # ã‚·ã‚°ãƒŠãƒ«ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
                        signal.signal(signal.SIGALRM, timeout_handler)
                        signal.alarm(3)  # 3ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

                        try:
                            exec(test_code, {}, local_vars)
                            success_count += 1
                        except TimeoutError:
                            pass
                        except Exception:
                            pass
                        finally:
                            signal.alarm(0)  # ã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ

                    except Exception:
                        continue

                # 2/3ä»¥ä¸ŠæˆåŠŸã—ãŸã‚‰OK
                return success_count >= max(1, len(test_cases[:3]) * 0.67)

        except Exception as e:
            return False

        return False

    def run_single_evaluation(self, sample: Dict, method: str,
                              scheduler_config: Dict = None,
                              gen_length: int = 256) -> Dict[str, Any]:
        """
        å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®è©•ä¾¡ã‚’å®Ÿè¡Œ

        Args:
            sample: è©•ä¾¡ã‚µãƒ³ãƒ—ãƒ«
            method: è©•ä¾¡æ‰‹æ³• ("adaptive" or "static")
            scheduler_config: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
            gen_length: ç”Ÿæˆé•·

        Returns:
            è©•ä¾¡çµæœ
        """
        prompt_text = self.format_prompt(sample)
        prompt_ids = self.tokenizer.encode(
            prompt_text, return_tensors='pt').to(self.device)

        start_time = time.time()

        try:
            if method == "adaptive":
                # ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
                output, metrics = generate_with_adaptive_scheduling(
                    model=self.model,
                    prompt=prompt_ids,
                    gen_length=gen_length,
                    enable_tiered_cache=True,
                    scheduler_config=scheduler_config,
                    verbose=False
                )

                nfe = metrics['nfe']
                adaptations = metrics['total_adaptations']
                mode_changes = metrics['total_adaptations']
                avg_block_size = metrics.get('avg_block_size', 0)

            else:  # static
                # é™çš„ãƒ‡ãƒ¥ã‚¢ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
                output, nfe = generate_with_dual_cache(
                    model=self.model,
                    prompt=prompt_ids,
                    steps=128,
                    gen_length=gen_length,
                    block_length=32,
                    threshold=0.8
                )

                adaptations = 0
                mode_changes = 0
                avg_block_size = 32

        except Exception as e:
            print(f"âŒ ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e),
                "generation_time": time.time() - start_time
            }

        generation_time = time.time() - start_time

        # ç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
        generated_text = self.tokenizer.decode(
            output[0, prompt_ids.shape[1]:],
            skip_special_tokens=True
        )

        # ç­”ãˆã‚’æŠ½å‡º
        extracted_answer = self.extract_answer(generated_text, sample)

        # æ­£ç¢ºæ€§ã‚’è©•ä¾¡
        if sample["type"] == "math":
            correct_answer = sample["answer"]
            is_correct = self.evaluate_math_answer(
                extracted_answer, correct_answer)
        elif sample["type"] == "code":
            is_correct = self.evaluate_code_execution(extracted_answer, sample)
        else:
            is_correct = False

        return {
            "success": True,
            "dataset": sample["dataset"],
            "sample_id": sample["id"],
            "method": method,
            "generation_time": generation_time,
            "nfe": nfe,
            "adaptations": adaptations,
            "mode_changes": mode_changes,
            "avg_block_size": avg_block_size,
            "is_correct": is_correct,
            "generated_text": generated_text[:500],  # æœ€åˆã®500æ–‡å­—ã®ã¿ä¿å­˜
            "extracted_answer": extracted_answer[:200]  # æœ€åˆã®200æ–‡å­—ã®ã¿ä¿å­˜
        }

    def run_comprehensive_benchmark(self,
                                    samples_per_dataset: int = 25,
                                    gen_length: int = 256,
                                    scheduler_config: Dict = None) -> Dict[str, Any]:
        """
        åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ

        Args:
            samples_per_dataset: å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°
            gen_length: ç”Ÿæˆé•·
            scheduler_config: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š

        Returns:
            ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        print(f"\nğŸš€ åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹")
        print(f"   å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {samples_per_dataset}ã‚µãƒ³ãƒ—ãƒ«")
        print(f"   ç”Ÿæˆé•·: {gen_length}")
        print(f"   åˆè¨ˆäºˆå®šã‚µãƒ³ãƒ—ãƒ«: {samples_per_dataset * 4}")

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
        if scheduler_config is None:
            scheduler_config = {
                'to_quality_threshold': 0.80,
                'to_efficiency_threshold': 0.95,
                'confidence_window_size': 2,
                'high_efficiency_params': {'block_size': 32, 'threshold': 0.75},
                'high_quality_params': {'block_size': 8, 'threshold': 0.95}
            }

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
        datasets = self.load_datasets(samples_per_dataset)

        # å…¨ã‚µãƒ³ãƒ—ãƒ«ã‚’ã¾ã¨ã‚ã‚‹
        all_samples = []
        for dataset_name, samples in datasets.items():
            all_samples.extend(samples)

        print(f"ğŸ“Š å®Ÿéš›ã®ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(all_samples)}")

        # è©•ä¾¡æ‰‹æ³•
        methods = ["adaptive", "static"]

        results = {
            "summary": {},
            "detailed_results": [],
            "dataset_breakdown": {},
            "config": {
                "samples_per_dataset": samples_per_dataset,
                "gen_length": gen_length,
                "scheduler_config": scheduler_config,
                "total_samples": len(all_samples)
            }
        }

        # å„æ‰‹æ³•ã§è©•ä¾¡
        for method in methods:
            print(f"\nğŸ“Š {method.upper()}æ‰‹æ³•è©•ä¾¡ä¸­...")
            method_results = []

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ä»˜ãã§å®Ÿè¡Œ
            for sample in tqdm(all_samples, desc=f"{method.upper()}"):
                try:
                    result = self.run_single_evaluation(
                        sample=sample,
                        method=method,
                        scheduler_config=scheduler_config,
                        gen_length=gen_length
                    )
                    method_results.append(result)

                except Exception as e:
                    print(f"âŒ ã‚µãƒ³ãƒ—ãƒ«{sample['id']}ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    error_result = {
                        "success": False,
                        "dataset": sample["dataset"],
                        "sample_id": sample["id"],
                        "method": method,
                        "error": str(e)
                    }
                    method_results.append(error_result)

            # çµæœã‚’è©³ç´°çµæœã«è¿½åŠ 
            results["detailed_results"].extend(method_results)

            # æ‰‹æ³•åˆ¥ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—
            successful_results = [
                r for r in method_results if r.get("success", False)]

            if successful_results:
                total_time = sum(r["generation_time"]
                                 for r in successful_results)
                total_nfe = sum(r["nfe"] for r in successful_results)
                total_correct = sum(r["is_correct"]
                                    for r in successful_results)

                if method == "adaptive":
                    total_adaptations = sum(r["adaptations"]
                                            for r in successful_results)
                    avg_block_size = np.mean(
                        [r["avg_block_size"] for r in successful_results])
                else:
                    total_adaptations = 0
                    avg_block_size = 32

                results["summary"][method] = {
                    "total_samples": len(successful_results),
                    "total_time": total_time,
                    "avg_time_per_sample": total_time / len(successful_results),
                    "total_nfe": total_nfe,
                    "avg_nfe_per_sample": total_nfe / len(successful_results),
                    "accuracy": total_correct / len(successful_results),
                    "total_adaptations": total_adaptations,
                    "avg_block_size": avg_block_size
                }

                print(f"âœ… {method.upper()}å®Œäº†:")
                print(f"   æˆåŠŸã‚µãƒ³ãƒ—ãƒ«: {len(successful_results)}")
                print(f"   å¹³å‡æ™‚é–“: {total_time/len(successful_results):.2f}ç§’")
                print(
                    f"   ç²¾åº¦: {total_correct/len(successful_results)*100:.1f}%")
                print(f"   å¹³å‡NFE: {total_nfe/len(successful_results):.1f}")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥åˆ†æ
        for dataset_name in datasets.keys():
            dataset_results = [r for r in results["detailed_results"]
                               if r.get("dataset") == dataset_name and r.get("success", False)]

            if not dataset_results:
                continue

            results["dataset_breakdown"][dataset_name] = {}

            for method in methods:
                method_dataset_results = [
                    r for r in dataset_results if r["method"] == method]

                if method_dataset_results:
                    total_time = sum(r["generation_time"]
                                     for r in method_dataset_results)
                    total_correct = sum(r["is_correct"]
                                        for r in method_dataset_results)

                    results["dataset_breakdown"][dataset_name][method] = {
                        "samples": len(method_dataset_results),
                        "avg_time": total_time / len(method_dataset_results),
                        "accuracy": total_correct / len(method_dataset_results)
                    }

        return results

    def print_results_summary(self, results: Dict[str, Any]):
        """
        çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º

        Args:
            results: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚µãƒãƒªãƒ¼")
        print(f"{'='*60}")

        summary = results["summary"]

        if "adaptive" in summary and "static" in summary:
            adaptive = summary["adaptive"]
            static = summary["static"]

            print(f"\nğŸ”„ ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ– vs é™çš„æ¯”è¼ƒ:")
            print(f"{'ãƒ¡ãƒˆãƒªãƒƒã‚¯':<20} {'ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–':<15} {'é™çš„':<15} {'æ”¹å–„ç‡':<10}")
            print("-" * 65)

            # æ™‚é–“æ¯”è¼ƒ
            time_improvement = (
                static["avg_time_per_sample"] - adaptive["avg_time_per_sample"]) / static["avg_time_per_sample"] * 100
            print(
                f"{'å¹³å‡æ™‚é–“(ç§’)':<20} {adaptive['avg_time_per_sample']:<15.2f} {static['avg_time_per_sample']:<15.2f} {time_improvement:>+8.1f}%")

            # NFEæ¯”è¼ƒ
            nfe_improvement = (
                static["avg_nfe_per_sample"] - adaptive["avg_nfe_per_sample"]) / static["avg_nfe_per_sample"] * 100
            print(
                f"{'å¹³å‡NFE':<20} {adaptive['avg_nfe_per_sample']:<15.1f} {static['avg_nfe_per_sample']:<15.1f} {nfe_improvement:>+8.1f}%")

            # ç²¾åº¦æ¯”è¼ƒ
            accuracy_improvement = (
                adaptive["accuracy"] - static["accuracy"]) * 100
            print(
                f"{'ç²¾åº¦':<20} {adaptive['accuracy']*100:<15.1f}% {static['accuracy']*100:<15.1f}% {accuracy_improvement:>+8.1f}%")

            print(f"{'é©å¿œå›æ•°':<20} {adaptive['total_adaptations']:<15}")
            print(
                f"{'å¹³å‡ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º':<20} {adaptive['avg_block_size']:<15.1f} {static['avg_block_size']:<15.1f}")

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµæœ
        if "dataset_breakdown" in results:
            print(f"\nğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ¥çµæœ:")
            breakdown = results["dataset_breakdown"]

            for dataset_name, dataset_results in breakdown.items():
                if "adaptive" in dataset_results and "static" in dataset_results:
                    adaptive = dataset_results["adaptive"]
                    static = dataset_results["static"]

                    print(f"\n{dataset_name.upper()}:")
                    print(
                        f"  ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–: ç²¾åº¦={adaptive['accuracy']*100:.1f}%, æ™‚é–“={adaptive['avg_time']:.2f}s")
                    print(
                        f"  é™çš„:        ç²¾åº¦={static['accuracy']*100:.1f}%, æ™‚é–“={static['avg_time']:.2f}s")

    def save_results(self, results: Dict[str, Any], output_dir: str = "benchmark_results"):
        """
        çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

        Args:
            results: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        timestamp = time.strftime("%Y%m%d_%H%M%S")

        # è©³ç´°çµæœä¿å­˜
        results_file = output_path / f"benchmark_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)

        # CSVã‚µãƒãƒªãƒ¼ä¿å­˜
        summary_file = output_path / f"benchmark_summary_{timestamp}.csv"
        with open(summary_file, 'w') as f:
            f.write("method,dataset,samples,avg_time,accuracy,avg_nfe,adaptations\n")

            for dataset_name, dataset_results in results.get("dataset_breakdown", {}).items():
                for method, method_results in dataset_results.items():
                    nfe = results["summary"][method]["avg_nfe_per_sample"] if method in results["summary"] else 0
                    adaptations = results["summary"][method]["total_adaptations"] if method in results["summary"] else 0

                    f.write(f"{method},{dataset_name},{method_results['samples']},"
                            f"{method_results['avg_time']:.3f},{method_results['accuracy']:.3f},"
                            f"{nfe:.1f},{adaptations}\n")

        print(f"ğŸ’¾ çµæœä¿å­˜å®Œäº†:")
        print(f"   è©³ç´°çµæœ: {results_file}")
        print(f"   CSVã‚µãƒãƒªãƒ¼: {summary_file}")


def main():
    """
    ä½¿ç”¨ä¾‹:

    # åŸºæœ¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (å„25ã‚µãƒ³ãƒ—ãƒ«)
    python benchmark_runner.py --samples 25

    # é«˜é€Ÿãƒ†ã‚¹ãƒˆ (å„5ã‚µãƒ³ãƒ—ãƒ«)
    python benchmark_runner.py --samples 5 --gen-length 128

    # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
    python benchmark_runner.py --samples 25 --to-quality-threshold 0.75 --efficiency-block-size 16
    """
    parser = argparse.ArgumentParser(
        description="Adaptive Scheduling Benchmark Runner")

    # åŸºæœ¬ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    parser.add_argument("--model", default="GSAI-ML/LLaDA-8B-Instruct",
                        help="ãƒ¢ãƒ‡ãƒ«å")
    parser.add_argument("--device", default="auto",
                        help="ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹")
    parser.add_argument("--samples", type=int, default=25,
                        help="å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°")
    parser.add_argument("--gen-length", type=int, default=256,
                        help="ç”Ÿæˆé•·")

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
    scheduler_group = parser.add_argument_group('scheduler', 'ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š')
    scheduler_group.add_argument("--to-quality-threshold", type=float, default=0.80,
                                 help="å“è³ªãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæ›¿ãˆé–¾å€¤")
    scheduler_group.add_argument("--to-efficiency-threshold", type=float, default=0.95,
                                 help="åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæ›¿ãˆé–¾å€¤")
    scheduler_group.add_argument("--confidence-window-size", type=int, default=2,
                                 help="ä¿¡é ¼åº¦ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º")
    scheduler_group.add_argument("--efficiency-block-size", type=int, default=32,
                                 help="åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º")
    scheduler_group.add_argument("--quality-block-size", type=int, default=8,
                                 help="å“è³ªãƒ¢ãƒ¼ãƒ‰ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º")
    scheduler_group.add_argument("--efficiency-threshold", type=float, default=0.75,
                                 help="åŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ã®ä¿¡é ¼åº¦é–¾å€¤")
    scheduler_group.add_argument("--quality-threshold", type=float, default=0.95,
                                 help="å“è³ªãƒ¢ãƒ¼ãƒ‰ã®ä¿¡é ¼åº¦é–¾å€¤")

    # å‡ºåŠ›è¨­å®š
    parser.add_argument("--output-dir", default="benchmark_results",
                        help="çµæœå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")

    args = parser.parse_args()

    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®šæ§‹ç¯‰
    scheduler_config = {
        'to_quality_threshold': args.to_quality_threshold,
        'to_efficiency_threshold': args.to_efficiency_threshold,
        'confidence_window_size': args.confidence_window_size,
        'high_efficiency_params': {
            'block_size': args.efficiency_block_size,
            'threshold': args.efficiency_threshold
        },
        'high_quality_params': {
            'block_size': args.quality_block_size,
            'threshold': args.quality_threshold
        }
    }

    print(f"ğŸ”§ è¨­å®š:")
    print(f"   å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {args.samples}ã‚µãƒ³ãƒ—ãƒ«")
    print(f"   ç”Ÿæˆé•·: {args.gen_length}")
    print(f"   ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š: {scheduler_config}")

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    runner = BenchmarkRunner(model_name=args.model, device=args.device)

    try:
        results = runner.run_comprehensive_benchmark(
            samples_per_dataset=args.samples,
            gen_length=args.gen_length,
            scheduler_config=scheduler_config
        )

        # çµæœè¡¨ç¤º
        runner.print_results_summary(results)

        # çµæœä¿å­˜
        runner.save_results(results, args.output_dir)

        print(f"\nğŸ‰ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†!")

    except Exception as e:
        print(f"âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        raise


if __name__ == "__main__":
    main()
