# ğŸš€ Quick Start Guide: Self-Correcting Adaptive Inference Scheduling

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Self-Correcting Adaptive Inference Schedulingã‚’å³åº§ã«ä½¿ã„å§‹ã‚ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“¦ å¿…è¦ãªè¦ä»¶

- PyTorch >= 1.12.0
- transformers >= 4.30.0
- numpy
- tqdm

## ğŸ”§ åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•

### 1. ç°¡å˜ãªä½¿ç”¨ä¾‹

```python
import torch
from transformers import AutoTokenizer
from llada.model.modeling_llada import LLaDAModelLM
from llada.generate import generate_adaptive

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿
model = LLaDAModelLM.from_pretrained('GSAI-ML/LLaDA-8B-Instruct')
tokenizer = AutoTokenizer.from_pretrained('GSAI-ML/LLaDA-8B-Instruct')
model.eval()

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™
prompt_text = "Write a Python function to calculate the factorial of a number:"
prompt = tokenizer.encode(prompt_text, return_tensors='pt')

# ã‚¢ãƒ€ãƒ—ãƒ†ã‚£ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã§ç”Ÿæˆ
output, nfe = generate_adaptive(
    model=model,
    prompt=prompt,
    gen_length=128,
    verbose=True
)

# çµæœã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰
generated_text = tokenizer.decode(output[0, prompt.shape[1]:], skip_special_tokens=True)
print(f"ç”Ÿæˆçµæœ: {generated_text}")
print(f"ä½¿ç”¨ã—ãŸNFE: {nfe}")
```

### 2. è©³ç´°åˆ¶å¾¡

```python
from llada.generate_adaptive import generate_with_adaptive_scheduling

# ã‚ˆã‚Šè©³ç´°ãªåˆ¶å¾¡
output, metrics = generate_with_adaptive_scheduling(
    model=model,
    prompt=prompt,
    gen_length=128,
    base_block_size=16,           # åˆæœŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º
    base_confidence_threshold=0.8, # åˆæœŸä¿¡é ¼åº¦é–¾å€¤
    adaptation_rate=0.2,          # é©å¿œæ„Ÿåº¦
    enable_tiered_cache=True,     # éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹åŒ–
    verbose=True
)

print(f"ç·NFE: {metrics['nfe']}")
print(f"é©å¿œå›æ•°: {metrics['total_adaptations']}")
print(f"å¹³å‡ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º: {metrics['avg_block_size']}")
print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {metrics['cache_efficiency']['cache_hit_rate']:.2%}")
```

### 3. ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼

```python
from llada.adaptive_scheduler import AdaptiveInferenceScheduler
from llada.cache_manager import TieredCacheManager
from llada.generate_adaptive import generate_with_custom_scheduler

# ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ä½œæˆ
scheduler = AdaptiveInferenceScheduler(
    min_block_size=8,
    max_block_size=64,
    base_confidence_threshold=0.85,
    adaptation_sensitivity=0.25,
    entropy_threshold_high=2.0,
    entropy_threshold_low=0.5
)

# ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
cache_manager = TieredCacheManager(
    tier2_stability_threshold=0.9,
    tier2_update_interval=3,
    memory_efficiency_mode=True
)

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã§ç”Ÿæˆ
output, metrics = generate_with_custom_scheduler(
    model=model,
    prompt=prompt,
    scheduler=scheduler,
    cache_manager=cache_manager,
    gen_length=128
)
```

## ğŸ” æ€§èƒ½æ¯”è¼ƒ

```python
from llada.generate import compare_generation_methods

# è¤‡æ•°æ‰‹æ³•ã®æ¯”è¼ƒ
results = compare_generation_methods(
    model=model,
    prompt=prompt,
    gen_length=128,
    verbose=True
)

# çµæœã®è¡¨ç¤º
if 'comparison' in results:
    print(f"ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—: {results['comparison']['speedup']:.2f}x")
    print(f"NFEå‰Šæ¸›: {results['comparison']['nfe_reduction_percent']:.1f}%")
```

## ğŸ“Š è©•ä¾¡ã¨ãƒ†ã‚¹ãƒˆ

### åŒ…æ‹¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

```bash
# åŸºæœ¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
cd llada
python test_adaptive_scheduling.py --benchmark --gen-length 128

# ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç ”ç©¶
python test_adaptive_scheduling.py --ablation --gen-length 128

# é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè©•ä¾¡
python test_adaptive_scheduling.py --long-context

# å…¨è©•ä¾¡
python test_adaptive_scheduling.py --comprehensive
```

### ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

```bash
# åŸºæœ¬ãƒ‡ãƒ¢
cd llada/examples
python adaptive_scheduling_demo.py
```

## âš™ï¸ è¨­å®šã‚¬ã‚¤ãƒ‰

### åŸºæœ¬è¨­å®š

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|-----------|-----------|------|
| `base_block_size` | 16 | åˆæœŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º |
| `base_confidence_threshold` | 0.8 | åˆæœŸä¿¡é ¼åº¦é–¾å€¤ |
| `adaptation_rate` | 0.2 | é©å¿œæ„Ÿåº¦ |
| `enable_tiered_cache` | True | éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ |

### ã‚¿ã‚¹ã‚¯åˆ¥æ¨å¥¨è¨­å®š

#### æ•°å­¦å•é¡Œãƒ»è«–ç†æ¨è«–
```python
# ä¿å®ˆçš„è¨­å®šï¼ˆç²¾åº¦é‡è¦–ï¼‰
generate_adaptive(
    model=model,
    prompt=prompt,
    base_block_size=8,
    base_confidence_threshold=0.9,
    adaptation_rate=0.1
)
```

#### ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
```python
# ãƒãƒ©ãƒ³ã‚¹è¨­å®š
generate_adaptive(
    model=model,
    prompt=prompt,
    base_block_size=16,
    base_confidence_threshold=0.8,
    adaptation_rate=0.2
)
```

#### å‰µä½œæ–‡ç« ãƒ»ã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯
```python
# ç©æ¥µçš„è¨­å®šï¼ˆé€Ÿåº¦é‡è¦–ï¼‰
generate_adaptive(
    model=model,
    prompt=prompt,
    base_block_size=32,
    base_confidence_threshold=0.7,
    adaptation_rate=0.3
)
```

## ğŸ¯ ä¸»è¦ãªåˆ©ç‚¹

1. **Training-Free**: ãƒ¢ãƒ‡ãƒ«ã®å†è¨“ç·´ä¸è¦
2. **Task-Agnostic**: ã‚¿ã‚¹ã‚¯ã«é–¢ä¿‚ãªãé©ç”¨å¯èƒ½
3. **Real-Time Adaptation**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã®å‹•çš„èª¿æ•´
4. **Memory Efficient**: éšå±¤ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
5. **Performance Gains**: 15-35%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œ

#### 1. ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼
```python
# ã‚¨ãƒ©ãƒ¼: ModuleNotFoundError
# è§£æ±º: ãƒ‘ã‚¹ã®ç¢ºèª
import sys
sys.path.append('/path/to/Fast-dLLM/llada')
```

#### 2. CUDA ãƒ¡ãƒ¢ãƒªä¸è¶³
```python
# è§£æ±º: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚„ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã‚’å°ã•ã
generate_adaptive(
    model=model,
    prompt=prompt,
    base_block_size=8,  # å°ã•ãã™ã‚‹
    enable_tiered_cache=True  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡åŒ–
)
```

#### 3. é©å¿œãŒå°‘ãªã„
```python
# è§£æ±º: é©å¿œæ„Ÿåº¦ã‚’ä¸Šã’ã‚‹
generate_adaptive(
    model=model,
    prompt=prompt,
    adaptation_rate=0.3,  # å¤§ããã™ã‚‹
)
```

## ğŸ“ˆ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ ã®èª­ã¿æ–¹

- **NFE**: Number of Function Evaluationsï¼ˆãƒ¢ãƒ‡ãƒ«æ¨è«–å›æ•°ï¼‰
- **Adaptations**: é©å¿œå®Ÿè¡Œå›æ•°
- **Block Size**: å‹•çš„ã«èª¿æ•´ã•ã‚ŒãŸãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º
- **Cache Hit Rate**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡
- **Confidence**: å¹³å‡ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢
- **Entropy**: äºˆæ¸¬ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼

## ğŸ”— é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«

- `llada/generate_adaptive.py`: ãƒ¡ã‚¤ãƒ³å®Ÿè£…
- `llada/adaptive_scheduler.py`: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
- `llada/cache_manager.py`: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
- `llada/test_adaptive_scheduling.py`: ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
- `llada/examples/adaptive_scheduling_demo.py`: ãƒ‡ãƒ¢

## ğŸ“š è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

è©³ç´°ãªæŠ€è¡“ä»•æ§˜ã«ã¤ã„ã¦ã¯ã€[README_ADAPTIVE_SCHEDULING.md](README_ADAPTIVE_SCHEDULING.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

## ğŸ¤ è²¢çŒ®

ãƒã‚°å ±å‘Šã‚„æ©Ÿèƒ½ææ¡ˆã¯ [CONTRIBUTING.md](CONTRIBUTING.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ 